[
  {
    "categories": [
      "Computer Science",
      "Quantum Mechanics"
    ],
    "authors": [
      "Andy Matuschak",
      "Michael Nielsen"
    ],
    "title": "Quantum computing for the very curious",
    "link": "https://quantum.country/qcvc",
    "description": "",
    "content": "Quantum computing for the very curious Quantum computing for the very curious Andy Matuschak and Michael Nielsen Part of a series of essays in a mnemonic medium which makes it almost effortless to remember what you read. Quantum computing for the very curious How the quantum search algorithm works How quantum teleportation works Quantum mechanics distilled by Andy Matuschak and Michael Nielsen Presented in a new mnemonic medium which makes it almost effortless to remember what you read. Quantum computing for the very curious In-text 5 days 2 weeks 1 month 2 months Long-term Part I: The state of a qubit Part II: Introducing quantum logic gates Part III: Universal quantum computing How the quantum search algorithm works How quantum teleportation works Quantum mechanics distilled Support us on Patreon Our future projects are funded in part by readers like you. Special thanks to our sponsor-level patrons, Adam Wiggins, Andrew Sutherland, Bert Muthalaly, Calvin French-Owen, Dwight Crow, fnnch, James Hill-Khurana, Lambda AI Hardware, Ludwig Petersson, Mickey McManus, Mintter, Patrick Collison, Paul Sutter, Peter Hartree, Sana Labs, Shripriya Mahesh, Tim O'Reilly. If humanity ever makes contact with alien intelligences, will those aliens possess computers? In science fiction, alien computers are commonplace. If that's correct, it means there is some way aliens can discover computers independently of humans. After all, we’d be very surprised if aliens had independently invented Coca-Cola or Pokémon or the Harry Potter books. If aliens have computers, it’s because computers are the answer to a question that naturally occurs to both human and alien civilizations. Here on Earth, the principal originator of computers was the English mathematician Alan Turing. In his paper, published in 1936 Alan M. Turing, On Computable Numbers, with an Application to the Entscheidungsproblem (1936)., Turing wasn’t trying to invent a clever gadget or to create an industry. Rather, he was attacking a problem about the nature of mathematics posed by the German mathematician David Hilbert in 1928. That sounds abstruse, but it’s worth understanding the gist of Hilbert and Turing’s thinking, since it illuminates where computers come from, and what computers will become in the future. Through his career, Hilbert was interested in the ultimate limits of mathematical knowledge: what can humans know about mathematics, in principle, and what (if any) parts of mathematics are forever unknowable by humans? Roughly speaking, Hilbert’s 1928 problem asked whether there exists a general algorithm a mathematician can follow which would let them figure out whether any given mathematical statement is provable. Hilbert’s hoped-for algorithm would be a little like the paper-and-pencil algorithm for multiplying two numbers. Except instead of starting with two numbers, you’d start with a mathematical conjecture, and after going through the steps of the algorithm you’d know whether that conjecture was provable. The algorithm might be too time-consuming to use in practice, but if such an algorithm existed, then there would be a sense in which mathematics was knowable, at least in principle. In 1928, the notion of an algorithm was pretty vague. Up to that point, algorithms were often carried out by human beings using paper and pencil, as in the multiplication algorithm just mentioned, or the long-division algorithm. Attacking Hilbert’s problem forced Turing to make precise exactly what was meant by an algorithm. To do this, Turing described what we now call a Turing machine : a single, universal programmable computing device that Turing argued could perform any algorithm whatsoever. Today we’re used to the idea that computers can be programmed to do many different things. In Turing’s day, however, the idea of a universal programmable computer was remarkable. Turing was arguing that a single, fixed device could imitate any algorithmic process whatsoever, provided the right program was supplied. It was an amazing leap of imagination, and the foundation of modern computing. In order to argue that his machine could imitate any algorithmic process, Turing considered what operations a human mathematician could perform when carrying out an algorithm. For each such operation, he had to argue that his machine could always do the same thing. His argument is too long to reproduce in full here, but it’s fun and instructive to see the style of Turing’s reasoning: Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child’s arithmetic book. In elementary arithmetic the two-dimensional character of the paper is sometimes used. But such a use is always avoidable, and I think that it will be agreed that the two-dimensional character of paper is no essential of computation. I assume then that the computation is carried out on one-dimensional paper, i.e. on a tape divided into squares. … The behavior of the computer [Turing is referring to the person performing an algorithm, not the machine!] at any moment is determined by the symbols which he is observing, and his “state of mind” at that moment. We may suppose that there is a bound B B B to the number of symbols or squares which the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We will also suppose that the number of states of mind which need be taken into account is finite. Obviously, this was an informal and heuristic argument! Invoking a child’s arithmetic book, or someone’s mental state is not the stuff of a rigorous, bulletproof argument. But Turing’s argument was convincing enough that later mathematicians and scientists have for the most part been willing to accept it. Turing’s machine became the gold standard: an algorithm was what we could perform on a Turing machine. And since that time, computing has blossomed into an industry, and billions of computers based on Turing’s model have been sold. Still, there’s something discomforting about Turing’s analysis. Might he have missed something in his informal reasoning about what an algorithm is? In 1985, the English physicist David Deutsch suggested a deeper approach to the problem of defining what is meant by an algorithm David Deutsch, “Quantum theory, the Church-Turing principle and the universal quantum computer” (1985).. Deutsch pointed out that every algorithm is carried out by a physical system, whether it be a mathematician with paper-and-pencil, a mechanical system such as an abacus, or a modern computer. Deutsch then considered the following question (I've slightly rephrased to make it easier to read): Is there a (single) universal computing device which can efficiently simulate any other physical system? If there was such a device, you could use it to perform any algorithm whatsoever, because algorithms have to be performed on some kind of physical system. And so the device would be a truly universal computer. What’s more, Deutsch pointed out, you wouldn’t need to rely on informal, heuristic arguments to justify your notion of algorithm, as Turing had done. You could use the laws of physics to prove your device was universal. So let’s come back to our opening question: will aliens have computers? Deutsch’s question above is a simple, fundamental question about the nature of the universe. It’s the kind of question which alien counterparts to Deutsch could plausibly come to ponder. And the alien civilizations of which they are a part would then be led inexorably to invent computers. In this sense, computers aren’t just human inventions. They are a fundamental feature of the universe, the answer to a simple and profound question about how the universe works. And they have likely been discovered over and over again by many alien intelligences. There’s a wrinkle in this story. Deutsch is a physicist with a background in quantum mechanics. And in trying to answer his question, Deutsch observed that ordinary, everyday computers based on Turing’s model have a lot of trouble simulating quantum mechanical systems Researchers such as Yu Manin and Richard Feynman had previously observed this, and as a result had speculated about computers based on quantum mechanics.. In particular, they seem to be extraordinarily slow and inefficient at doing such simulations. To answer his question affirmatively, Deutsch was forced to invent a new type of computing system, a quantum computer. Those quantum computers can do everything conventional computers can do, but are also capable of efficiently simulating quantum-mechanical processes. And so they are arguably a more natural computing model than conventional computers. If we ever meet aliens, my bet is that they’ll use quantum computers (or, perhaps, will have quantum computing brains). After all, it’s likely that aliens will be far more technologically advanced than current human civilization. And so they’ll use the computers natural for any technologically advanced society. This essay explains how quantum computers work. It’s not a survey essay, or a popularization based on hand-wavy analogies. We’re going to dig down deep so you understand the details of quantum computing. Along the way, we’ll also learn the basic principles of quantum mechanics, since those are required to understand quantum computation. Learning this material is challenging. Quantum computing and quantum mechanics are famously “hard” subjects, often presented as mysterious and forbidding. If this were a conventional essay, chances are that you’d rapidly forget the material. But the essay is also an experiment in the essay form. As I’ll explain in detail below the essay incorporates new user interface ideas to help you remember what you read. That may sound surprising, but uses a well-validated idea from cognitive science known as spaced-repetition testing. More detail on how it works below. The upshot is that anyone who is curious and determined can understand quantum computing deeply and for the long term. That said, you need some mathematical background to understand the essay. I’ll assume you’re comfortable with complex numbers and with linear algebra – vectors, matrices, and so on. I’ll also assume you’re comfortable with the logic gates used in conventional computers – gates such as AND, OR, NOT, and so on. If you don’t have that mathematical background, you’ll need to acquire it. How you do that depends on your prior experience and learning preferences – there’s no one-size-fits-all approach, you’ll need to figure it out for yourself. But two resources you may find helpful are: (1) 3Blue1Brown’s series of YouTube videos on linear algebra; and (2) the more in-depth linear algebra lectures by Gil Strang. Try them out, and if you find them helpful, keep going. If not, explore other resources. It may seem tempting to try to avoid this mathematics. If you look around the web, there are many flashy introductions to quantum computing that avoid mathematics. There are, for instance, many rather slick videos on YouTube. They can be fun to watch, and the better ones give you some analogies to help make sense of quantum computing. But there’s a hollowness to them. Bluntly, if they don’t explain the actual underlying mathematical model, then you could spend years watching and rewatching such videos, and you’d never really get it. It’s like hanging out with a group of basketball players and listening to them talk about basketball. You might enjoy it, and feel as though you’re learning about basketball. But unless you actually spend a lot of time playing, you’re never going to learn to play basketball. To understand quantum computing, you absolutely must become fluent in the mathematical model. Part I: The state of a qubit As you know, in ordinary, everyday computers the fundamental unit of information is the bit. It’s a familiar but astonishing fact that all the things those computers do can be broken down into patterns of 0 0 0 s and 1 1 1 s, and simple manipulations of 0 0 0 s and 1 1 1 s. For me, I feel this most strongly when playing video games. I’ll be enjoying playing a game, when I’ll suddenly be hit by a realization of the astounding complexity behind the imaginary world visible on my screen: Source Copyright Wildfire Games, used under a Creative Commons Attribution-Share Alike 3.0 license.. Underlying every such image is millions of pixels, described by tens of millions of bits. When I move the game controller, I am effectively conducting an orchestra, tens of millions strong, organized through many layers of intermediary ideas, in such a way as to create enjoyment and occasionally sheer delight. I’ve described a bit as an abstract entity, whose state is 0 0 0 or 1 1 1 . But in the real world, not the world of mathematics, we must find some way of storing our bits in a physical system. That can be done in many different ways. In your computer’s memory chips, bits are most likely stored as tiny electric charges on nanometer-scale capacitors (i.e., little reservoirs of charge), just above the surface of the chip. Old-fashioned hard disks take a different approach, using tiny magnets to store bits. Furthermore, different types of memory use different types of capacitor; different types of hard disk use different approaches to magnetization. For the most part you don’t notice these differences when you use your computer. Computer designers work very, very hard to make the details of the physical instantiation of the bits invisible not just to the user, but also (often) invisible even to programmers. Many programmers never think about whether a bit is stored in fast on-microprocessor cache memory, in the dynamic RAM chips, or in some type of virtual memory (say, on a hard disk). There are exceptions – programmers working on high-performance programs sometimes do think about these things, to make their programs as fast as possible. But for many programmers it doesn’t much matter how bits are stored. Rather, they can think of the bit in purely abstract terms, as having a state which is either 0 0 0 or 1 1 1 . In a manner similar to the way conventional computers are made up of bits, quantum computers are made up of quantum bits, or qubits. Just like a bit, a qubit has a state. But whereas the state of a bit is a number ( 0 0 0 or 1 1 1 ), the state of a qubit is a vector. More specifically, the state of a qubit is a vector in a two-dimensional vector space. This vector space is known as state space. For instance, here’s a possible state for a qubit: [ 1 0 ] \\left[ \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right] [ 1 0 ​ ] That perhaps sounds strange! What does it mean that the state of a qubit is a two-dimensional vector? We’re going to unpack the answer slowly and gradually. You won’t have a single epiphany where you think “ahah, that’s what a qubit is!” Rather, you’ll gradually build up many details in your understanding, until you get to the point where you’re comfortable working with qubits, with quantum computations, and more generally with quantum mechanics. One way qubits are similar to bits: we’ve said absolutely nothing about what the qubit actually is, physically. Maybe the state of the qubit is being stored somehow on an electron, or a photon, or an atom. Or maybe it’s being stored in something stranger, perhaps inside some exotic particle or state of matter, even further removed from our everyday experience. For our purposes in this essay none of this matters, no more than you should worry about what type of capacitor is storing the bits inside your computer’s RAM. What you should take away is that: (a) qubits have a state; (b) much like a bit, that state is an abstract mathematical object; but (c) whereas a bit’s abstract state is a number, 0 0 0 or 1 1 1 , the state of a qubit is a 2 2 2 - dimensional vector; (d) we call the 2 2 2 - dimensional vector space where states live state space. Alright, let’s review what we’ve learnt. Please indulge me by answering the questions just below. It’ll only take a few seconds – for each question, think about what you believe the answer to be, click to reveal the actual answer, and then mark whether you remembered or not. If you can recall, that’s great. If not, that’s also fine, just note the correct answer, and continue. Didn’t remember Remembered A medium which makes memory a choice Perhaps you correctly recalled the answers to all three questions just now. Even if so, will you remember the answers in a week? In a year? Human memory is fallible. If your memory is like mine, you might vaguely remember the answers in a week: “what’s the state of a qubit, oh yes, it’s a vector!” But the chances you’ll remember in a month or a year are low. And if you forget such things, you won’t have any durable understanding of quantum computing. How can we ensure you don’t remember these answers for just a few minutes or a few hours, but well into the future, perhaps even permanently? One way is for you to be supremely virtuous, to keep coming back and re-reviewing the material until it’s firmly locked in your memory. If you are such a virtuous person, congratulations! But for the other 99 percent of us that’s not likely. What can we do? For more than a century, cognitive scientists have studied human memory. And they’ve figured out some simple strategies that ensure you’ll remember something permanently. The single most important idea is to re-test you on your knowledge, with expanding time intervals between tests. As an example, consider the question above: “How many dimensions does the state space of a qubit have?” If you got it right, you’d ideally be tested again in 5 days. And if you got it right again, you’d be tested again 2 weeks after that. And then a month after that. And then 2 months. And so on, a gradually expanding schedule. If you get the question wrong on one of those tests, the schedule would contract, so you can relearn the answer. In other words, the schedule for questions would look something like this: It turns out that such an expanding schedule is the optimal way to retain information. Each time you’re re-tested your brain consolidates the answer a little better into long-term memory, until eventually it’s permanent. Spaced-repetition testing is a simple idea, but has profound consequences. First, it doesn’t take much overall time. Because of the expanding test schedule, it typically only takes a few minutes of total review time to memorize the answer to a question for years or decades. I won’t go through the math showing that, but you can see it worked out elsewhere. Second, spaced-repetition testing gives you a guarantee you will remember the answer to the question. For the most part our memories work in a haphazard manner. We read or hear something interesting, and hope we remember it in future. Spaced-repetition testing makes memory into a choice. This sounds great, but also like you’ll need to be very disciplined in re-testing yourself. Fortunately, the computer can handle all the scheduling for you. And so this essay isn’t just a conventional essay, it’s also a new medium, a mnemonic medium which integrates spaced-repetition testing. The medium itself makes memory a choice. This comes at some cost: you’re committing to future review. But consider what that buys you. This essay will likely take you an hour or two to read. In a conventional essay, you’d forget most of what you learned over the next few weeks, perhaps retaining a handful of ideas. But with spaced-repetition testing built into the medium, a small additional commitment of time means you will remember all the core material of the essay. Doing this won’t be difficult, it will be easier than the initial read. Furthermore, you’ll be able to read other material which builds on these ideas; it will open up an entire world. This spaced-repetition approach is why the questions only require a few seconds to read and answer. They’re not complex exercises, in the style of a textbook. Rather, the questions have a different point: the promise each question makes is that you will remember the answer forever. It’s to permanently change your thinking. So, I invite you to set up an account by signing in below. If you do so, your review schedule for each question in the essay will be tracked, and you’ll receive reminders each day (or few days), containing a link which takes you to an online review session. That review session isn’t this full essay – rather, it looks just like the question set you answered above, but contains instead all the questions which are due, so you can quickly run through them. The time commitment will usually be a few minutes per day – a little more early on, when questions need frequent re-testing, but rapidly dropping off. You can study on your phone while grabbing coffee, or standing in line, or going for a walk, or in transit. The return for that small time commitment is greatly improved fluency in basic quantum computing and quantum mechanics. And that understanding will be internalized, a part of who you are, retained for years instead of days. To keep this promise, we’re tracking your review schedule for each question, and sending you occasional reminders to check in, and to run through the questions which are due. You can review on your phone while grabbing coffee, or standing in line, or going for a walk, or on transit. The return for that commitment is greatly improved fluency in basic quantum computing and quantum mechanics. And that understanding will be internalized, a part of who you are, retained for years instead of weeks. Please sign in so we can save your progress and let you know the best times to review. Thank you! Your progress will be saved as you read. Having extolled the virtues of spaced-repetition testing, let’s try another question: Didn’t remember Remembered This question is similar to an earlier question: “How many dimensions does the state space of a qubit have?” It may seem inefficient to have such similar questions, but it helps build fluency with the material when you have the “same” information encoded into memory in multiple ways, triggering off different associations. And so many of the questions below have this nature, elaborating ideas in multiple ways. Connecting qubits to bits: the computational basis states Let’s get back to understanding qubits. I’ve described what the state of a qubit is, but given no hint about how (or whether) that’s connected to the state of a classical bit. (Henceforth we’ll use the phrase “classical bit” instead of “conventional bit”, after “classical physics”). In fact, there are two special quantum states which correspond to the 0 0 0 and 1 1 1 states of a classical bit. The quantum state corresponding to 0 0 0 is usually denoted ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ . That’s a fancy notation for the following vector: ∣ 0 ⟩ : = [ 1 0 ] . |0\\rangle := \\left[ \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right]. ∣ 0 ⟩ : = [ 1 0 ​ ] . This special state ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ is called a computational basis state. It’s tempting to see the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ notation and wonder what all the separate pieces mean – what, for instance, does the ∣ | ∣ mean; what does the ⟩ \\rangle ⟩ mean; and so on? In fact, it’s best to regard ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ as a single symbol, standing for a single mathematical object – that vector we just saw above, [ 1 0 ] \\left[ \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right] [ 1 0 ​ ] . The ∣ | ∣ and ⟩ \\rangle ⟩ don’t really have separate meanings except to signify this is a quantum state. In this, ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ is much like the symbol 0 0 0 : both stand for a single mathematical entity. And, as we’ll gradually come to see, a quantum computer can manipulate ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ in ways very similar to how a conventional computer can manipulate 0 0 0 . This notation with ∣ | ∣ and ⟩ \\rangle ⟩ is called the ket notation, and things like ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ are called kets. But don’t be thrown off by the unfamiliar terminology – a ket is just a vector, and when we say something is a ket, all we mean is that it’s a vector. That said, the term ket is most often used in connection with notations like ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ . That is, we wouldn’t usually refer to [ 1 0 ] \\left[ \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right] [ 1 0 ​ ] as a ket; we’d call it a vector, instead. If you’re a fan of really sharp definitions and strict consistency, this may seem vague and wishy-washy, but in practice doesn’t cause confusion. It’s just a convention to be aware of! Alright, so ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ is a computational basis state for a qubit, and plays much the same role as 0 0 0 does for a classical bit. It won’t surprise you to learn that there is another computational basis state, denoted ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ , which plays the same role as 1 1 1 does for a bit. Like ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ , ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ is just a notation for a two-dimensional vector, in this case: ∣ 1 ⟩ : = [ 0 1 ] . |1\\rangle := \\left[ \\begin{array}{c} 0 \\\\ 1 \\end{array} \\right]. ∣ 1 ⟩ : = [ 0 1 ​ ] . Again, we’ll gradually come to see that ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ has all the properties we expect of the 1 1 1 state of a classical bit. Time for a few more questions. A reminder that these have a different purpose to conventional textbook exercises. Textbook exercises are about setting you challenges; the point of the questions below is instead to help you commit the answer to long-term memory. Didn’t remember Remembered How to use (or not use!) the questions Mathematics is a process of staring hard enough with enough perseverance at the fog of muddle and confusion to eventually break through to improved clarity. I’m happy when I can admit, at least to myself, that my thinking is muddled, and I try to overcome the embarrassment that I might reveal ignorance or confusion. Over the years, this has helped me develop clarity in some things, but I remain muddled in many others. — William Thurston, Fields-medal winning mathematician How should you think about it when you get one of these embedded questions wrong? Often in life testing means being judged, and being judged can be uncomfortable. If you don’t know the answer to a question you may feel like you’ve failed. But the purpose of these questions has nothing to do with external judgment. Rather, they have two quite different purposes. One purpose is, as explained earlier, to help strengthen your memory, so your new knowledge is consolidated in your long-term memory. The second purpose is to help you diagnose what you do and don’t know, and to help you fill the gaps as rapidly as possible. In this view, getting a question wrong is useful information. It pinpoints exactly what you need to understand, and puts the onus on you to figure it out. Honestly, the appropriate response to getting a question wrong is probably to throw your hands up in the air and shout “wonderful!”, since it’s at those points of what may appear to be “failure” that your learning rate will be at its highest. With that said, it’s a common canard in education that “it doesn’t matter if you get something right or wrong, so long as you’re learning”. Unfortunately, that statement is often bullshit. If getting something right or wrong is used to determine your grade, or to influence other people’s opinion of you, then it damn well does matter. For that reason, I suggest keeping your results private. They are for you alone, and their purpose should be solely to help you learn as rapidly as possible. If they’re not helping you – if they’re making you feel bad, for instance, or being used to judge you – then stop doing the questions. You’ve probably noticed the questions are self-assessed. If you want to mark yourself “correct” sometimes, even when you’re not, go for it! What impact do you think that will have on your learning? Do you enjoy the slightly transgressive feeling? I must admit that I do. Don’t be embarrassed, if so: this is supposed to be, above all else, fun. Or try marking yourself wrong when you’re correct, or skipping the questions entirely. What impact do these actions have on your learning? The point is to figure out how to engage with the questions to learn as rapidly as possible. And that means experimenting playfully with how you engage, to find what works for you. How to approach this essay? This essay is an unusual form. It’s certainly not a product in the conventional startup sense; it’s a research project, an experiment in developing a new and improved form of reading. A conventional product would aim to draw you in, and form a regular, long-term habit. There’d be tens (or hundreds) of millions of words of content for you to read, and lots of people on social media excitedly pointing you toward that content. You’d engage every day, learning more and more and more. That’s not what’s going on here. “Quantum Computing for the Very Curious” is, instead, like a video game or book or movie, a single one-off project for you to work through. So you commit to it for a while, and then the experience is over. (Also, like a video game, book, or movie, sequels are planned!) With that said, it’s different from those forms too. Many people consume games, books, and movies as binge activities, hungrily devouring them until complete. “Quantum Computing for the Very Curious” is, by contrast, intentionally an experience spread out over time. Yes, you probably binge at first, working your way quickly through the text over a couple of hours. But then you return occasionally for brief review sessions, prompted by our notifications. That form of spaced testing is the best way for you to remember what you read. Still, while it might be optimal from the point of view of memory formation, it means not using some of the techniques that games, books, and movies use to keep you interested. Nonetheless, I hope you’ll be willing to trust the team at “Quantum Computing for the Very Curious”, and to participate in the experiment. How can you get the most out of reading in this new mnemonic medium? The ideal is to do an initial read, followed by a few short review sessions over the next few days (prompted by our notifications) to help you internalize the ideas. Then (optionally) a followup read, where you can more deeply understand the material. And finally, some sessions of followup review to ensure you remember for the long term: As you work through this process, we’ll track your overall progress toward completion (meaning: most questions reliably committed to memory), and each time you review we’ll show you that progress. This is a larger commitment than traditional reading. But for a small factor in effort, you will understand the material far more deeply, and remember it for more than 10x as long. What’s more, while the reading process above looks complex, you’ll be cued at each step by reminders that will help the review process become a habit. Trust the reminders, and this will all happen as a matter of course. You may find the essay particularly helpful if you’re taking an introductory class on quantum computing. If that’s your situation, I advise you to read the entire essay immediately at the beginning of semester (or even before), answering all the questions as you go. Then continue to follow the procedure described just above, taking a few minutes each day to complete your review sessions, prompted by the reminders you’ll be sent. This will make it far easier to understand the rest of the course you’re taking, and help you get much more out of it. General states of a qubit The computational basis states ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ are just two possible states for a qubit. Many more states are possible, and those extra states endow qubits with capabilities not available to ordinary classical bits. In general, remember, a quantum state is a two-dimensional vector. Here’s an example, with a graphical illustration emphasizing the vector nature of the state: In this example, the state 0.6 ∣ 0 ⟩ + 0.8 ∣ 1 ⟩ 0.6|0\\rangle+0.8|1\\rangle 0 . 6 ∣ 0 ⟩ + 0 . 8 ∣ 1 ⟩ is just 0.6 0.6 0 . 6 times the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ vector, plus 0.8 0.8 0 . 8 times the ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ vector. In the usual vector notation that means the state is: 0.6 ∣ 0 ⟩ + 0.8 ∣ 1 ⟩ = 0.6 [ 1 0 ] + 0.8 [ 0 1 ] = [ 0.6 0.8 ] . 0.6|0\\rangle + 0.8 |1\\rangle = 0.6 \\left[ \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right] + 0.8 \\left[ \\begin{array}{c} 0 \\\\ 1 \\end{array} \\right] = \\left[ \\begin{array}{c} 0.6 \\\\ 0.8 \\end{array} \\right]. 0 . 6 ∣ 0 ⟩ + 0 . 8 ∣ 1 ⟩ = 0 . 6 [ 1 0 ​ ] + 0 . 8 [ 0 1 ​ ] = [ 0 . 6 0 . 8 ​ ] . I’ve been talking about quantum states as two-dimensional vectors. What I didn’t yet mention is that in general they’re complex vectors, that is, they can have complex numbers as entries. Of course, the example just shown has real entries, as do the computational basis states. But for a general quantum state the entries can be complex numbers. So, for instance, another quantum state is the vector 1 + i 2 ∣ 0 ⟩ + i 2 ∣ 1 ⟩ , \\frac{1+i}{2} |0\\rangle + \\frac{i}{\\sqrt{2}} |1\\rangle, 2 1 + i ​ ∣ 0 ⟩ + 2 ​ i ​ ∣ 1 ⟩ , which can be written in the conventional component vector representation as [ 1 + i 2 i 2 ] . \\left[ \\begin{array}{c} \\frac{1+i}{2} \\\\ \\frac{i}{\\sqrt{2}} \\end{array} \\right]. [ 2 1 + i ​ 2 ​ i ​ ​ ] . Because quantum states are in general vectors with complex entries, the illustration above shouldn’t be taken too literally – the plane is a real vector space, not a complex vector space. Still, visualizing it as a plane is sometimes a handy way of thinking. I’ve said what a quantum state is, as a mathematical object: it’s a two-dimensional vector in a complex vector space. But why is that true? What does it mean, physically, that it’s a vector? Why a complex vector space, and how should we think about the complex numbers? And what’s a quantum state good for, anyway? These are good questions. But they do take some time to answer. For context consider that the discovery of quantum mechanics wasn’t a single event, but occurred over 25 years of work, from 1900 to 1925. Many Nobel prizes were won for milestones along the way. That includes Albert Einstein’s Nobel Prize, won primarily for work related to quantum mechanics (not relativity, as people sometimes assume). When some of the brightest people in the world struggle for 25 years to develop a theory, it’s not an obvious theory! In fact, the idea of describing a simple quantum system using a complex vector in two dimensions summarizes much of what was learned over that 25 years. In that sense, it’s quite a simple and beautiful statement. But it’s not an obvious statement, and it’s not unreasonable that it might take a few hours to understand. That’s better than taking 25 years to understand! As part of that journey toward understanding, let’s get familiar with some more nomenclature commonly used for quantum states. One of the most common terms is superposition. People will say a state like 0.6 ∣ 0 ⟩ + 0.8 ∣ 1 ⟩ 0.6|0\\rangle+0.8|1\\rangle 0 . 6 ∣ 0 ⟩ + 0 . 8 ∣ 1 ⟩ is a superposition of ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ . All they mean is that the state is a linear combination of ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ . You may wonder why they don’t just say “linear combination” (and sometimes they do), but the reason is pretty much the same reason English-speakers say “hello” while Spanish-speakers say “hola” – the two terms come out of different cultures and different histories. Another common term is amplitude. An amplitude is the coefficient for a particular state in superposition. For instance, in the state 0.6 ∣ 0 ⟩ + 0.8 ∣ 1 ⟩ 0.6|0\\rangle+0.8|1\\rangle 0 . 6 ∣ 0 ⟩ + 0 . 8 ∣ 1 ⟩ the amplitude for ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ is 0.6 0.6 0 . 6 , and the amplitude for ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ is 0.8 0.8 0 . 8 . We’ve learnt that a quantum state is a two-dimensional complex vector. Actually, it can’t be just any old vector, a fact you might have guessed from the very particular amplitudes in some of the examples above. There’s a constraint. The constraint is this: the sums of the squares of the amplitudes must be equal to 1 1 1 . So, for example, for the state 0.6 ∣ 0 ⟩ + 0.8 ∣ 1 ⟩ 0.6 |0\\rangle+0.8 |1\\rangle 0 . 6 ∣ 0 ⟩ + 0 . 8 ∣ 1 ⟩ the sum of the squares of the amplitudes is 0. 6 2 + 0. 8 2 0.6^2+0.8^2 0 . 6 2 + 0 . 8 2 , which is 0.36 + 0.64 = 1 0.36+0.64 = 1 0 . 3 6 + 0 . 6 4 = 1 , as we desired. For a more general quantum state, the amplitudes can be complex numbers, let’s denote them by α \\alpha α and β \\beta β so the state is α ∣ 0 ⟩ + β ∣ 1 ⟩ \\alpha|0\\rangle+\\beta|1\\rangle α ∣ 0 ⟩ + β ∣ 1 ⟩ . The constraint is now that the sum of the squares of the amplitudes is 1 1 1 , i.e., ∣ α ∣ 2 + ∣ β ∣ 2 = 1 |\\alpha|^2+|\\beta|^2 = 1 ∣ α ∣ 2 + ∣ β ∣ 2 = 1 . This is called the normalization constraint. It’s called that because if you think of ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ as orthonormal vectors, as I drew them earlier, then the normalization constraint is the requirement that the length of the state is equal to 1 1 1 . So it’s a unit vector, or a normalized vector, and that’s why this is called a normalization constraint. Summing up all these ideas in one sentence: the quantum state of a qubit is a vector of unit length in a two-dimensional complex vector space known as state space. We’ve gone through a few refinements of this sentence but that sentence is the final version – there’s no missing parts, or further refinement necessary! That’s what the quantum state of a qubit is. Of course, we will explore the definition further, deepening our understanding, but it will always come back to that basic fact. One common gotcha in thinking about qubits is to look at the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state and think it must be the zero vector in the vector space, often denoted 0 0 0 . But that’s not right at all. The zero vector is at the origin, 0 = [ 0 0 ] 0 = \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right] 0 = [ 0 0 ​ ] , while the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ vector is quite different, ∣ 0 ⟩ = [ 1 0 ] ≠ 0 |0\\rangle = \\left[ \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right] \\neq 0 ∣ 0 ⟩ = [ 1 0 ​ ]  ​ = 0 . It’s just an unfortunate notational accident that ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ looks as though it should be the 0 0 0 vector. Fortunately, in practice this distinction is easy to get used to, and doesn’t cause confusion. But it’s worth noting. Didn’t remember Remembered What does the quantum state mean? Why is it a vector in a complex vector space? My own conclusion is that today there is no interpretation of quantum mechanics that does not have serious flaws. This view is not universally shared. Indeed, many physicists are satisfied with their own interpretation of quantum mechanics. But different physicists are satisfied with different interpretations. — Steven Weinberg, Nobel Laureate in Physics Let’s come back to the question of what the quantum state means, and why it’s a vector in a complex vector space. In the case of classical bits, it’s pretty easy to think about the state. You can think of the 0 0 0 or 1 1 1 states of a bit as corresponding to two very different (but stable) states of a physical system. For instance, a 0 0 0 or 1 1 1 can be indicated by the presence or absence of a hole at some location in a punch card. And so a single punch card can be used to store hundreds or thousands of bits: Source: Gwern (2006). Most ways of storing classical bits are variations on this idea. For instance, the dynamic random access memory (RAM) inside your computer is based on the idea of having two tiny metal plates separated by a miniscule gap. Electric charge is stored on those plates, setting up an electric field between them. The 0 0 0 and 1 1 1 states of the bit correspond to two different configurations of charge on the plates. In practice, real dynamic RAM systems use slightly more elaborate ideas, but that’s the heart of it. This is harder to think about than punch cards – most of us don’t have so much experience thinking about moving electric charges around metal plates. But it’s still pretty concrete. So, how should we think about quantum states? The ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ states often aren’t difficult to think about, since they often correspond to very concrete physical states of the world, much like classical bits. Indeed, in some proposals they may correspond to different charge configurations, similar to dynamic RAM. Or perhaps a photon being in one of two different locations in space – again, a pretty simple, concrete notion, even if photons aren’t that familiar. There are also many more exotic proposals for the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ states. But by and large you can think of the computational basis states as representing a physical system in one of two well-defined configurations. Of course, qubits have states which aren’t computational basis states, states like 0.6 ∣ 0 ⟩ + 0.8 ∣ 1 ⟩ 0.6|0\\rangle+0.8|1\\rangle 0 . 6 ∣ 0 ⟩ + 0 . 8 ∣ 1 ⟩ . How should we think about such superposition states? At least in popular media accounts, a very common description is that a state like 0.6 ∣ 0 ⟩ + 0.8 ∣ 1 ⟩ 0.6|0\\rangle + 0.8|1\\rangle 0 . 6 ∣ 0 ⟩ + 0 . 8 ∣ 1 ⟩ is “simultaneously” in the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state and the ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ state. I must confess, I don’t understand what people mean by this. As far as I can tell, what they’re trying to do is explain the quantum state in terms of classical concepts they’re already familiar with. But I can’t make much sense of it – saying 0.6 ∣ 0 ⟩ + 0.8 ∣ 1 ⟩ 0.6|0\\rangle + 0.8|1\\rangle 0 . 6 ∣ 0 ⟩ + 0 . 8 ∣ 1 ⟩ is simultaneously in the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state and the ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ state seems like word salad, and makes about as much sense to me as Lewis Carroll’s nonsense poem Jabberwocky : ’Twas brillig, and the slithy toves Did gyre and gimble in the wabe: All mimsy were the borogoves, And the mome raths outgrabe.… Of course, lacking any other interpretation it’s tempting to try to impose our classical prejudices on the quantum state. Or, even if you reject that, to get hung up worrying about what a quantum state is. But the trouble is that there is enormous disagreement amongst physicists themselves about how to think about the quantum state. Indeed, many active researchers are trying to understand what the correct way of thinking about the quantum state is, exploring multiple approaches in great depth. So we’re going to defer worrying too much about this until later. To understand why we’re deferring, suppose someone gives you a Rubik’s cube (or some other challenging puzzle or game) for the first time, all scrambled up. You start to theorize about the best ways of solving it, how to understand it, and so on. But you never actually play around with it, getting familiar with how it behaves. Often the best way to understand something is to first use it, to get comfortable, to play a lot, and to do lots of informal experiments. As you build familiarity you understand why things are the way they are. At that point, you can go back and better understand the meaning of the basics. Well, we can think of quantum computing and quantum mechanics as an especially complicated type of puzzle! So the strategy we’re taking is to start with the mathematics of quantum computing – we’ll keep getting familiar with qubits and the quantum state, and developing the consequences. Doing that is how we’ll build up intuition, and will give us the chops needed to come back and think harder about the meaning of the quantum state. Part II: Introducing quantum logic gates In Part I we learned about the state of a qubit. But in order to quantum compute, it’s not enough just to understand quantum states. We need to be able to do things with them! We do that using quantum logic gates. A quantum logic gate is simply a way of manipulating quantum information, that is, the quantum state of a qubit or a collection of qubits. They’re analogous to the classical logic gates used in ordinary, everyday computers – gates such as the AND, OR, and NOT gates. And, much like classical gates’ role in conventional computers, quantum gates are the basic building blocks of quantum computation. They’re also a convenient way of describing many other quantum information processing tasks, such as quantum teleportation. In Part II of this essay we’ll discuss several types of quantum logic gate. As we’ll see, the gates we discuss are sufficient to do any quantum computation. In particular, much as any classical computation can be built up using AND, OR, and NOT gates, the quantum gates we describe over the next few sections suffice to do any quantum computation. Many of the quantum gates we’ll learn about are based on familiar classical logic gates. But a few are different. Those differences appear innocuous, almost trivial. But in those differences lies the power of quantum computation, and the possibility for quantum computers to be vastly superior to classical computers. Part II is, frankly, a bit of a slog. Learning quantum gates is like learning basic vocabulary in a new language: there’s no getting round spending a fair bit of time working on it. Still, the spaced-repetition testing should make this basic memory work much easier than is ordinarily the case. And it will prepare you well for some of the more conceptual issues discussed in Part III, where we return to high-level questions about the ultimate nature of computation, and what quantum computers are good for. The quantum NOT gate Let’s take a look at our very first quantum logic gate, the quantum NOT gate. As you can no doubt surmise, the quantum NOT gate is a generalization of the classical NOT gate. On the computational basis states the quantum NOT gate does just what you’d expect, mimicking the classical NOT gate. That is, it takes the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state to ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ , and vice versa: N O T ∣ 0 ⟩ = ∣ 1 ⟩ NOT|0\\rangle = |1\\rangle N O T ∣ 0 ⟩ = ∣ 1 ⟩ N O T ∣ 1 ⟩ = ∣ 0 ⟩ NOT|1\\rangle = |0\\rangle N O T ∣ 1 ⟩ = ∣ 0 ⟩ But the computational basis states aren’t the only states possible for a qubit. What happens when we apply the NOT gate to a general superposition state, that is, α ∣ 0 ⟩ + β ∣ 1 ⟩ \\alpha|0\\rangle+\\beta|1\\rangle α ∣ 0 ⟩ + β ∣ 1 ⟩ ? In fact, it does pretty much the simplest possible thing: it acts linearly on the quantum state, interchanging the role of ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ : N O T ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) = α ∣ 1 ⟩ + β ∣ 0 ⟩ . NOT (\\alpha|0\\rangle +\\beta|1\\rangle) = \\alpha|1\\rangle+\\beta |0\\rangle. N O T ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) = α ∣ 1 ⟩ + β ∣ 0 ⟩ . I’ve been using the notation NOT for the quantum NOT gate. But for historical reasons people working on quantum computing usually use a different notation, the notation X X X . And so the above may be rewritten: X ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) = α ∣ 1 ⟩ + β ∣ 0 ⟩ . X (\\alpha|0\\rangle +\\beta|1\\rangle) = \\alpha|1\\rangle+\\beta |0\\rangle. X ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) = α ∣ 1 ⟩ + β ∣ 0 ⟩ . I’ll use the terms X X X gate and NOT gate interchangeably. Historically, the notation X X X traces its origin to 1927, when the physicist Wolfgang Pauli introduced an operation s x s_x s x ​ (often written σ x \\sigma_x σ x ​ in textbooks today) to help describe rotations of certain objects around the x x x spatial axis. This operation later became of interest to people working on quantum computing. But by that point the s s s (and the connection to rotation) was irrelevant, and so s x s_x s x ​ just became X X X . What we’ve seen so far are very algebraic ways of writing down the way the X X X gate works. There’s an alternate representation, the quantum circuit representation. In a quantum circuit we depict an X X X gate as follows: The line from left to right is what’s called a quantum wire. A quantum wire represents a single qubit. The term “wire” and the way it’s drawn looks like the qubit is moving through space. But it's often helpful to instead think of left-to-right as representing the passage of time. So the initial segment of wire is just the passage of time, with nothing happening to the qubit. Then the X X X gate is applied. And then the quantum wire continues, leaving the desired output. Sometimes we’ll put the input and output states explicitly in the quantum circuit, so we have something like: So that’s the quantum circuit representation of the X X X gate. It is, in fact, our first quantum computation. A simple computation, involving just a single qubit and a single gate, but a genuine quantum computation nonetheless! There’s a third representation for the X X X gate that’s worth knowing about, a representation as a 2 × 2 2 \\times 2 2 × 2 matrix: X = [ 0 1 1 0 ] . X = \\left[ \\begin{array}{cc} 0 & 1 \\\\ 1 & 0 \\end{array} \\right]. X = [ 0 1 ​ 1 0 ​ ] . To understand in what sense this is a representation of the NOT gate, recall that α ∣ 0 ⟩ + β ∣ 1 ⟩ \\alpha|0\\rangle + \\beta|1\\rangle α ∣ 0 ⟩ + β ∣ 1 ⟩ is just the vector [ α β ] \\left[ \\begin{array}{c} \\alpha \\\\ \\beta \\end{array} \\right] [ α β ​ ] . And so we have: [ 0 1 1 0 ] ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) = [ 0 1 1 0 ] [ α β ] = [ β α ] = α ∣ 1 ⟩ + β ∣ 0 ⟩ = X ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) . \\begin{aligned} \\left[ \\begin{array}{cc} 0 & 1 \\\\ 1 & 0 \\end{array} \\right] (\\alpha|0\\rangle + \\beta|1\\rangle) & = \\left[ \\begin{array}{cc} 0 & 1 \\\\ 1 & 0 \\end{array} \\right] \\left[ \\begin{array}{c} \\alpha \\\\ \\beta \\end{array} \\right] \\\\ & = \\left[ \\begin{array}{c} \\beta \\\\ \\alpha \\end{array} \\right] \\\\ & = \\alpha|1\\rangle+\\beta|0\\rangle \\\\ & = X (\\alpha|0\\rangle + \\beta|1\\rangle). \\end{aligned} [ 0 1 ​ 1 0 ​ ] ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) ​ = [ 0 1 ​ 1 0 ​ ] [ α β ​ ] = [ β α ​ ] = α ∣ 1 ⟩ + β ∣ 0 ⟩ = X ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) . ​ This tells us that X X X and [ 0 1 1 0 ] \\left[ \\begin{array}{cc} 0 & 1 \\\\ 1 & 0 \\end{array} \\right] [ 0 1 ​ 1 0 ​ ] act in exactly the same way on all vectors, and thus are the same operation. In fact, as we’ll see later, it turns out that all quantum gates can be thought of as matrices, with the matrix entries specifying the exact details of the gate. By the way, regarding the X X X gate as a matrix clarifies what may have been a confusing point earlier. I wrote the X X X gate as having the action X ∣ 0 ⟩ = ∣ 1 ⟩ X|0\\rangle = |1\\rangle X ∣ 0 ⟩ = ∣ 1 ⟩ , X ∣ 1 ⟩ = ∣ 0 ⟩ X|1\\rangle = |0\\rangle X ∣ 1 ⟩ = ∣ 0 ⟩ . Implicitly – I never quite said this, though it’s true – the X X X gate is a mathematical function, taking input states to output states. But when we write functions we usually use parentheses, so why didn’t I write X ( ∣ 0 ⟩ ) X(|0\\rangle) X ( ∣ 0 ⟩ ) and similarly for ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ ? The reason is that for linear functions, i.e., matrices, it’s conventional to omit parentheses and just write X ∣ 0 ⟩ X|0\\rangle X ∣ 0 ⟩ – function application is just matrix multiplication. Didn’t remember Remembered Quantum wires: why the simplest quantum circuit is often also the hardest to implement We’ve now seen a simple quantum circuit and quantum logic gate. But it’s not quite the simplest possible quantum circuit. The simplest possible quantum circuit does nothing at all. That is, it’s a single quantum wire: This circuit is just a single qubit being preserved in time. To be more explicit, if some arbitrary quantum state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ is input to the circuit, then the exact same state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ is output (it’s common practice to use the Greek letter ψ \\psi ψ to denote an arbitrary quantum state): Mathematically, this circuit is trivial. But physically it’s far from trivial. In many physical systems, the quantum wire is actually one of the hardest quantum computations to implement! The reason is that quantum states are often incredibly fragile. If your qubit is being stored in some tiny system – perhaps a single photon or a single atom – then it’s very, very easy to disturb that state. It really doesn’t take much to upset an atom or a photon. And so while quantum wires are mathematically trivial, they can be one of the hardest elements to build in real systems. That said, there are some systems where quantum wires are easy to implement. If you store your qubit in a neutrino then the state will actually be pretty well preserved. The reason is that neutrinos barely interact with other forms of matter at all – a neutrino can easily pass through a mile of lead without being disturbed. But while it’s intriguing that neutrinos are so stable, it doesn’t mean they make good qubits. The trouble is that since there’s no easy way of using ordinary matter to interact with them, we can’t manipulate their quantum state in a controlled fashion, and so can’t implement a quantum gate. There’s a tension here that applies to many proposals to do quantum information processing, not just neutrinos. If we want to store the quantum state, then it’s helpful if our qubits only interact very weakly with other systems, so those systems don’t disrupt them. But if the qubits only interact weakly with other systems then that also makes it hard to manipulate the qubits. Thus, systems which make good quantum wires are often hard to build quantum gates for. Much of the art of designing quantum computers is about finding ways to navigate this tension. Often, that means trying to design systems which interact weakly most of the time, but some of the time can be caused to interact strongly, and so serve as part of a quantum gate. Didn’t remember Remembered You’ll note that some of the questions above have a different flavor to the questions earlier in the essay. Early questions had cut-and-dried answers. The answer to the question “What’s the standard notation for the quantum NOT gate?” is just: “ X X X ”. But some of the questions above have less well-specified answers. They’re a little fluffy. This fluffiness may cause you difficulties as you decide how to respond. Your answer to the question “Why is it that systems which make good quantum wires are often hard to build quantum gates for?” may not quite match the answer given. If this is the case, don’t worry. You should mark yourself correct if you’re confident you’ve understood the point, even in terms different from my phrasing. And mark yourself incorrect if the point still needs reinforcement. A multi-gate quantum circuit Let’s take a look at a simple multiple-gate quantum circuit. It’s just a circuit with two X X X gates in a row: It’s worth pausing for a moment, and trying to guess what this circuit does to the input state. It’s worth doing this even if you usually find this kind of guessing frustrating. Even when you get stuck, building up strategies for dealing with stuckness is part of learning any difficult subject. So take a minute or so now. We’ll try two different ways of figuring out what’s going on. Here’s one approach, based on applying X X X twice to an arbitrary input state, α ∣ 0 ⟩ + β ∣ 1 ⟩ \\alpha|0\\rangle+\\beta|1\\rangle α ∣ 0 ⟩ + β ∣ 1 ⟩ . It’s simple algebra, using the fact X X X interchanges the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ states: X ( X ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) ) = X ( α ∣ 1 ⟩ + β ∣ 0 ⟩ ) = α ∣ 0 ⟩ + β ∣ 1 ⟩ \\begin{aligned} X(X(\\alpha|0\\rangle+\\beta|1\\rangle)) & = X(\\alpha|1\\rangle + \\beta|0\\rangle) \\\\ & = \\alpha|0\\rangle + \\beta|1\\rangle \\end{aligned} X ( X ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) ) ​ = X ( α ∣ 1 ⟩ + β ∣ 0 ⟩ ) = α ∣ 0 ⟩ + β ∣ 1 ⟩ ​ So the net effect is to recover the original quantum state, no matter what that state was. In other words, this circuit is equivalent to a quantum wire: A second way of seeing this is based on the matrix representation we found earlier for the X X X gate. Observe that if the input is some arbitrary quantum state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ , then after the first gate the state is X ∣ ψ ⟩ X|\\psi\\rangle X ∣ ψ ⟩ , and after the second gate the state is X X ∣ ψ ⟩ X X |\\psi\\rangle X X ∣ ψ ⟩ . Then we observe that the product X X X X X X is X X = [ 0 1 1 0 ] [ 0 1 1 0 ] = [ 1 0 0 1 ] . \\begin{aligned} XX & = \\left[ \\begin{array}{cc} 0 & 1 \\\\ 1 & 0 \\end{array} \\right] \\left[ \\begin{array}{cc} 0 & 1 \\\\ 1 & 0 \\end{array} \\right] \\\\ & = \\left[ \\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array} \\right]. \\end{aligned} X X ​ = [ 0 1 ​ 1 0 ​ ] [ 0 1 ​ 1 0 ​ ] = [ 1 0 ​ 0 1 ​ ] . ​ That is, X X XX X X is the identity operation, and so the output X X ∣ ψ ⟩ XX|\\psi\\rangle X X ∣ ψ ⟩ is just the original input ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ . In other words, two X X X gates is the same as a quantum wire. And so we’ve arrived at the same conclusion in a different way, using matrix multiplication. Doing such matrix multiplications is a pretty common way of analyzing quantum circuits. Didn’t remember Remembered The Hadamard gate We’ve seen our first quantum gate, the NOT or X X X gate. Of course, the X X X didn’t appear to do all that much beyond what is possible with a classical NOT gate. In this section I introduce a gate that clearly involves quantum effects, the Hadamard gate. As with the X X X gate, we’ll start by explaining how the Hadamard gate acts on computational basis states. Denoting the gate by H H H , here’s what it does: H ∣ 0 ⟩ = ∣ 0 ⟩ + ∣ 1 ⟩ 2 H|0\\rangle = \\frac{|0\\rangle +|1\\rangle}{\\sqrt 2} H ∣ 0 ⟩ = 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ H ∣ 1 ⟩ = ∣ 0 ⟩ − ∣ 1 ⟩ 2 H|1\\rangle = \\frac{|0\\rangle -|1\\rangle}{\\sqrt 2} H ∣ 1 ⟩ = 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ Of course, ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ aren’t the only quantum states. How does the Hadamard gate act on more general quantum states? It won’t surprise you to learn that it acts linearly, as did the quantum NOT gate. In particular, the Hadamard gate takes a superposition α ∣ 0 ⟩ + β ∣ 1 ⟩ \\alpha|0\\rangle + \\beta|1\\rangle α ∣ 0 ⟩ + β ∣ 1 ⟩ to the corresponding superposition of outputs: H ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) = α ( ∣ 0 ⟩ + ∣ 1 ⟩ 2 ) + β ( ∣ 0 ⟩ − ∣ 1 ⟩ 2 ) . H(\\alpha|0\\rangle + \\beta|1\\rangle) = \\alpha \\left( \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} \\right) + \\beta \\left( \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} \\right). H ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) = α ( 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ ) + β ( 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ ) . That’s a mess. We can make it less of a mess by combining the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ terms together, and also the ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ terms together: H ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) = α + β 2 ∣ 0 ⟩ + α − β 2 ∣ 1 ⟩ . H(\\alpha|0\\rangle + \\beta|1\\rangle) = \\frac{\\alpha+\\beta}{\\sqrt 2} |0\\rangle + \\frac{\\alpha-\\beta}{\\sqrt 2}|1\\rangle. H ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) = 2 ​ α + β ​ ∣ 0 ⟩ + 2 ​ α − β ​ ∣ 1 ⟩ . That’s better, but still not pretty! Fortunately, we mostly won’t be dealing with such complex expressions. The only reason I’ve done it here is to be really explicit. Instead of dealing with such explicit expressions, we’ll mostly work with the H H H gate in its circuit and matrix representations (see below). Those let us focus at a more enlightening level of abstraction, rather than messing around with coefficients. Indeed, much work on quantum computing is about attempting to develop ways of moving from low levels of abstraction to higher, more conceptual levels. Up to now most of our work has been at a very low level, seeming more an exercise in linear algebra than a discussion of a new model of computing. That perhaps seems strange. After all, if you were explaining classical computers to someone, you wouldn’t start in the weeds, with AND and NOT gates and the like. You’d start with a well-designed high-level programming language, and then bounce back and forth between different layers of abstraction. Modern computers aren’t just about logic gates – they’re at least as much about beautiful higher-level ideas: say, lazy evaluation, or higher-order functions, or homoiconicity, and so on. I wish I could start with high-level abstractions for quantum computers. However, we’re still in the early days of quantum computing, and for the most part humanity hasn’t yet discovered such high-level abstractions. People are still scratching around, trying to find good ideas. That’s an exciting situation: it means almost all the big breakthroughs are ahead. There’s a sense in which we still understand very little about quantum computing. That might sound surprising: after all, there are great big fat textbooks on the subject. But you could have written a great big fat textbook about the ENIAC computer in the late 1940s. It was, after all, a very complex system. That textbook would have looked intimidating, but it wouldn’t have been the final word in computing. For the most part the way we understand quantum computing today is at an ENIAC-like level, looking at the nuts-and-bolts of qubits and logic gates and linear algebra, and wondering what the higher-level understanding may be. The situation can be thought of as much like programming language design before the breakthroughs that led to languages such as Lisp and Haskell and Prolog and Smalltalk. That makes it a remarkable creative opportunity, a challenge for the decades and centuries ahead. Speaking of nuts-and-bolts, let’s get back to the Hadamard gate. Here’s the circuit representation for the Hadamard gate. It looks just like the X X X gate in the circuit representation, except we change the gate label to H H H : Just like the X X X gate, H H H has a matrix representation: H = 1 2 [ 1 1 1 − 1 ] . H = \\frac{1}{\\sqrt 2} \\left[ \\begin{array}{cc} 1 & 1 \\\\ 1 & -1 \\end{array} \\right]. H = 2 ​ 1 ​ [ 1 1 ​ 1 − 1 ​ ] . To see that this matrix representation is correct, let’s check the action of the matrix on the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and the ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ states. Here we check for the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state: 1 2 [ 1 1 1 − 1 ] ∣ 0 ⟩ = 1 2 [ 1 1 1 − 1 ] [ 1 0 ] = 1 2 [ 1 1 ] = ∣ 0 ⟩ + ∣ 1 ⟩ 2 . \\begin{aligned} \\frac{1}{\\sqrt 2} \\left[ \\begin{array}{cc} 1 & 1 \\\\ 1 & -1 \\end{array} \\right] |0\\rangle & = \\frac{1}{\\sqrt 2} \\left[ \\begin{array}{cc} 1 & 1 \\\\ 1 & -1 \\end{array} \\right] \\left[ \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right] \\\\ & = \\frac{1}{\\sqrt 2} \\left[ \\begin{array}{c} 1 \\\\ 1 \\end{array} \\right] \\\\ & = \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2}. \\end{aligned} 2 ​ 1 ​ [ 1 1 ​ 1 − 1 ​ ] ∣ 0 ⟩ ​ = 2 ​ 1 ​ [ 1 1 ​ 1 − 1 ​ ] [ 1 0 ​ ] = 2 ​ 1 ​ [ 1 1 ​ ] = 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ . ​ That is, this matrix acts the same way as the Hadamard gate on the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state. Now let’s check on the ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ state: 1 2 [ 1 1 1 − 1 ] ∣ 1 ⟩ = 1 2 [ 1 1 1 − 1 ] [ 0 1 ] = 1 2 [ 1 − 1 ] = ∣ 0 ⟩ − ∣ 1 ⟩ 2 . \\begin{aligned} \\frac{1}{\\sqrt 2} \\left[ \\begin{array}{cc} 1 & 1 \\\\ 1 & -1 \\end{array} \\right] |1\\rangle & = \\frac{1}{\\sqrt 2} \\left[ \\begin{array}{cc} 1 & 1 \\\\ 1 & -1 \\end{array} \\right] \\left[ \\begin{array}{c} 0 \\\\ 1 \\end{array} \\right] \\\\ & = \\frac{1}{\\sqrt 2} \\left[ \\begin{array}{c} 1 \\\\ -1 \\end{array} \\right] \\\\ & = \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2}. \\end{aligned} 2 ​ 1 ​ [ 1 1 ​ 1 − 1 ​ ] ∣ 1 ⟩ ​ = 2 ​ 1 ​ [ 1 1 ​ 1 − 1 ​ ] [ 0 1 ​ ] = 2 ​ 1 ​ [ 1 − 1 ​ ] = 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ . ​ So the matrix acts the same way as the Hadamard gate on both the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ states. By the linearity of matrix multiplication it follows that the matrix acts the same way as the Hadamard on all input states, and so they are the same operation. What makes the Hadamard gate interesting as a quantum gate? What can we use it to do? We don’t (quite) have enough background to give precise answers to these questions yet. But there is an analogy which gives insight. Imagine you are living in North Africa, thousands of years in the past, and decide for some reason that you want to get over to the Iberian peninsula. If you don’t yet have boats or some other reliable method of moving across large bodies of water, you need to go all the way across Africa, past the Arabian peninsula, up and around through Europe, back to the Iberian peninsula: Original photo source: Reto Stöckli, NASA Earth Observatory (2004). Suppose, however, that you invent a new device, the boat, which expands the range of locations you can traverse. Then you can take a much more direct route over to the Iberian peninsula, greatly cutting down the time required: What the Hadamard and similar gates do is expand the range of operations that it’s possible for a computer to perform. That expansion makes it possible for the computer to take shortcuts, as the computer “moves” in a way that’s not possible in a conventional classical computer. And, we hope, that may enable us to solve some computational problems faster. Another helpful analogy is to the game of chess. Imagine you’re playing chess and the rules are changed in your favor, enabling your rook an expanded range of moves. That extra flexibility might enable you to achieve checkmate much faster because you can get to new positions much more quickly. A similar thing is going on with the Hadamard gate. By expanding the range of states we can access (or, more precisely, the range of dynamical operations we can generate) beyond what’s possible on a classical computer, it becomes possible to take shortcuts in our computation. We’ll see explicit examples in subsequent essays. To get more familiar with the Hadamard gate, let’s analyze a simple circuit: What’s this circuit do? Before we compute, it’s worth pausing for a second to try guessing the result. The point of guessing isn’t to get it right – rather, it’s to challenge yourself to start coming up with heuristic mental models for thinking about what’s going on in quantum circuits. Those mental models likely won’t be very good at first, but that’s okay – if you keep doing this, they’ll get better. Here’s one heuristic you can use to think about this circuit: you can think of H H H as sort of mixing the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ states together. So if you apply H H H twice to ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ , perhaps it would thoroughly mix the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ up together. What sort of state do you think would result, according to this heuristic? Do you believe the result? Why or why not? Can you think of other heuristics that might help you guess an answer? Alright, let’s compute what actually happens. After we apply the first Hadamard to ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ we get ∣ 0 ⟩ + ∣ 1 ⟩ 2 . \\frac{|0\\rangle+|1\\rangle}{\\sqrt{2}}. 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ . Then we apply a second Hadamard gate. This takes the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ term above to ∣ 0 ⟩ + ∣ 1 ⟩ 2 \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ and the ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ term to ∣ 0 ⟩ − ∣ 1 ⟩ 2 \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ , so the output is: 1 2 ( ∣ 0 ⟩ + ∣ 1 ⟩ 2 + ∣ 0 ⟩ − ∣ 1 ⟩ 2 ) . \\frac{1}{\\sqrt 2} \\left( \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} + \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} \\right). 2 ​ 1 ​ ( 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ + 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ ) . If you look at the expression above, you’ll notice that the ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ terms cancel each other out, so you’re just left with the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ terms. Collecting them up, we’re left with the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state, same as we started with: 1 2 ( ∣ 0 ⟩ 2 + ∣ 0 ⟩ 2 ) = ∣ 0 ⟩ . \\frac{1}{\\sqrt 2} \\left(\\frac{|0\\rangle}{\\sqrt 2}+ \\frac{|0\\rangle}{\\sqrt 2} \\right) = |0\\rangle. 2 ​ 1 ​ ( 2 ​ ∣ 0 ⟩ ​ + 2 ​ ∣ 0 ⟩ ​ ) = ∣ 0 ⟩ . In a similar fashion, after we run the ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ state through the first Hadamard gate, we get: ∣ 0 ⟩ − ∣ 1 ⟩ 2 . \\frac{|0\\rangle-|1\\rangle}{\\sqrt{2}}. 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ . Then we apply a second Hadamard gate to get: 1 2 ( ∣ 0 ⟩ + ∣ 1 ⟩ 2 − ∣ 0 ⟩ − ∣ 1 ⟩ 2 ) . \\frac{1}{\\sqrt 2} \\left( \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} - \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} \\right). 2 ​ 1 ​ ( 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ − 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ ) . This time it’s the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ terms which cancel out, and the ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ terms reinforce. When we collect up these terms, we see that the output is just the ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ state, same as we started with. And so both the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ states are left unchanged by this quantum circuit, and the net effect of the circuit is exactly the same as a quantum wire: There’s an alternate way of seeing this, which I’ll sketch out without working through in detail. It’s to note that if we input an arbitrary quantum state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ to the circuit, then the output must be H H ∣ ψ ⟩ H H |\\psi\\rangle H H ∣ ψ ⟩ , i.e., the result of applying two H H H matrices to ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ . But if you just compute the matrix product H H H H H H it turns out to be the 2 × 2 2 \\times 2 2 × 2 identity matrix, H H = I H H = I H H = I . And so the output from the circuit must be the same as the input, H H ∣ ψ ⟩ = I ∣ ψ ⟩ = ∣ ψ ⟩ HH|\\psi\\rangle = I |\\psi\\rangle = |\\psi\\rangle H H ∣ ψ ⟩ = I ∣ ψ ⟩ = ∣ ψ ⟩ , just as from a quantum wire. Of course, this result violates our intuitive guess, which was that two Hadamards would thoroughly mix up the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and the ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ . It’s interesting to ponder what went wrong with our intuition, say by looking through the calculation for H H H acting twice on ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ . You see that after the second gate the ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ terms exactly cancel one another out, while the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ terms reinforce one another. This seems innocuous, almost like a mathematical accident. Still, I draw your attention to it because this type of cancellation or reinforcement is crucial in many algorithms for quantum computers. Without getting into details, the rough way many such algorithms work is to first use Hadamard gates to “spread out” in quantum states like ∣ 0 ⟩ + ∣ 1 ⟩ 2 \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ (or many-qubit analogs), i.e., in superpositions of multiple computational basis states. At the end of the algorithm they use clever patterns of cancellation and reinforcement to bring things back together again into one (or possibly a few, in the many-qubit case) computational basis state, containing the desired answer. That’s a somewhat vague and perhaps tantalizing description, but the point to take away is that the kind of cancellation-or-reinforcement we saw above is actually crucial in many quantum computations. I’ll now pose a few simple exercises related to the Hadamard gate. Unlike the spaced-repetition questions, the point of the exercises below isn’t as an aid to memory, and so you won’t see these exercises repeatedly. Rather, they’re here because (should you choose to work through them) they will help you better understand the material of the essay. But they’ll only incidentally help with memorization. We’ll follow them with some spaced-repetition questions. Note that even if you don't work through the exercises, it's worth at least reading through them, since some of the results will be tested in the spaced-repetition questions. Exercise: Verify that H H = I HH = I H H = I , where I I I is the 2 × 2 2 \\times 2 2 × 2 identity matrix, I = [ 1 0 0 1 ] I = \\left[ \\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array} \\right] I = [ 1 0 ​ 0 1 ​ ] . Exercise: Suppose that instead of H H H we’d defined a matrix J J J by: J : = 1 2 [ 1 1 1 1 ] J := \\frac{1}{\\sqrt 2} \\left[ \\begin{array}{cc} 1 & 1 \\\\ 1 & 1 \\end{array} \\right] J : = 2 ​ 1 ​ [ 1 1 ​ 1 1 ​ ] At first, it might seem that J J J would make an interesting quantum gate, along lines similar to H H H . For instance, J ∣ 0 ⟩ = ∣ 0 ⟩ + ∣ 1 ⟩ 2 J|0\\rangle = \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} J ∣ 0 ⟩ = 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ , and J ∣ 1 ⟩ = ∣ 0 ⟩ + ∣ 1 ⟩ 2 J|1\\rangle = \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} J ∣ 1 ⟩ = 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ . These are both good, normalized quantum states. But what happens if we apply J J J to the quantum state ∣ 0 ⟩ − ∣ 1 ⟩ 2 \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ ? Why does this make J J J unsuitable for use as a quantum gate? Exercise: Consider the quantum circuit: Explain why the output from this circuit is X H ∣ ψ ⟩ XH|\\psi\\rangle X H ∣ ψ ⟩ , not H X ∣ ψ ⟩ HX|\\psi\\rangle H X ∣ ψ ⟩ , as you might naively assume if you wrote down gates in the order they occur in the circuit. This is a common gotcha to be aware of – it occurs because quantum gates compose left-to-right in the circuit representation, while matrix multiplications compose right-to-left. Didn’t remember Remembered Measuring a qubit Suppose a (hypothetical!) quantum physicist named Alice prepares a qubit in her laboratory, in a quantum state α ∣ 0 ⟩ + β ∣ 1 ⟩ \\alpha|0\\rangle+\\beta|1\\rangle α ∣ 0 ⟩ + β ∣ 1 ⟩ . Then she gives her qubit to another quantum physicist, Bob, but doesn’t tell him the values of α \\alpha α and β \\beta β . Is there some way Bob can figure out α \\alpha α and β \\beta β ? That is, is there some experiment Bob can do to figure out the identity of the quantum state? The surprising answer to this question turns out to be NO! There is, in fact, no way to figure out α \\alpha α and β \\beta β if they start out unknown. To put it a slightly different way, the quantum state of any system – whether it be a qubit or a some other system – is not directly observable. I say this is surprising, because it’s very different from our usual everyday way of thinking about how the world works. If there’s something wrong with your car, a mechanic can use diagnostic tools to learn about the internal state of the engine. The better the diagnostic tools, the more they can learn. Of course, there may be parts of the engine that would be impractical to access – maybe they’d have to break a part, or use a microscope, for instance. But you’d probably be rather suspicious if the mechanic told you the laws of physics prohibited them from figuring out the internal state of the engine. Similarly, when you first start learning about quantum circuits, it seems like we should be able to observe the amplitudes of a quantum state whenever we like. But that turns out to be prohibited by the laws of physics. Those amplitudes are better thought of as a kind of hidden information. So, what can we figure out from the quantum state? Rather than somehow measuring α \\alpha α and β \\beta β , there are other ways of getting useful information out of a qubit. Let me describe an especially important process called measurement in the computational basis. This is a fundamental primitive in quantum computing: it’s the way we typically extract information from our quantum computers. I’ll explain now how it works for a single qubit, and later generalize to multi-qubit systems. Suppose a qubit is in the state α ∣ 0 ⟩ + β ∣ 1 ⟩ \\alpha|0\\rangle+\\beta|1\\rangle α ∣ 0 ⟩ + β ∣ 1 ⟩ . When you measure this qubit in the computational basis it gives you a classical bit of information: it gives you the outcome 0 0 0 with probability ∣ α ∣ 2 |\\alpha|^2 ∣ α ∣ 2 , and the outcome 1 1 1 with probability ∣ β ∣ 2 |\\beta|^2 ∣ β ∣ 2 . To think a little more concretely about this process, suppose your qubit is instantiated in some physical system. Perhaps it’s being stored in the state of an atom somehow. It doesn’t matter exactly what, but you have this qubit in your laboratory. And you have some measurement apparatus, probably something large and complicated, maybe involving lasers and microprocessors and a screen for readout of the measurement result. And this measurement apparatus interacts in some way with your qubit. After the measurement interaction, your measurement apparatus registers an outcome. For instance, it might be that you get the outcome 0 0 0 . Or maybe instead you get the outcome 1 1 1 . The crucial fact is that the outcome is ordinary classical information – the stuff you already know how to think about – which you can then use to do other things, and to control other processes. So the way a quantum computation works is that we manipulate a quantum state using a series of quantum gates, and then at the end of the computation (typically) we do a measurement to read out the result of the computation. If our quantum computer is just a single qubit, then that result will be a single classical bit. If, as is more usually the case, it’s multiple qubits, then the measurement result will be multiple classical bits. A fundamental fact about this measurement process is that it disturbs the state of the quantum system. In particular, it doesn’t just leave the quantum state alone. After the measurement, if you get the outcome 0 0 0 then the state of the qubit afterwards (the “posterior state”) is the computational basis state ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ . On the other hand, if you get the outcome 1 1 1 then the posterior state of the qubit is the computational basis state ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ . Summing all this up: if we measure a qubit with state α ∣ 0 ⟩ + β ∣ 1 ⟩ \\alpha |0\\rangle+\\beta|1\\rangle α ∣ 0 ⟩ + β ∣ 1 ⟩ in the computational basis, then the outcome is a classical bit: either 0 0 0 , with probability ∣ α ∣ 2 |\\alpha|^2 ∣ α ∣ 2 , or 1 1 1 , with probability ∣ β ∣ 2 |\\beta|^2 ∣ β ∣ 2 . The corresponding state of the qubit after the measurement is ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ or ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ . A key point to note is that after the measurement, no matter what the outcome, α \\alpha α and β \\beta β are gone. No matter whether the posterior state is ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ or ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ , there is no trace of α \\alpha α or β \\beta β . And so you can’t get any more information about them. In that sense, α \\alpha α and β \\beta β are a kind of hidden information – the measurement doesn’t tell you what they were. One reason this is important is because it means you can’t store an infinite amount of classical information in a qubit. After all, α \\alpha α is a complex number, and you could imagine storing lots of classical bits in the binary expansion of the real component of α \\alpha α . If there was some experimental way you could measure the value of α \\alpha α exactly, then you could extract that classical information. But without a way of measuring α \\alpha α that’s not possible. I’ve been talking about measurement in the computational basis. In fact, there are other types of measurement you can do in quantum systems. But there’s a sense in which computational basis measurements turn out to be fundamental. The reason is that by combining computational basis measurements with quantum gates like the Hadamard and NOT (and other) gates, it’s possible to simulate arbitrary quantum measurements. So this is all you absolutely need to know about measurement, from an in-principle point of view. It’s useful to have a way of denoting measurements in the quantum circuit model. Here’s a simple example: It’s a single-qubit quantum circuit, with input the state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ . A NOT gate is applied, followed by a Hadamard gate. The circuit finishes with a measurement in the computational basis, denoted by the slightly elongated semi-circle. The m m m is a classical bit denoting the measurement result – either 0 0 0 or 1 1 1 – and we use the double wire to indicate the classical bit m m m going off and being used to do something else. Of course, I said above that after measurement the qubit is in either the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ or the ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ state. You might think we’d draw a corresponding quantum wire coming out the other side of the measurement. But often in quantum circuits the qubit is discarded after measurement, and that’s assumed by this notation. One final comment on measurement is that it’s connected to the normalization condition for quantum states that we discussed earlier. Suppose we have the quantum state: α ∣ 0 ⟩ + β ∣ 1 ⟩ \\alpha|0\\rangle+\\beta|1\\rangle α ∣ 0 ⟩ + β ∣ 1 ⟩ Then the probability of the two possible measurement outcomes, 0 0 0 and 1 1 1 , must sum to 1 1 1 , and so we have: ∣ α ∣ 2 + ∣ β ∣ 2 = 1. |\\alpha|^2+|\\beta|^2 = 1. ∣ α ∣ 2 + ∣ β ∣ 2 = 1 . This is exactly the normalization condition for quantum states – i.e., the quantum state must have length 1 1 1 . The origin of that constraint is really just the fact that measurement probabilities must add up to 1 1 1 . Exercise: Suppose we’ve been given either the state ∣ 0 ⟩ + ∣ 1 ⟩ 2 \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ or the state ∣ 0 ⟩ − ∣ 1 ⟩ 2 \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ , but not told which state we’ve been given. We’d like to figure out which state we’ve been given. If we just do a computational basis measurement, then for both states we get outcome 0 0 0 with probability 1 2 \\frac 12 2 1 ​ , and outcome 1 1 1 with probability 1 2 \\frac 12 2 1 ​ . So we can’t distinguish the states directly using a computational basis measurement. But suppose instead we put the state into the following circuit: Show that if the state ∣ 0 ⟩ + ∣ 1 ⟩ 2 \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ is input then the output is m = 0 m = 0 m = 0 with probability 1 1 1 , while if the state ∣ 0 ⟩ − ∣ 1 ⟩ 2 \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ is input then the output is m = 1 m = 1 m = 1 with probability 1 1 1 . Thus while these two states are indistinguishable if just measured in the computational basis, they can be distinguished with the help of a simple quantum circuit. Didn’t remember Remembered General single-qubit gates So far we’ve learned about two quantum gates, the NOT and the Hadamard gate, and also about the measurement process that can be used to extract classical information from our quantum circuits. In this section, I return to quantum gates, and take a look at the most general single-qubit gate. To do that it helps to recall the matrix representations of the NOT and Hadamard gates: X = [ 0 1 1 0 ] ; H = 1 2 [ 1 1 1 − 1 ] \\begin{aligned} X = \\left[ \\begin{array}{cc} 0 & 1 \\\\ 1 & 0 \\end{array} \\right]; \\,\\,\\,\\, H = \\frac{1}{\\sqrt 2} \\left[ \\begin{array}{cc} 1 & 1 \\\\ 1 & -1 \\end{array} \\right] \\end{aligned} X = [ 0 1 ​ 1 0 ​ ] ; H = 2 ​ 1 ​ [ 1 1 ​ 1 − 1 ​ ] ​ If the input to these gates is the quantum state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ , then the output is X ∣ ψ ⟩ X|\\psi\\rangle X ∣ ψ ⟩ and H ∣ ψ ⟩ H|\\psi\\rangle H ∣ ψ ⟩ respectively. A general single-qubit gate works similarly. In particular, a general single-qubit gate can be represented as a 2 × 2 2 \\times 2 2 × 2 unitary matrix, U U U . (If you’re rusty, I’ll remind you what it means for a matrix to be unitary in a moment, and you can just think of it as a matrix for now.) If the input to the gate is the state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ then the output from the gate is U ∣ ψ ⟩ U|\\psi\\rangle U ∣ ψ ⟩ . And so the NOT and Hadamard gates correspond to the special cases where U = X U = X U = X and U = H U = H U = H , respectively. What does it mean for a matrix U U U to be unitary? It’s easiest to answer this question algebraically, where it simply means that U † U = I U^\\dagger U = I U † U = I , that is, the adjoint of U U U , denoted U † U^\\dagger U † , times U U U , is equal to the identity matrix. That adjoint is, recall, the complex transpose of U U U : U † : = ( U T ) ∗ . U^\\dagger := (U^T)^*. U † : = ( U T ) ∗ . So for a 2 × 2 2 \\times 2 2 × 2 matrix, the adjoint operation is just: [ a b c d ] † = [ a ∗ c ∗ b ∗ d ∗ ] . \\left[ \\begin{array}{cc} a & b \\\\ c & d \\end{array} \\right]^\\dagger = \\left[ \\begin{array}{cc} a^* & c^* \\\\ b^* & d^* \\end{array} \\right]. [ a c ​ b d ​ ] † = [ a ∗ b ∗ ​ c ∗ d ∗ ​ ] . (Note that the † \\dagger † is also sometimes called the dagger operation, or Hermitian conjugation, or just the conjugation operation. We’ll use all three terms on occasion.) There are a few basic questions you might ask: why are single-qubit gates described by unitary matrices? And how can we get an intuitive feel for what it means for a matrix to be unitary, anyway? While the equation U † U = I U^\\dagger U = I U † U = I is easy to check algebraically, we’d like some intuition for what that equation means. Another natural question is whether the NOT gate and the Hadamard gate are unitary? Of course, we’ll see that they are – I wouldn’t have described them as quantum gates if not – but we should go to the trouble of checking. Yet another good question is whether there are useful examples of single-qubit gates that aren’t the NOT or Hadamard gates? The equation U † U = I U^\\dagger U = I U † U = I is all very well, but it’d be nice to have more concrete examples than just an abstract equation. We’ll answer all these questions over the next few sections. Let’s start by checking the unitarity of the Hadamard gate. We start by computing the adjoint of H H H : H † = ( ( 1 2 [ 1 1 1 − 1 ] ) T ) ∗ . H^\\dagger = \\left( \\left( \\frac{1}{\\sqrt 2} \\left[ \\begin{array}{cc} 1 & 1 \\\\ 1 & -1 \\end{array} \\right]\\right)^T\\right)^*. H † = ( ( 2 ​ 1 ​ [ 1 1 ​ 1 − 1 ​ ] ) T ) ∗ . Note that taking the transpose doesn’t change the matrix, since it is already symmetric. And taking the complex conjugate doesn’t change anything, since all the entries are real. So we have H † = H H^\\dagger = H H † = H , and thus H † H = H H H^\\dagger H = HH H † H = H H . But we saw earlier in the essay that H H = I HH = I H H = I . So H H H is, indeed, unitary. Exercise: Show that X X X is unitary. Exercise: Show that the identity matrix I I I is unitary. Exercise: Can you find an example of a 2 × 2 2 \\times 2 2 × 2 matrix that is unitary, and is not I I I , X X X , or H H H ? For flavor, let’s give a few more examples of single-qubit quantum gates. Earlier in the essay, I mentioned that the NOT gate X X X was introduced by the physicist Wolfgang Pauli in the early days of quantum mechanics. He introduced two other matrices, Y Y Y and Z Z Z , which are also useful quantum gates. The three gates, X , Y X, Y X , Y , and Z Z Z are known collectively as the Pauli matrices. The Y Y Y and Z Z Z gates will be useful extra tools in our toolkit of quantum gates; in terms of the earlier analogy they expand the repertoire of moves we have available to us. They’re crucial, for example, in protocols such as quantum teleportation and quantum error correction. The Y Y Y gate is similar to the X X X gate, but instead of 1 1 1 s on the off-diagonal, it has i i i and − i -i − i , so it takes ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ to i ∣ 1 ⟩ i|1\\rangle i ∣ 1 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ to − i ∣ 0 ⟩ -i|0\\rangle − i ∣ 0 ⟩ : Y : = [ 0 − i i 0 ] . Y := \\left[ \\begin{array}{cc} 0 & -i \\\\ i & 0 \\end{array} \\right]. Y : = [ 0 i ​ − i 0 ​ ] . The Z Z Z gate leaves ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ unchanged, and takes ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ to − ∣ 1 ⟩ -|1\\rangle − ∣ 1 ⟩ : Z : = [ 1 0 0 − 1 ] . Z := \\left[ \\begin{array}{cc} 1 & 0 \\\\ 0 & -1 \\end{array} \\right]. Z : = [ 1 0 ​ 0 − 1 ​ ] . Exercise: Show that the Y Y Y and Z Z Z matrices are unitary, and so legitimate quantum gates. Another good example of a quantum gate is a rotation, the kind of matrix you’ve most likely been seeing since high school. It’s just the ordinary rotation of the 2 2 2 - dimensional plane by an angle θ \\theta θ : [ cos ⁡ ( θ ) − sin ⁡ ( θ ) sin ⁡ ( θ ) cos ⁡ ( θ ) ] . \\left[ \\begin{array}{cc} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{array} \\right]. [ cos ( θ ) sin ( θ ) ​ − sin ( θ ) cos ( θ ) ​ ] . You can easily check that this is a unitary matrix, and so it’s valid quantum gate. And sometimes it’s a very useful quantum gate! That mostly wraps up our little library of single-qubit gates. In the next couple of sections we’ll build up some intuition about what unitarity means, but we won’t add any extra elements to our quantum computing model. In fact, at this point we know almost everything needed for quantum computation. There are just two extra elements needed: extending our description of single qubits to multiple qubits, and describing a simple two-qubit quantum gate. We’ll get to those things shortly – not surprisingly, they look much like what we’ve already seen. Didn’t remember Remembered I am, by the way, somewhat uncomfortable with some of the questions just asked. My personal experience is that spaced-repetition learning works well when learning facts I have a lot of context for, and care a lot about. It’s a rare person who finds the detailed entries in a unitary matrix fascinating! That’s not to say they aren’t – actually, they are, but you need a lot more context than I’ve provided to see why (for instance) that − i -i − i in the Y Y Y is just so darn interesting. With that said, this essay is genuinely an experiment, and the questions above are included in that spirit. Maybe it will turn out that readers can use spaced-repetition to learn the entries of unitary matrices. And maybe they cannot. No matter which that’ll be a valuable thing to learn about the world, and to inform future experiments with spaced-repetition learning. What does it mean for a matrix to be unitary? Can we get an intuition for what it means for a matrix to be unitary? It turns out that unitary matrices preserve the length of their inputs. In other words, if we take any vector ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ and compute the length ∥ U ∣ ψ ⟩ ∥ \\| U|\\psi\\rangle \\| ∥ U ∣ ψ ⟩ ∥ it’s always equal to the length ∥ ∣ ψ ⟩ ∥ \\||\\psi\\rangle\\| ∥ ∣ ψ ⟩ ∥ of the original vector. In this, they’re much like rotations or reflections in ordinary (real) space, which also don’t change lengths. In a sense, the unitary matrices are a complex generalization of real rotations and reflections. Up to now we’ve been dealing mostly with vectors which are states of qubits, i.e., normalized 2-dimensional vectors. But in fact the statement of the last paragraph is true for d × d d \\times d d × d unitary matrices, too, i.e., ∥ U ∣ ψ ⟩ ∥ = ∥ ∣ ψ ⟩ ∥ \\|U|\\psi\\rangle\\| = \\||\\psi\\rangle\\| ∥ U ∣ ψ ⟩ ∥ = ∥ ∣ ψ ⟩ ∥ is true for any vector ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ in d d d dimensions. The proof is actually pretty simple – mostly, we just compute ∥ U ∣ ψ ⟩ ∥ \\|U|\\psi\\rangle \\| ∥ U ∣ ψ ⟩ ∥ and use a little algebra to check that it’s equal to the length ∥ ∣ ψ ⟩ ∥ \\||\\psi\\rangle\\| ∥ ∣ ψ ⟩ ∥ . But before we get to the proof, observe that this result is good news for quantum gates. The reason is that in a quantum gate the input state will be normalized (have length 1 1 1 ), since that’s a requirement for a quantum state. And we’d expect the corresponding output state to also be normalized, otherwise it wouldn’t be a legitimate quantum state. Fortunately, the length-preserving property of unitary matrices ensures the output state is properly normalized. Recall also the larger story this is part of: not only are gates unitary, to ensure that states remain normalized; also, quantum states are normalized, since measurement probabilities must sum to 1 1 1 . It all fits together. In fact it turns out that unitary matrices are the only matrices which preserve length in this way. And so a good way of thinking about unitary matrices is that they’re exactly the class of matrices which preserve length. That’s the geometric interpretation (and intuitive meaning) of the algebraic condition U † U = I U^\\dagger U = I U † U = I . Let’s prove now that unitary matrices really are length-preserving. The proof is straightforward. The main thing is to compute ∥ U ∣ ψ ⟩ ∥ \\|U|\\psi\\rangle\\| ∥ U ∣ ψ ⟩ ∥ . Actually, it’s a little easier to compute the square of the length, ∥ U ∣ ψ ⟩ ∥ 2 \\|U|\\psi\\rangle\\|^2 ∥ U ∣ ψ ⟩ ∥ 2 . This is just the sum of the squares of the absolute values of the components of the vector U ∣ ψ ⟩ U|\\psi\\rangle U ∣ ψ ⟩ : ∥ U ∣ ψ ⟩ ∥ 2 = ∑ j ( U ψ ) j ∗ ( U ψ ) j \\|U|\\psi\\rangle\\|^2 = \\sum_j (U\\psi)_j^* (U\\psi)_j ∥ U ∣ ψ ⟩ ∥ 2 = j ∑ ​ ( U ψ ) j ∗ ​ ( U ψ ) j ​ Note that I’ve used ( U ψ ) j (U\\psi)_j ( U ψ ) j ​ to denote the j j j t h component of U ∣ ψ ⟩ U|\\psi\\rangle U ∣ ψ ⟩ , dropping the full ket notation, and just using ψ \\psi ψ alone. To proceed, we’ll expand the equation above out, and look for opportunities to apply the unitarity of U U U . In particular, the component ( U ψ ) j (U\\psi)_j ( U ψ ) j ​ is given by ∑ l U j l ψ l \\sum_l U_{jl} \\psi_l ∑ l ​ U j l ​ ψ l ​ and similarly for the complex conjugate term. So we can rewrite the above equation as: ∥ U ∣ ψ ⟩ ∥ 2 = ∑ j k l U j k ∗ ψ k ∗ U j l ψ l \\|U|\\psi\\rangle\\|^2 = \\sum_{jkl} U_{jk}^*\\psi_k^* U_{jl}\\psi_l ∥ U ∣ ψ ⟩ ∥ 2 = j k l ∑ ​ U j k ∗ ​ ψ k ∗ ​ U j l ​ ψ l ​ To make use of the unitarity of U U U we’ll move the U U U terms together, and rewrite in terms of U † U^\\dagger U † . This gives ∥ U ∣ ψ ⟩ ∥ 2 = ∑ j k l U k j † U j l ψ k ∗ ψ l , \\|U|\\psi\\rangle\\|^2 = \\sum_{jkl} U_{kj}^\\dagger U_{jl} \\psi_k^* \\psi_l, ∥ U ∣ ψ ⟩ ∥ 2 = j k l ∑ ​ U k j † ​ U j l ​ ψ k ∗ ​ ψ l ​ , where we interchanged the j k jk j k indices on the first U U U in order to rewrite it in terms of U † U^\\dagger U † . The only place the j j j index appears anywhere in this equation is in the U k j † U j l U_{kj}^\\dagger U_{jl} U k j † ​ U j l ​ term. We can therefore perform the sum over j j j and those terms become ( U † U ) k l (U^\\dagger U)_{kl} ( U † U ) k l ​ , which is just the k l kl k l t h term in the identity matrix, since U † U = I U^\\dagger U = I U † U = I . So the above equation becomes: ∥ U ∣ ψ ⟩ ∥ 2 = ∑ k l δ k l ψ k ∗ ψ l . \\|U|\\psi\\rangle\\|^2 = \\sum_{kl} \\delta_{kl} \\psi_k^* \\psi_l. ∥ U ∣ ψ ⟩ ∥ 2 = k l ∑ ​ δ k l ​ ψ k ∗ ​ ψ l ​ . When we sum, the only time δ k l \\delta_{kl} δ k l ​ is not equal to zero is when k = l k=l k = l , in which case it’s 1 1 1 . And so we can get rid of one of the summation indices, and the equation simplifies to: ∥ U ∣ ψ ⟩ ∥ 2 = ∑ k ψ k ∗ ψ k . \\|U|\\psi\\rangle\\|^2 = \\sum_{k} \\psi_k^* \\psi_k. ∥ U ∣ ψ ⟩ ∥ 2 = k ∑ ​ ψ k ∗ ​ ψ k ​ . The right-hand side is, of course, equal to the norm of ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ squared. And so we’ve shown ∥ U ∣ ψ ⟩ ∥ 2 = ∥ ∣ ψ ⟩ ∥ 2 \\|U|\\psi\\rangle\\|^2 = \\||\\psi\\rangle\\|^2 ∥ U ∣ ψ ⟩ ∥ 2 = ∥ ∣ ψ ⟩ ∥ 2 , and therefore ∥ U ∣ ψ ⟩ ∥ = ∥ ∣ ψ ⟩ ∥ \\|U|\\psi\\rangle\\| = \\||\\psi\\rangle\\| ∥ U ∣ ψ ⟩ ∥ = ∥ ∣ ψ ⟩ ∥ . That completes the proof that unitary matrices are length-preserving. QED In fact, as I mentioned earlier, it’s also possible to prove that unitary matrices are the only matrices which preserve lengths in this way. Let me state that a little more precisely: Theorem: Let M M M be a matrix. Then ∥ M ∣ ψ ⟩ ∥ = ∥ ∣ ψ ⟩ ∥ \\|M|\\psi\\rangle \\| = \\||\\psi\\rangle\\| ∥ M ∣ ψ ⟩ ∥ = ∥ ∣ ψ ⟩ ∥ for all vectors ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ if and only if M M M is unitary. We’ve proved one half of this already; we’ll prove the other half in the next section. This theorem answers many of our questions from earlier: we see why quantum gates must be unitary, since they’re the only matrices which preserve normalization. Of course, it doesn’t completely answer the question, since it doesn’t tell us why quantum gates should be matrices (i.e., linear operations) in the first place. That fact we’re simply going to accept. In fact, it is possible to develop deeper levels of understanding of why that is true, but in this essay we’ll be satisfied with the partial explanation that unitary matrices are length-preserving. Didn’t remember Remembered Why are unitaries the only matrices which preserve length? Alright, let’s prove the missing part from the last section: let’s show that unitaries are the only matrices which preserve length. The proof is a little messy. But it turns out to be a good way to get familiar with a few extra pieces of standard quantum mechanical terminology and notation. I’ll be frank: while these pieces of terminology are extremely useful in quantum computing, we don’t strictly need them elsewhere in this essay (though we will in later essays). If you want to skip the section, or skim it, that’s okay – this is the best section of the essay to skip. But at some point you should come back and work through the material. And there is, in any case, a certain beauty to the proof. We’ve been dealing with 2-dimensional vectors up to now, but what I’m about to say applies no matter how many dimensions we’re working in. So suppose we have a vector ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ which can be written in component form as: ∣ ψ ⟩ = [ a b ⋮ z ] . |\\psi\\rangle = \\left[ \\begin{array}{c} a \\\\ b \\\\ \\vdots \\\\ z \\end{array} \\right]. ∣ ψ ⟩ = ⎣ ⎢ ⎢ ⎢ ⎡ ​ a b ⋮ z ​ ⎦ ⎥ ⎥ ⎥ ⎤ ​ . We’re going to define a new object, also labeled with a ψ \\psi ψ , but now with a bracket in the other direction: ⟨ ψ ∣ : = [ a ∗ b ∗ … z ∗ ] . \\langle \\psi| := [a^* b^* \\dots z^*]. ⟨ ψ ∣ : = [ a ∗ b ∗ … z ∗ ] . That is, ⟨ ψ ∣ \\langle\\psi| ⟨ ψ ∣ is a row vector, whose entries are the same as ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ , but complex conjugated. The vector ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ was called a ket, and (you’re going to groan) ⟨ ψ ∣ \\langle \\psi| ⟨ ψ ∣ is called a bra, making this the bra-ket, or bracket notation. Yes, theoretical physicists make dad jokes, too. These names were given by the theoretical physicist Paul Dirac in 1939, and it’s often called the Dirac bra-ket notation, or sometimes just the Dirac notation. A key fact about the bra ⟨ ψ ∣ \\langle \\psi| ⟨ ψ ∣ is that it’s related to the ket ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ by the dagger operation: ∣ ψ ⟩ † = ⟨ ψ ∣ . |\\psi\\rangle^\\dagger = \\langle \\psi|. ∣ ψ ⟩ † = ⟨ ψ ∣ . It’s easy to see why this identity is true. Take the vector ∣ ψ ⟩ = [ a b ⋮ z ] , |\\psi\\rangle = \\left[ \\begin{array}{c} a \\\\ b \\\\ \\vdots \\\\ z \\end{array} \\right], ∣ ψ ⟩ = ⎣ ⎢ ⎢ ⎢ ⎡ ​ a b ⋮ z ​ ⎦ ⎥ ⎥ ⎥ ⎤ ​ , and apply the dagger operation, which means taking the transpose, turning it into a row vector with entries a , b , … , z a, b, \\ldots, z a , b , … , z , and then take the complex conjugate, giving us [ a ∗ b ∗ … z ∗ ] [a^* b^* \\ldots z^*] [ a ∗ b ∗ … z ∗ ] , which is just the definition of ⟨ ψ ∣ \\langle \\psi| ⟨ ψ ∣ . In a similar way we see that ⟨ ψ ∣ † = ∣ ψ ⟩ \\langle \\psi|^\\dagger = |\\psi\\rangle ⟨ ψ ∣ † = ∣ ψ ⟩ . Another useful identity expresses the length of ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ in terms of the Dirac notation: ∥ ∣ ψ ⟩ ∥ 2 = ⟨ ψ ∣ ∣ ψ ⟩ . \\||\\psi\\rangle\\|^2 = \\langle \\psi| |\\psi\\rangle. ∥ ∣ ψ ⟩ ∥ 2 = ⟨ ψ ∣ ∣ ψ ⟩ . That is, the length squared is just equal to the product of the row vector ⟨ ψ ∣ \\langle \\psi| ⟨ ψ ∣ with the column vector ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ . To prove it just notice that the right-hand side is [ a ∗ b ∗ … ] [ a b ⋮ ] = ∣ a ∣ 2 + ∣ b ∣ 2 + … [a^* b^* \\ldots ] \\left[ \\begin{array}{c} a \\\\ b \\\\ \\vdots \\end{array} \\right] = |a|^2 + |b|^2 + \\ldots [ a ∗ b ∗ … ] ⎣ ⎢ ⎡ ​ a b ⋮ ​ ⎦ ⎥ ⎤ ​ = ∣ a ∣ 2 + ∣ b ∣ 2 + … And that, of course, is ∥ ∣ ψ ⟩ ∥ 2 \\||\\psi\\rangle\\|^2 ∥ ∣ ψ ⟩ ∥ 2 , just as we wanted. Physicists using the Dirac notation don’t usually write ⟨ ψ ∣ ∣ ψ ⟩ \\langle \\psi| |\\psi\\rangle ⟨ ψ ∣ ∣ ψ ⟩ . They simplify it slightly, omitting one of the vertical bars ∣ | ∣ in the middle, and just write it as: ⟨ ψ ∣ ψ ⟩ . \\langle \\psi| \\psi \\rangle. ⟨ ψ ∣ ψ ⟩ . It’s only a slight simplification, but this omission of the extra bar turns out to make life considerably easier, and is well worth it. I’ve shown it above just for the special case of ⟨ ψ ∣ ψ ⟩ \\langle \\psi| \\psi \\rangle ⟨ ψ ∣ ψ ⟩ but the same omission of a vertical bar is done often in other contexts. In practice, it rarely causes confusion, although of course you do need to get used to it. Another useful identity is that if M M M is a matrix and ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ is a ket, then ( M ∣ ψ ⟩ ) † = ⟨ ψ ∣ M † . (M|\\psi\\rangle)^\\dagger = \\langle \\psi|M^\\dagger. ( M ∣ ψ ⟩ ) † = ⟨ ψ ∣ M † . If you think of ⟨ ψ ∣ \\langle \\psi| ⟨ ψ ∣ as the dagger of ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ , then what this equation means is that taking the dagger of the product M ∣ ψ ⟩ M|\\psi\\rangle M ∣ ψ ⟩ is the same as taking the product of the dagger of M M M with the dagger ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ , but reversing the order. This is a useful little mnemonic for remembering the above identity. The identity is easy to prove. As a challenge to yourself you might want to stop right now and take a shot at proving it. Here’s the details if you get stuck. Proof: The way to prove the identity is to apply the definitions. We’re going to look at the j j j t h component of the left-hand side, ( M ∣ ψ ⟩ ) j † (M|\\psi\\rangle)^\\dagger_j ( M ∣ ψ ⟩ ) j † ​ , and we’ll show it’s equal to the j j j t h component of the right-hand side. By definition, the j j j t h row component ( M ∣ ψ ⟩ ) j † (M|\\psi\\rangle)^\\dagger_j ( M ∣ ψ ⟩ ) j † ​ is equal to the complex conjugate of the j j j t h column component of M ∣ ψ ⟩ M|\\psi\\rangle M ∣ ψ ⟩ , i.e., ( M ∣ ψ ⟩ ) j ∗ (M|\\psi\\rangle)^*_j ( M ∣ ψ ⟩ ) j ∗ ​ . That column component is ∑ k M j k ∗ ψ k ∗ \\sum_k M_{jk}^* \\psi_k^* ∑ k ​ M j k ∗ ​ ψ k ∗ ​ . We can move the ψ \\psi ψ terms to the left, and swap the indices on the M M M term to convert the ∗ * ∗ to a dagger, giving ∑ k ψ k ∗ M k j † \\sum_k \\psi_k^* M_{kj}^\\dagger ∑ k ​ ψ k ∗ ​ M k j † ​ . That’s just the j j j t h component of the row vector ⟨ ψ ∣ M † \\langle \\psi|M^\\dagger ⟨ ψ ∣ M † , as we set out to show. QED With all these ideas in mind, here’s an exercise for you to work through, putting several of these ideas together: Exercise: Show that for any matrix M M M and vector ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ , the following identity holds, expressing the length of M ∣ ψ ⟩ M|\\psi\\rangle M ∣ ψ ⟩ : ∥ M ∣ ψ ⟩ ∥ 2 = ⟨ ψ ∣ M † M ∣ ψ ⟩ . \\|M|\\psi\\rangle\\|^2 = \\langle\\psi|M^\\dagger M |\\psi\\rangle. ∥ M ∣ ψ ⟩ ∥ 2 = ⟨ ψ ∣ M † M ∣ ψ ⟩ . You may be wondering why we care about all these identities? Of course, really I’m just frontloading them – they’re all identities we’re going to find useful in a moment in the proof. So in some sense they are a bit ad hoc in motivation. But an underlying theme is that they’re about relating lengths to matrix operations. And it’s not so surprising that’s of interest – we’re going to assume we have a length-preserving operation. It’s very convenient to be able to relate that property to familiar operations about matrix multiplication. That’s the essential source of the interest in the above identities. Alright, we’ve been working through some heavy, detailed material. We’ve got just a little more background to get through, but it’s easier going. I’m going to introduce a unit vector, denoted ∣ e j ⟩ |e_j\\rangle ∣ e j ​ ⟩ , meaning the vector with a 1 1 1 in the j j j t h component, and 0 0 0 s everywhere else. So, for instance, for a qubit: ∣ e 0 ⟩ = [ 1 0 ] ∣ e 1 ⟩ = [ 0 1 ] \\begin{aligned} |e_0\\rangle & = \\left[ \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right] \\\\ |e_1\\rangle & = \\left[ \\begin{array}{c} 0 \\\\ 1 \\end{array} \\right] \\end{aligned} ∣ e 0 ​ ⟩ ∣ e 1 ​ ⟩ ​ = [ 1 0 ​ ] = [ 0 1 ​ ] ​ From elementary linear algebra, if M M M is a matrix, then M ∣ e k ⟩ M|e_k\\rangle M ∣ e k ​ ⟩ is just the k k k t h column of M M M . (If you don’t recall that from elementary linear algebra, I encourage you to stop and figure out why it’s true.) And from that you can deduce easily that ⟨ e j ∣ M ∣ e k ⟩ \\langle e_j|M|e_k\\rangle ⟨ e j ​ ∣ M ∣ e k ​ ⟩ is the j k jk j k t h entry of M M M . Exercise: If you’re not familiar with the proof, show that M ∣ e k ⟩ M|e_k\\rangle M ∣ e k ​ ⟩ is the k k k t h column of the matrix M M M , and that ⟨ e j ∣ M ∣ e k ⟩ \\langle e_j|M|e_k\\rangle ⟨ e j ​ ∣ M ∣ e k ​ ⟩ is the j k jk j k t h entry of M M M . Alright, that’s more than enough notational background! Let’s get to the main event. In particular, let’s recall the statement of the theorem we want to complete the proof of. Also recall that we proved the reverse implication in the last section, so we just need to prove the forward implication: Theorem: Let M M M be a matrix. Then ∥ M ∣ ψ ⟩ ∥ = ∥ ∣ ψ ⟩ ∥ \\|M|\\psi\\rangle \\| = \\| |\\psi\\rangle\\| ∥ M ∣ ψ ⟩ ∥ = ∥ ∣ ψ ⟩ ∥ for all vectors ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ if and only if M M M is unitary. Proof: We’ll assume M M M is length-preserving, and analyze the matrix M † M M^\\dagger M M † M . Our goal is to show that this is the identity matrix, and thus M M M is unitary. To do this, we’re going to start by analyzing the diagonal elements ( M † M ) j j (M^\\dagger M)_{jj} ( M † M ) j j ​ , and show that they’re all equal to 1 1 1 . Then we’ll turn our attention to the off-diagonal elements and show that they’re all equal to 0 0 0 . To understand the diagonal elements, we know from earlier that: ( M † M ) j j = ⟨ e j ∣ M † M ∣ e j ⟩ = ∥ M ∣ e j ⟩ ∥ 2 (M^\\dagger M)_{jj} = \\langle e_j|M^\\dagger M|e_j\\rangle = \\|M|e_j\\rangle\\|^2 ( M † M ) j j ​ = ⟨ e j ​ ∣ M † M ∣ e j ​ ⟩ = ∥ M ∣ e j ​ ⟩ ∥ 2 . But since M M M is length-preserving, the latter is just ∥ ∣ e j ⟩ ∥ 2 \\||e_j\\rangle\\|^2 ∥ ∣ e j ​ ⟩ ∥ 2 , which is 1 1 1 . And so we conclude that all the diagonal elements are, indeed, 1 1 1 . What about the off-diagonal elements, i.e., ( M † M ) j k (M^\\dagger M)_{jk} ( M † M ) j k ​ where j ≠ k j \\neq k j  ​ = k ? Can we show that these are all equal to 0 0 0 ? Well, what we’d like to do is somehow to relate ( M † M ) j k (M^\\dagger M)_{jk} ( M † M ) j k ​ to the length of some vector M ∣ ψ ⟩ M|\\psi\\rangle M ∣ ψ ⟩ , and then use the length-preserving property. One idea is to try using ∣ ψ ⟩ = ∣ e j ⟩ + ∣ e k ⟩ |\\psi\\rangle = |e_j\\rangle + |e_k\\rangle ∣ ψ ⟩ = ∣ e j ​ ⟩ + ∣ e k ​ ⟩ , since that involves both the j j j t h and k k k t h directions. From the length-preserving property we have: ∥ M ∣ ψ ⟩ ∥ 2 = ∥ ∣ ψ ⟩ ∥ 2 = 1 2 + 1 2 = 2. \\|M|\\psi\\rangle\\|^2 = \\| |\\psi\\rangle\\|^2 = 1^2+1^2 = 2. ∥ M ∣ ψ ⟩ ∥ 2 = ∥ ∣ ψ ⟩ ∥ 2 = 1 2 + 1 2 = 2 . We also have: ∥ M ∣ ψ ⟩ ∥ 2 = ⟨ ψ ∣ M † M ∣ ψ ⟩ = ⟨ e j ∣ M † M ∣ e j ⟩ + ⟨ e j ∣ M † M ∣ e k ⟩ = = + ⟨ e k ∣ M † M ∣ e j ⟩ + ⟨ e k ∣ M † M ∣ e k ⟩ = 1 + ⟨ e j ∣ M † M ∣ e k ⟩ + ⟨ e k ∣ M † M ∣ e j ⟩ + 1 \\begin{aligned} \\|M|\\psi\\rangle\\|^2 & = \\langle \\psi| M^\\dagger M |\\psi\\rangle \\\\ & = \\langle e_j|M^\\dagger M|e_j\\rangle + \\langle e_j|M^\\dagger M|e_k\\rangle \\\\ & \\hphantom{ == } + \\langle e_k|M^\\dagger M|e_j\\rangle + \\langle e_k|M^\\dagger M |e_k\\rangle \\\\ & = 1 + \\langle e_j|M^\\dagger M|e_k\\rangle + \\langle e_k|M^\\dagger M|e_j\\rangle + 1 \\end{aligned} ∥ M ∣ ψ ⟩ ∥ 2 ​ = ⟨ ψ ∣ M † M ∣ ψ ⟩ = ⟨ e j ​ ∣ M † M ∣ e j ​ ⟩ + ⟨ e j ​ ∣ M † M ∣ e k ​ ⟩ = = + ⟨ e k ​ ∣ M † M ∣ e j ​ ⟩ + ⟨ e k ​ ∣ M † M ∣ e k ​ ⟩ = 1 + ⟨ e j ​ ∣ M † M ∣ e k ​ ⟩ + ⟨ e k ​ ∣ M † M ∣ e j ​ ⟩ + 1 ​ Comparing the last two sets of equations, we have: ⟨ e j ∣ M † M ∣ e k ⟩ + ⟨ e k ∣ M † M ∣ e j ⟩ = 0. \\langle e_j|M^\\dagger M|e_k\\rangle + \\langle e_k|M^\\dagger M|e_j\\rangle = 0. ⟨ e j ​ ∣ M † M ∣ e k ​ ⟩ + ⟨ e k ​ ∣ M † M ∣ e j ​ ⟩ = 0 . This is close to what we want, but isn’t quite right. It tells us that ( M † M ) j k + ( M † M ) k j = 0 (M^\\dagger M)_{jk} + (M^\\dagger M)_{kj} = 0 ( M † M ) j k ​ + ( M † M ) k j ​ = 0 . Can we do better? It’s tempting to go back and fiddle around and try to find some way of eliminating one or the other of those terms. But there’s no direct way to do it – at least, no direct way that I know of. But what if we’d done something slightly different, and instead of using ∣ ψ ⟩ = ∣ e j ⟩ + ∣ e k ⟩ |\\psi\\rangle = |e_j\\rangle+|e_k\\rangle ∣ ψ ⟩ = ∣ e j ​ ⟩ + ∣ e k ​ ⟩ we’d used ∣ ψ ⟩ = ∣ e j ⟩ + i ∣ e k ⟩ |\\psi\\rangle = |e_j\\rangle+i|e_k\\rangle ∣ ψ ⟩ = ∣ e j ​ ⟩ + i ∣ e k ​ ⟩ ? It seems pretty plausible that following the same line of reasoning we’d get an equation involving ( M † M ) j k (M^\\dagger M)_{jk} ( M † M ) j k ​ and ( M † M ) k j (M^\\dagger M)_{kj} ( M † M ) k j ​ again. I won’t explicitly go through the steps – you can do that yourself – but if you do go through them you end up with the equation: ( M † M ) j k − ( M † M ) k j = 0. (M^\\dagger M)_{jk} - (M^\\dagger M)_{kj} = 0. ( M † M ) j k ​ − ( M † M ) k j ​ = 0 . This is great! We can add it to the earlier equation to deduce that ( M † M ) j k = 0 (M^\\dagger M)_{jk} = 0 ( M † M ) j k ​ = 0 whenever j ≠ k j \\neq k j  ​ = k , and so we conclude that M † M = I M^\\dagger M = I M † M = I , i.e., M M M is unitary. QED Didn’t remember Remembered The controlled-NOT gate We’ve developed most of the ideas needed to do universal quantum computing. We understand qubits, quantum states, and have a repertoire of quantum gates. However, all our gates involve just a single qubit. To compute, we need some way for qubits to interact with one another. That is, we need quantum gates which involve two (or more) qubits. An example of such a gate is the controlled-NOT (or CNOT) gate. In the quantum circuit language we have two wires, representing two qubits, and the following notation to represent the CNOT gate: The wire with the small, filled dot on it (the top wire, in this example) is called the control qubit, for reasons which will become clear in a moment. And the wire with the larger, unfilled circle on it is called the target qubit. Up to now I haven’t said what the possible states of a two-qubit system are, but you can probably guess. We now have four computational basis states, corresponding to the four possible states of a two-bit system: ∣ 00 ⟩ , ∣ 01 ⟩ , ∣ 10 ⟩ |00\\rangle, |01\\rangle, |10\\rangle ∣ 0 0 ⟩ , ∣ 0 1 ⟩ , ∣ 1 0 ⟩ , and ∣ 11 ⟩ |11\\rangle ∣ 1 1 ⟩ . And, for a two-qubit system, not only can we have those four states, we can also have superpositions (i.e., linear combinations) of them: α ∣ 00 ⟩ + β ∣ 01 ⟩ + γ ∣ 10 ⟩ + δ ∣ 11 ⟩ \\alpha|00\\rangle+\\beta|01\\rangle+\\gamma |10\\rangle+\\delta|11\\rangle α ∣ 0 0 ⟩ + β ∣ 0 1 ⟩ + γ ∣ 1 0 ⟩ + δ ∣ 1 1 ⟩ Here, the amplitudes α , β , γ , δ \\alpha, \\beta, \\gamma, \\delta α , β , γ , δ are just complex numbers, and the sum of the squares of the absolute values is 1 1 1 , i.e, ∣ α ∣ 2 + ∣ β ∣ 2 + ∣ γ ∣ 2 + ∣ δ ∣ 2 = 1 |\\alpha|^2+|\\beta|^2+|\\gamma|^2+|\\delta|^2 = 1 ∣ α ∣ 2 + ∣ β ∣ 2 + ∣ γ ∣ 2 + ∣ δ ∣ 2 = 1 . This is the same kind of normalization condition as we had for a single qubit. Now, the CNOT gate is, much like the quantum NOT gate, inspired directly by a classical gate. What it does is very simple. If the control qubit is set to 1 1 1 , as in the states ∣ 10 ⟩ |10\\rangle ∣ 1 0 ⟩ and ∣ 11 ⟩ |11\\rangle ∣ 1 1 ⟩ , then it flips (i.e., NOTs) the target qubit. And otherwise it does nothing. Writing out the action on all four computational basis states we have: ∣ 00 ⟩ → ∣ 00 ⟩ ∣ 01 ⟩ → ∣ 01 ⟩ ∣ 10 ⟩ → ∣ 11 ⟩ ∣ 11 ⟩ → ∣ 10 ⟩ \\begin{aligned} |00\\rangle & \\rightarrow & |00\\rangle \\\\ |01\\rangle & \\rightarrow & |01\\rangle \\\\ |10\\rangle & \\rightarrow & |11\\rangle \\\\ |11\\rangle & \\rightarrow & |10\\rangle \\end{aligned} ∣ 0 0 ⟩ ∣ 0 1 ⟩ ∣ 1 0 ⟩ ∣ 1 1 ⟩ ​ → → → → ​ ∣ 0 0 ⟩ ∣ 0 1 ⟩ ∣ 1 1 ⟩ ∣ 1 0 ⟩ ​ If you’re familiar with classical programming languages, then you can think of the CNOT as a very simple kind of if-then statement: if the control qubit is set, then NOT the target qubit. But while simple, it can be used as a building block to build up other, more complex kinds of conditional behavior. There’s a way of summing up all four of the equations above in a single equation. Suppose x x x and y y y are classical bits, i.e., 0 0 0 or 1 1 1 . Then we can rewrite the equations above in a single equation as: ∣ x , y ⟩ → ∣ x , y ⊕ x ⟩ . \\begin{aligned} |x, y\\rangle & \\rightarrow & |x, y\\oplus x\\rangle. \\end{aligned} ∣ x , y ⟩ ​ → ​ ∣ x , y ⊕ x ⟩ . ​ Note the commas inserted to make this easier to read – this is pretty common in working with multi-qubit states. The above equation makes clear that the CNOT leaves the control qubit x x x alone, but flips the target qubit y y y if x x x is set to 1 1 1 . Note that ⊕ \\oplus ⊕ is addition modulo 2 2 2 , where 1 ⊕ 1 = 0 1 \\oplus 1 = 0 1 ⊕ 1 = 0 , as we would expect from the fact that the CNOT takes ∣ 11 ⟩ |11\\rangle ∣ 1 1 ⟩ to ∣ 10 ⟩ |10\\rangle ∣ 1 0 ⟩ . That’s all there is to the CNOT. It’s really a very simple idea and quantum gate. Note that it of course acts linearly on superpositions of computational basis states, as we expect for a quantum gate. So: → α ∣ 00 ⟩ + β ∣ 01 ⟩ + γ ∣ 10 ⟩ + δ ∣ 11 ⟩ → α ∣ 00 ⟩ + β ∣ 01 ⟩ + γ ∣ 11 ⟩ + δ ∣ 10 ⟩ \\begin{aligned} & \\hphantom{ \\rightarrow } \\alpha|00\\rangle+\\beta|01\\rangle+\\gamma |10\\rangle+\\delta|11\\rangle \\\\ & \\rightarrow \\alpha|00\\rangle+\\beta|01\\rangle+\\gamma |11\\rangle+\\delta|10\\rangle \\end{aligned} ​ → α ∣ 0 0 ⟩ + β ∣ 0 1 ⟩ + γ ∣ 1 0 ⟩ + δ ∣ 1 1 ⟩ → α ∣ 0 0 ⟩ + β ∣ 0 1 ⟩ + γ ∣ 1 1 ⟩ + δ ∣ 1 0 ⟩ ​ And, though I won’t explicitly carry out the verification, the CNOT is unitary, and thus preserves the length of quantum states, as we expect. Of course, the CNOT doesn’t just appear in two-qubit computations. It also appears in computations involving more qubits. Let’s suppose we have three qubits, for instance, and computational basis states such as ∣ 000 ⟩ , ∣ 001 ⟩ |000\\rangle, |001\\rangle ∣ 0 0 0 ⟩ , ∣ 0 0 1 ⟩ , and so on. Here’s a CNOT with the second qubit as the control qubit and the third qubit as the target: What goes on? Well, we can write out what happens on an arbitrary computational basis state, ∣ x , y , z ⟩ |x, y, z\\rangle ∣ x , y , z ⟩ , where x , y x, y x , y and z z z are all classical bits. Of course, the first bit x x x isn’t changed at all, since it’s not involved in the CNOT. The second bit y y y is the control bit, and so isn’t changed. But the third bit z z z is flipped if the control bit y y y is set to 1 1 1 . And so we can write the action of the CNOT as: ∣ x , y , z ⟩ → ∣ x , y , z ⊕ y ⟩ |x,y,z\\rangle \\rightarrow |x,y, z\\oplus y\\rangle ∣ x , y , z ⟩ → ∣ x , y , z ⊕ y ⟩ I’ve described the CNOT as a “classical” gate, but it can be combined with single-qubit gates to do non-classical things. Let me give you an explicit example. It’s another two-qubit computation. It starts with the ∣ 00 ⟩ |00\\rangle ∣ 0 0 ⟩ computational basis state, we apply a Hadamard gate to the first qubit, and then do a CNOT: Recall that for a single qubit the Hadamard gate takes ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ to an equal superposition ( ∣ 0 ⟩ + ∣ 1 ⟩ ) / 2 (|0\\rangle+|1\\rangle)/\\sqrt{2} ( ∣ 0 ⟩ + ∣ 1 ⟩ ) / 2 ​ . For these two qubits it doesn’t affect the second qubit at all, and so it takes ∣ 00 ⟩ |00\\rangle ∣ 0 0 ⟩ to ( ∣ 00 ⟩ + ∣ 10 ⟩ ) / 2 (|00\\rangle+|10\\rangle)/\\sqrt{2} ( ∣ 0 0 ⟩ + ∣ 1 0 ⟩ ) / 2 ​ . Next we apply the CNOT gate. This leaves the ∣ 00 ⟩ |00\\rangle ∣ 0 0 ⟩ state unchanged, since the control bit is 0 0 0 . And it takes ∣ 10 ⟩ |10\\rangle ∣ 1 0 ⟩ to ∣ 11 ⟩ |11\\rangle ∣ 1 1 ⟩ , since the control bit is 1 1 1 . And so the output from the circuit is: ∣ 00 ⟩ + ∣ 11 ⟩ 2 . \\frac{|00\\rangle+|11\\rangle}{\\sqrt 2}. 2 ​ ∣ 0 0 ⟩ + ∣ 1 1 ⟩ ​ . This output state is a highly non-classical state – it’s actually a type of state called an entangled state. There’s no obvious interpretation of this state as a classical state, unlike say a computational basis state such as ∣ 00 ⟩ |00\\rangle ∣ 0 0 ⟩ . In fact, entangled states can be used to do all sorts of interesting information processing tasks, including quantum teleportation and fast quantum algorithms. A point I glossed over above, but worth mentioning: in the circuit I drew ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ separately as input qubits. It’s conventional to do that kind of thing to denote a combined input of ∣ 00 ⟩ |00\\rangle ∣ 0 0 ⟩ . More generally, people use ∣ 0 ⟩ ∣ 0 ⟩ |0\\rangle|0\\rangle ∣ 0 ⟩ ∣ 0 ⟩ interchangeably with ∣ 00 ⟩ |00\\rangle ∣ 0 0 ⟩ , ∣ 0 ⟩ ∣ 1 ⟩ |0\\rangle|1\\rangle ∣ 0 ⟩ ∣ 1 ⟩ interchangeably with ∣ 01 ⟩ |01\\rangle ∣ 0 1 ⟩ , and so on. Going back and forth takes a bit of getting used to, but everything works pretty much as you expect, and you just need a little practice before it seems quite natural. More generally, if we have single-qubit states α ∣ 0 ⟩ + β ∣ 1 ⟩ \\alpha|0\\rangle+\\beta|1\\rangle α ∣ 0 ⟩ + β ∣ 1 ⟩ and γ ∣ 0 ⟩ + δ ∣ 1 ⟩ \\gamma|0\\rangle+\\delta|1\\rangle γ ∣ 0 ⟩ + δ ∣ 1 ⟩ , then the combined state when the two qubits are put together is just: = ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) ( γ ∣ 0 ⟩ + δ ∣ 1 ⟩ ) = α γ ∣ 00 ⟩ + α δ ∣ 01 ⟩ + β γ ∣ 10 ⟩ + β δ ∣ 11 ⟩ . \\begin{aligned} & \\hphantom{ = } (\\alpha|0\\rangle+\\beta|1\\rangle)(\\gamma|0\\rangle+\\delta|1\\rangle) \\\\ & = \\alpha\\gamma|00\\rangle+\\alpha\\delta |01\\rangle+ \\beta\\gamma |10\\rangle+ \\beta\\delta |11\\rangle. \\end{aligned} ​ = ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) ( γ ∣ 0 ⟩ + δ ∣ 1 ⟩ ) = α γ ∣ 0 0 ⟩ + α δ ∣ 0 1 ⟩ + β γ ∣ 1 0 ⟩ + β δ ∣ 1 1 ⟩ . ​ I said that the CNOT leaves the control qubit alone, and modifies the target qubit. That’s true in the computational basis. In fact, it’s actually possible for the target qubit to affect the control qubit. It’s worth taking a minute or two to at least understand (and, if you’re feeling energetic, attempting to solve) the following exercise: Exercise: Can you find single-qubit states ∣ a ⟩ |a\\rangle ∣ a ⟩ and ∣ b ⟩ |b\\rangle ∣ b ⟩ so that applying a CNOT to the combined state ∣ a b ⟩ |ab\\rangle ∣ a b ⟩ changes the first qubit, i.e., the control qubit? Let me give you an example which solves the above exercise. Suppose we introduce single-qubit states ∣ + ⟩ |+\\rangle ∣ + ⟩ and ∣ − ⟩ |-\\rangle ∣ − ⟩ , defined by: ∣ + ⟩ : = ∣ 0 ⟩ + ∣ 1 ⟩ 2 ∣ − ⟩ : = ∣ 0 ⟩ − ∣ 1 ⟩ 2 \\begin{aligned} |+\\rangle & := \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} \\\\ |-\\rangle & := \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} \\end{aligned} ∣ + ⟩ ∣ − ⟩ ​ : = 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ : = 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ ​ A mnemonic for this notation is that these are both equal superpositions of ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ , but the “+” or “-” corresponds to the sign of the amplitude for the ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ state. The following circuit identity holds: That is, if we put ∣ + − ⟩ |+-\\rangle ∣ + − ⟩ in, then ∣ − − ⟩ |--\\rangle ∣ − − ⟩ comes out, i.e., the target qubit isn’t changed, but the control qubit is! The proof is just to follow the definitions and the algebra through. It’s honestly not terribly enlightening to follow the algebra through at this point – it’s a lot easier once certain other facts are known to you. But, again, if you’re feeling enthusiastic it’s a good exercise to work through to become familiar with how things work. It’s worth taking some caution from this example. It means that intuitions coming from the computational basis can sometimes be incomplete or misleading. The CNOT isn’t simply doing something to the target, conditional on the control. It’s also doing something to the control, conditional on the target. Exercise: Show that the inverse of the CNOT gate is just the CNOT gate. Didn’t remember Remembered Part III: Universal quantum computing In natural science, Nature has given us a world and we’re just to discover its laws. In computers, we can stuff laws into it and create a world. — Alan Kay Let’s return to the question we began with: is there a single computing device that can efficiently simulate any other physical system? At the moment, the best candidate humanity has for such a computing device is a quantum computer. As you’ve probably guessed, you make such a device by combining all the elements we’ve been discussing. In Part III of this essay we’ll discuss what a quantum computer is, why they’re useful, and whether they can be used to efficiently simulate any other physical system. The quantum computing model The theory of computation has traditionally been studied almost entirely in the abstract, as a topic in pure mathematics. This is to miss the point of it. Computers are physical objects, and computations are physical processes. What computers can or cannot compute is determined by the laws of physics alone, and not by pure mathematics. — David Deutsch In a general quantum computation, you start out with many qubits – I’ll draw four here, but in general it might be many more (or less). You apply quantum gates of various kinds, in particular, single-qubit gates and CNOT gates. And at the end of the circuit you read out the result by measuring in the computational basis. Here’s what it all looks like: I haven’t labeled the single-qubit gates explicitly, but they might be various things – Hadamard gates, NOT gates, rotations, and perhaps others. Note also that while the computation starts in the computational basis state ∣ 0000 ⟩ |0000\\rangle ∣ 0 0 0 0 ⟩ , you can also start in some other computational basis state. I’ve just chosen ∣ 0000 ⟩ |0000\\rangle ∣ 0 0 0 0 ⟩ for definiteness. We can summarize the three steps in a quantum computation as follows: Start in a computational basis state. Apply a sequence of CNOT and single-qubit gates. To obtain the result, measure in the computational basis. The probability of any result, say 00 … 0 00\\ldots 0 0 0 … 0 , is just the square of the absolute value of the corresponding amplitude. That’s all a general quantum computation is! If you understand this model, you know what a quantum computer is. It's pretty simple, really – enough so that I've sometimes heard people say “Is that all there is to it?” But while the model is simple, it contains remarkable depths, and exploring it could occupy many lifetimes. In practice, people sometimes introduce other ideas into the way they describe quantum computations. If you’re a programmer, you can think of this as like the way programming language designers introduce higher-level abstractions to help people design different kinds of programs. In principle, those abstractions can always be reduced down to the level of AND and NOT gates. And if you understand that level – AND and NOT, or the type of quantum circuit shown above – then you have a foundation for building an understanding of the other ideas. That may leave you wondering: does that mean you also need to master all the higher-level abstractions? The answer is no! There are several reasons for this. One reason is that, as we discussed earlier, humanity doesn’t yet know what the higher-level abstractions are. We’re still trying to figure them out. A second reason is that it seems likely that the list of higher-level abstractions is inexhaustible. My guess – and it’s just a guess – is that we will continue to discover beautiful new abstractions forever, in both classical and quantum computing. A related idea is that there are models of quantum computation different to the quantum circuit model. Some are merely small variations on the model I’ve described. For instance, instead of only using CNOT gates, we might allow any two-qubit unitary gate to be used in the circuit. Or perhaps instead of using qubits, we might use some other type of basic quantum system – say, the qutrit, which has three computational basis states, ∣ 0 ⟩ , ∣ 1 ⟩ |0\\rangle, |1\\rangle ∣ 0 ⟩ , ∣ 1 ⟩ , and ∣ 2 ⟩ |2\\rangle ∣ 2 ⟩ . It probably won’t surprise you that the resulting models of computation are essentially equivalent to the quantum circuit model I’ve described. By this, I mean they can simulate the quantum circuit model (and vice versa) using roughly comparable numbers of gates and other physical resources. There are also much more exotic variations, ideas such as measurement-based quantum computation, topological quantum computation, and others. I won’t describe these in any detail here, but suffice to say that they appear superficially very different to the circuit model. Nonetheless, they’re all mathematically equivalent to one another, including to the quantum circuit model. Thus a quantum computation in any of those models can be translated into an equivalent in the quantum circuit model, with only a small overhead in the cost of computation. And vice versa. You may wonder why people bother thinking about other models, if they’re mathematically equivalent to the quantum circuit model. The reason is that just because two models are mathematically equivalent doesn’t mean they’re psychologically equivalent. Different models of computation stimulate different ways of thinking, and give rise to different ideas. And so it’s valuable to have other equivalent models. Exercise: We saw earlier that composition of quantum gates corresponds to matrix multiplication (in reverse order). Show that the product of two unitary matrices U U U and V V V is also unitary. As a consequence, the net effect of any quantum circuit (before measurement) is to effect a unitary operation on the state space of the system. Didn’t remember Remembered Let me conclude this section with a brief comment about a particular class of quantum gates. These are gates which are multiples of the identity matrix, I I I : [ e i θ 0 0 e i θ ] = e i θ [ 1 0 0 1 ] = e i θ I . \\begin{aligned} \\left[ \\begin{array}{cc} e^{i\\theta} & 0 \\\\ 0 & e^{i\\theta} \\end{array} \\right] = e^{i\\theta} \\left[ \\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array} \\right] = e^{i \\theta} I. \\end{aligned} [ e i θ 0 ​ 0 e i θ ​ ] = e i θ [ 1 0 ​ 0 1 ​ ] = e i θ I . ​ When θ \\theta θ is a real number this is a unitary matrix, and so a valid quantum gate. The effect of the gate is simply to multiply the state of the quantum computer by e i θ e^{i\\theta} e i θ . The number e i θ e^{i\\theta} e i θ is called a global phase factor. But even though the gate is valid, we rarely explicitly put such gates into quantum circuits. The reason is that such global phase factors have no impact on the results of the computation. To see why, imagine a quantum circuit which includes many such quantum gates, possibly for different values, θ 1 , θ 2 , … \\theta_1, \\theta_2, \\ldots θ 1 ​ , θ 2 ​ , … . No matter where in the circuit the gates occur, the net effect is to multiply the state output from the circuit by an overall factor of e i θ 1 + i θ 2 + … e^{i \\theta_1 + i\\theta_2 + \\ldots} e i θ 1 ​ + i θ 2 ​ + … . This doesn’t change the squared amplitudes of the computational basis states, and so has no impact on the measurement probabilities at the end of the computation. We could have left the gates out and it would make no difference. Quantum computing people tend to be rather blase about such global phase factors. They’ll do things like not bother to distinguish between unitary gates such as X X X and − X -X − X , saying these gates are “the same up to a global phase factor”. They simply mean the − X -X − X gate is the same as doing the X X X gate, followed by the − I -I − I gate. Since the latter gate makes no difference to the output from the computation, it can safely be omitted. We’ll see examples in the next essay, about the quantum search algorithm, where at a couple of places it makes sense to multiply the quantum state by a global phase factor of − 1 -1 − 1 . Didn’t remember Remembered What are quantum computers good for? Now we know what a quantum computer is, what are they good for? Earlier, I suggested quantum computers expand the range of operations available when computing. This is similar to the way boats expand the range of ways we can traverse space, so instead of having to get from point A to point B by land, we can take a shortcut greatly cutting down the time required: In order for the analogous story to hold for quantum computers, they need to be at least as capable as classical computers. Fortunately, it’s possible to convert any classical circuit into a quantum circuit. This requires some care. The obvious thing to do is to imagine that your classical circuit is expressed in terms of some standard universal set – say, the AND and NOT gates – and then to convert those gates into equivalent quantum gates. This is easy to do with the NOT gate – we just turn it into an X X X gate. But what’s not so easy is the AND gate. You might wonder if there’s some quantum gate which takes two bits x x x and y y y as input, in the computational basis, ∣ x , y ⟩ |x,y\\rangle ∣ x , y ⟩ , and then outputs a single qubit ∣ x ∧ y ⟩ |x \\wedge y\\rangle ∣ x ∧ y ⟩ , where x ∧ y x \\wedge y x ∧ y is just the logical AND of the bits x x x and y y y . Unfortunately, that “quantum gate” makes no sense at all! Not only is it not unitary, it’s not even close: a unitary gate with two qubits as input necessarily has two qubits as output. The “gate” I described has just a single qubit as output, so there’s no way it can be unitary. You might wonder if instead there’s some way we can find a two-qubit quantum gate which has x ∧ y x \\wedge y x ∧ y as one output, and something else as the other output. I won’t prove it, but it turns out that this is impossible. The proof actually isn’t all that hard – it’s a fun exercise to think through, if you want a challenge – but is more of a digression than I want to get into here. That’s all rather disappointing. But there is a solution. It’s to use a three-qubit quantum gate called the Toffoli gate. The Toffoli gate is much like the CNOT gate, but instead of having a single control qubit, it has two control qubits, x x x and y y y , and a single target qubit, z z z . If both controls qubits are set, then the target is flipped. Otherwise, the target is left alone: If the target starts out as z = 0 z = 0 z = 0 , then you can see that the target output is just x ∧ y x \\wedge y x ∧ y , and so the Toffoli gate can be used to simulate a classical AND gate. So if we have any classical circuit of AND and NOT gates then there’s a corresponding quantum circuit involving the same number of X X X and Toffoli gates which computes the same function. Exercise: What’s a quantum circuit that can compute the NAND gate? Recall that the NAND of two bits x x x and y y y is just the NOT of x ∧ y x \\wedge y x ∧ y . Exercise: Can you find a way of implementing a NAND gate using just a single Toffoli gate and no other quantum gates? Note that your answer here may be the same as your answer to the previous exercise, if you answered that exercise using just a single Toffoli gate and no other quantum gates. A wrinkle in all this is that the Toffoli gate isn’t in our standard set of basic quantum gates. However, it’s possible to build the Toffoli gate up out of CNOT and single-qubit unitary gates. One way of doing the breakdown is shown below: There are various slick ways of “explaining” why this circuit works, but I’ll let you in on a secret: much of the earliest work on this was done by pure brute force, people simply trying lots and lots of different ways of implementing the Toffoli gate (sometimes, using a computer to assist in doing the search). Frankly, I wouldn’t worry too much about why this circuit works, just take it for granted that it does. You can look it up if you ever need to, or dig down into why. Needing to know the circuit details actually isn’t all that common, so I wouldn’t suggest memorizing it – not until you have a good reason. Exercise: Show that the inverse of the Toffoli gate is just the Toffoli gate. Didn’t remember Remembered No, really, what are quantum computers good for? It’s comforting that we can always simulate a classical circuit – it means quantum computers aren’t slower than classical computers – but doesn’t answer the question of the last section: what problems are quantum computers good for? Can we find shortcuts that make them systematically faster than classical computers? It turns out there’s no general way known to do that. But there are some interesting classes of computation where quantum computers outperform classical. Over the long term, I believe the most important use of quantum computers will be simulating other quantum systems. That may sound esoteric – why would anyone apart from a quantum physicist care about simulating quantum systems? But everybody in the future will (or, at least, will care about the consequences). The world is made up of quantum systems. Pharmaceutical companies employ thousands of chemists who synthesize molecules and characterize their properties. This is currently a very slow and painstaking process. In an ideal world they’d get the same information thousands or millions of times faster, by doing highly accurate computer simulations. And they’d get much more useful information, answering questions chemists can’t possibly hope to answer today. Unfortunately, classical computers are terrible at simulating quantum systems. The reason classical computers are bad at simulating quantum systems isn’t difficult to understand. Suppose we have a molecule containing n n n atoms – for a small molecule, n n n may be 1 1 1 - 10 10 1 0 , for a complex molecule it may be hundreds or thousands or even more. And suppose we think of each atom as a qubit (not true, but go with it): to describe the system we’d need 2 n 2^n 2 n different amplitudes, one amplitude for each n n n - bit computational basis state, e.g., ∣ 010011 … ⟩ |010011\\ldots\\rangle ∣ 0 1 0 0 1 1 … ⟩ . Of course, atoms aren’t qubits. They’re more complicated, and we need more amplitudes to describe them. Without getting into details, the rough scaling for an n n n - atom molecule is that we need k n k^n k n amplitudes, where k ≥ 2 k \\geq 2 k ≥ 2 . The value of k k k depends upon context – which aspects of the atom’s behavior are important. For generic quantum simulations k k k may be in the hundreds or more. That’s a lot of amplitudes! Even for comparatively simple atoms and small values of n n n , it means the number of amplitudes will be in the trillions. And it rises very rapidly, doubling or more for each extra atom. If k = 100 k=100 k = 1 0 0 , then even n = 10 n = 10 n = 1 0 atoms will require 100 million trillion amplitudes. That’s a lot of amplitudes for a pretty simple molecule. The result is that simulating such systems is incredibly hard. Just storing the amplitudes requires mindboggling amounts of computer memory. Simulating how they change in time is even more challenging, involving immensely complicated updates to all the amplitudes. Physicists and chemists have found some clever tricks for simplifying the situation. But even with those tricks simulating quantum systems on classical computers seems to be impractical, except for tiny molecules, or in special situations. The reason most educated people today don’t know simulating quantum systems is important is because classical computers are so bad at it that it’s never been practical to do. We’ve been living too early in history to understand how incredibly important quantum simulation really is. That’s going to change over the coming century. Many of these problems will become vastly easier when we have scalable quantum computers, since quantum computers turn out to be fantastically well suited to simulating quantum systems. Instead of each extra simulated atom requiring a doubling (or more) in classical computer memory, a quantum computer will need just a small (and constant) number of extra qubits. One way of thinking of this is as a loose quantum corollary to Moore’s law: The quantum corollary to Moore’s law: Assuming both quantum and classical computers double in capacity every few years, the size of the quantum system we can simulate scales linearly with time on the best available classical computers, and exponentially with time on the best available quantum computers. In the long run, quantum computers will win, and win easily. The punchline is that it’s reasonable to suspect that if we could simulate quantum systems easily, we could greatly speed up drug discovery, and the discovery of other new types of materials. I will risk the ire of my (understandably) hype-averse colleagues and say bluntly what I believe the likely impact of quantum simulation will be: there’s at least a 50 percent chance quantum simulation will result in one or more multi-trillion dollar industries. And there’s at least a 30 percent chance it will completely change human civilization. The catch: I don’t mean in 5 years, or 10 years, or even 20 years. I’m talking more over 100 years. And I could be wrong. What makes me suspect this may be so important? For most of history we humans understood almost nothing about what matter is. That’s changed over the past century or so, as we’ve built an amazingly detailed understanding of matter. But while that understanding has grown, our ability to control matter has lagged. Essentially, we’ve relied on what nature accidentally provided for us. We’ve gotten somewhat better at doing things like synthesizing new chemical elements and new molecules, but our control is still very primitive. We’re now in the early days of a transition where we go from having almost no control of matter to having almost complete control of matter. Matter will become programmable; it will be designable. This will be as big a transition in our understanding of matter as the move from mechanical computing devices to modern computers was for computing. What qualitatively new forms of matter will we create? I don’t know, but the ability to use quantum computers to simulate quantum systems will be an essential part of this burgeoning design science. Alright, enough speculation. Let me also briefly mention the sober-minded conventional answer given to the question “what are quantum computers good for?” That answer is to list various algorithmic problems that we have some evidence can be solved faster on a quantum computer than on a classical computer. The most famous example is Peter Shor’s beautiful quantum factoring algorithm. To find the prime factors of an n n n - bit integer seems to be a very difficult problem on a classical computer. The best existing algorithms are incredibly computationally expensive, with a cost that rises exponentially with n n n . Even numbers with just a few hundred digits aren’t currently feasible to factor on classical computers. By contrast, Shor’s quantum factoring algorithm would make factoring into a comparatively easy task, if large-scale quantum computers can be built. Factoring perhaps doesn’t seem like a very interesting application. But it turns out that the ability to factor lets you break some of the most widely-used encryption schemes, used by services such as Gmail and Amazon to keep your communications private. This ability to break encryption has made the world’s intelligence agencies very interested in factoring, and they’ve poured enormous sums of money into quantum computing research since the mid-1990s. Indeed, there’s a good (as yet unwritten) history book to be written about how the rise of quantum computing was caused by the interest of the world’s intelligence agencies in accessing humanity’s private thoughts. There’s been surprisingly little public reflection about this on the part of the quantum computing community. I occasionally meet quantum computing researchers who complain in private about what they perceive as privacy violations by governments, and the dangers of surveillance states. But then some of those same people will take money to help those governments in their plans for surveillance, usually with some transparently self-serving justification about how they’re not really helping. One exception to this lack of public reflection is a brief discussion in Ronald de Wolf’s thoughtful essay The Potential Impact of Quantum Computers on Society (2017). Didn’t remember Remembered Are quantum computers really universal devices? The eternal mystery of the world is its comprehensibility… The fact that it is comprehensible is a miracle. — Albert Einstein At the beginning of this essay I asked whether there is any single universal computing device that can efficiently simulate any other physical system? We’ve learned that classical computers seem to have a lot of trouble efficiently simulating quantum systems. What about quantum computers? While they can certainly simulate many quantum systems, does that mean they can be used to efficiently simulate any physical system? This question is an open problem! We don’t yet know the answer. Part of the trouble in answering the question is that humanity hasn’t yet discovered the final fundamental laws of physics. Modern physics is based on two astonishingly effective theories: Einstein’s general theory of relativity, which describes how gravitation works; and the standard model of particle physics, which explains how pretty much everything else (electromagnetism, the strong and weak nuclear forces) work. Trouble is, we don’t yet have a good theory of quantum gravity which combines general relativity and the standard model. Without such a theory of quantum gravity we’re not able to answer the question of whether quantum computers can efficiently simulate any other physical system. Perhaps some future class of quantum gravitating computers, more powerful even than quantum computers, will be needed to simulate quantum gravity. Let’s ask a slightly less ambitious question, which is whether we can use quantum computers to efficiently simulate general relativity and the standard model? The standard model is an example of a particular type of quantum mechanical theory called a quantum field theory. John Preskill and his collaborators have written a series of papers For a review of progress see: John Preskill, Simulating quantum field theory with a quantum computer (2018). explaining how to use quantum computers to efficiently simulate quantum field theories. Those papers don’t yet simulate the full standard model, but they do make considerable progress. It remains an exciting open problem, albeit a problem where much encouraging progress has been made. In the case of general relativity, as far as I know the problem remains open. Indeed, even stating what the problem means is not trivial. General relativity supports the existence of closed timelike curves, which can be used in some sense to send information back in time. This has interesting consequences for computation: there’s a way in which the computer can know the results of future computations. Unsurprisingly, this changes what is possible! Another complication is that when you talk about an “efficient simulation” in computation you mean the time and space overhead isn’t too large. But in general relativity even the basic units of space and time aren’t so clear. It’s hard to say what efficiency means. Finally, near singularities time and space get distorted in strange ways, again making it challenging to say exactly what it means to do an efficient computation. There is, by the way, significant issue that I've been sweeping under the rug, and which may be bugging you: as I've explained it, a quantum computer isn't a single computing device at all, since there are many possible quantum circuits. This is okay, though, since there's a model known as the universal quantum Turing machine, which is a single computing device, and which can simulate any quantum circuit. So you should understand the discussion above as being implicitly about the universal quantum Turing machine. I won't explain the details of the universal quantum Turing machine in this essay, since in practice the quantum circuit model is far more commonly used. But if you're interested in the details, I recommend this paper by Bernstein and Vazirani. The existence of universal computers is easy to take for granted. But there’s no a priori logical reason there should be a single machine that can efficiently simulate every other physical system. It’s like being able to use your car also as a surfboard, a supermarket trolley, and a rainforest In fact, there’s a sense in which this is possible, within limits: if you could rearrange protons, neutrons, and electrons arbitrarily well, you could turn a car into a surfboard, a supermarket trolley, or a (small) part of a rainforest. So matter does have intriguing universality properties. This is also remarkable, of course.. Yet the evidence so far suggests our universe does allow such universal machines. It’s a good problem – and, so far as I know, largely unexplored – to think about sets of laws of physics in which such a universal machine is not possible. That might sound pointless – why imagine other possible universes? Yet exploring such radical counterfactuals is often an excellent strategy for better understanding our own universe. So there’s a very strange loop here. It’s that the laws of physics determine what kind of computations can be done. And yet the kind of computations which can be done seem to be powerful enough to describe the laws of physics. And that description can then be used to (efficiently!) simulate any physical system: The top part of the loop is almost tautological. The bottom half of the loop is extraordinary. There’s no a priori reason the laws of physics should enable the existence of machines which can simulate physical systems. You might argue on anthropic grounds – we humans are here in the universe, and doing physics pretty successfully, so conditioned on that, it must be true. But that’s not a very satisfactory explanation of why. It remains a mystery. Einstein was right: the fact that the world is comprehensible at all is a miracle. Didn’t remember Remembered Final Reflections We’ve worked through all the basics of the quantum computing model, but we haven’t yet used it in a full-on application – the sort of application which make people excited about quantum computing. But there will soon be available two considerably shorter(!) followup essays, explaining the quantum search algorithm and quantum teleportation. Someone who understands all three essays will have a good understanding of elementary quantum computing. And what of the experimental mnemonic medium we’ve developed in this essay? Mastering new subjects requires internalizing the basic terminology and ideas of the subject. The mnemonic medium should radically speed up this memory step, converting it from a challenging obstruction into a routine step. Frankly, I believe it would accelerate human progress if all the deepest ideas of our civilization were available in a form like this. Perhaps some day. With that said, memory is only part of what it means to understand. Is it possible to build powerful environments which enable deeper forms of understanding? That enable people to take action in new ways, to grow their sense of agency and of ability to contribute? This requires many things beyond memory: the ability to problem solve and to problem find; to connect with opportunities that genuinely matter, and to find pathways for meaningful contribution. We, the team at “Quantum Computing for the Very Curious”, believe there are many powerful and under-exploited patterns available to achieve these goals. In future projects we will explore and develop these patterns. Some of this exploration will continue to be done in the context of quantum computing. But we also hope to launch projects discussing some of humanity’s other great challenges, including optimistic, detailed visions of problems such as space travel, climate change, longevity, and the development of new forms of matter. Thanks for reading this far. In a few days, you’ll receive a notification containing a link to your first review session. In that review session you’ll be retested on the material you’ve learned, helping you further commit it to memory. It should only take a few minutes. In subsequent days you’ll receive more notifications linking you to re-review, gradually working toward genuine long-term memory of all the core material in the essay. Thanks for reading this far. If you’d like to remember the core ideas of this essay durably, please sign in below to set up an account. We’ll track your review schedule and send you occasional reminders containing links that will take you to the review experience. Please sign in so we can save your progress and let you know the best times to review. Thank you! Your progress will be saved as you read. Acknowledgments Michael Nielsen is supported by Y Combinator Research. This essay is based in part on Michael’s earlier lecture series on Quantum Computing for the Determined. Citing this work In academic work, please cite this as: Andy Matuschak and Michael A. Nielsen, “Quantum Computing for the Very Curious”, https://quantum.country/qcvc, San Francisco (2019). Authors are listed in alphabetical order. License This work is licensed under a Creative Commons Attribution-NonCommercial 3.0 Unported License. This means you’re free to copy, share, and build on this essay, but not to sell it. If you’re interested in commercial use, please contact us. Last updated March 18, 2019 See our User Agreement and Privacy Policy."
  },
  {
    "categories": [
      "Computer Science",
      "Quantum Mechanics"
    ],
    "authors": [
      "Andy Matuschak",
      "Michael Nielsen"
    ],
    "title": "How quantum teleportation works",
    "link": "https://quantum.country/teleportation",
    "description": "",
    "content": "How quantum teleportation works How quantum teleportation works Andy Matuschak and Michael Nielsen Part of a series of essays in a mnemonic medium which makes it almost effortless to remember what you read. Quantum computing for the very curious How the quantum search algorithm works How quantum teleportation works Quantum mechanics distilled by Andy Matuschak and Michael Nielsen Presented in a new mnemonic medium which makes it almost effortless to remember what you read. Quantum computing for the very curious How the quantum search algorithm works How quantum teleportation works In-text 5 days 2 weeks 1 month 2 months Long-term The teleportation protocol How to remember the teleportation protocol Does teleportation enable faster-than-light communication? How partial measurements work Verifying the teleportation protocol works Summary of the teleportation protocol Discussion Quantum mechanics distilled Support us on Patreon Our future projects are funded in part by readers like you. Special thanks to our sponsor-level patrons, Adam Wiggins, Andrew Sutherland, Bert Muthalaly, Calvin French-Owen, Dwight Crow, fnnch, James Hill-Khurana, Lambda AI Hardware, Ludwig Petersson, Mickey McManus, Mintter, Patrick Collison, Paul Sutter, Peter Hartree, Sana Labs, Shripriya Mahesh, Tim O'Reilly. According to an old Mexican folk story, on October 24, 1593 a Spanish soldier named Gil Pérez was guarding the Governor's Palace in Manila, in what is today known as the Philippines. The Governor had been assassinated by pirates on the previous night, and the soldiers guarding the palace were exhausted. Tired, Pérez leaned against a wall, and shut his eyes. When he opened his eyes, he was no longer in Manila. Somehow, miraculously, he'd been transported instantaneously across the Pacific Ocean. He was in the Zócalo, the great public square in Mexico City. He was found by guards, who suspected that he was a deserter, due to his uniform, and threw him in jail. But he had the presence of mind to tell the guards of the death of the Governor in Manila. Months later, Pérez's story was confirmed when news of the Governor's death arrived by boat, and he was released Story and text adapted from Wikipedia.. It's an entertaining story. For centuries, teleportation – especially the teleportation of humans! – has been a trope of folk stories, of magic shows, and of science fiction. But in 1993 a group of physicists discovered a genuine type of quantum teleportation, which enables a quantum state to be transported across long distances, without any need to directly send the quantum state The original paper is by Charles H. Bennett, Gille Brassard, Claude Crépeau, Richard Jozsa, Asher Peres, and William K. Wootters, Teleporting an unknown quantum state via dual classical and Einstein-Podolsky-Rosen channels (1993). An account of the discovery has been provided by one of the discoverers: Asher Peres, What is actually teleported? (2003).. The initial quantum teleportation paper was theoretical, using the mathematical rules of quantum mechanics to predict the teleportation phenomenon. That prediction has since been followed by many experiments demonstrating the effect. Furthermore, it turns out that quantum teleportation isn't just a fun stunt. It underlies many other phenomena in quantum computing and, more broadly, in quantum information science. For instance, it's led to important ideas about reducing the effects of noise on quantum computers, and to new hardware ideas for building quantum computers. These connections were a surprise – it's not at all obvious that quantum teleportation should have such applications. But because of applications such as these, teleportation is today viewed as a core primitive in quantum information science. In this essay I explain A note on pronouns: Michael wrote the text of the essay, and will use singular pronouns like “I” to denote the author, and “we” to mean the reader and the author jointly. Andy and Michael developed the mnemonic medium together. how quantum teleportation works. We'll delve deep into the details of the teleportation protocol, and discuss some of the implications of teleportation. To read the essay you need to be familiar with the quantum circuit model of computation. If you're not, you can learn the elements from the earlier essay Quantum Computing for the Very Curious, and you may wish to read that essay now. You shouldn't need any other prerequisites. The essay is presented in an unusual style. It's an example of what Andy Matuschak and I have dubbed a mnemonic medium, incorporating new user interface elements intended to make it almost effortless for you to remember the content of the essay. The motivator is that most people (myself included) quickly forget much of what we read in books and articles. But cognitive scientists know a lot about how human beings commit ideas to memory. The mnemonic medium takes advantage of this understanding, creating an interface which will make it easy to remember the material for the long term. That is, not only will you learn quantum teleportation now: with a little extra effort, you'll remember it near-permanently. More on how that works below. If you look ahead, you'll see that the essay contains a lot of details, and perhaps seems like a lot of work to read. In fact, it is a lot of work to read! Why not just read some popular account of teleportation instead? Or do something else entirely? The payoff is that with less than an hour's work you can understand in full detail an astonishing fact about the way the universe works. And it won't be a handwavy explanation, the kind you find in popular science accounts. It'll be the full story. Furthermore, if you explore quantum information and computation further, you'll find teleportation pops up all over the place. It's a fundamental tool in the toolkit of quantum computing, well nigh as useful as a chef's knife in a kitchen. So let's get on with understanding it. The teleportation protocol What might it mean to teleport a quantum state from one location to another? For simplicity, we'll consider the case of teleporting the simplest possible quantum system – a qubit. We'll suppose an experimentalist, Alice, has in her laboratory a single qubit, in a quantum state ∣ ψ ⟩ = α ∣ 0 ⟩ + β ∣ 1 ⟩ |\\psi\\rangle = \\alpha|0\\rangle+\\beta|1\\rangle ∣ ψ ⟩ = α ∣ 0 ⟩ + β ∣ 1 ⟩ . Alice would like to send her quantum state to a distant laboratory operated by a colleague, Bob. Of course, Alice could simply physically send the qubit. We're going to rule that out by fiat, as violating the desired spirit of teleportation. Another approach would be for Alice to simply tell Bob the amplitudes α \\alpha α and β \\beta β for the quantum state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ . To do this she doesn't need to send a quantum state – she can simply send the complex numbers α \\alpha α and β \\beta β to Bob, as ordinary classical information (perhaps over the internet). Bob could then re-create the state in his laboratory. But in general Alice won't know those amplitudes – there's no particular reason to suppose she knows the identity of her quantum state. It turns out that quantum teleportation works even when the identity of the state isn't known to Alice or Bob. Yet another possible approach is for Alice to somehow measure the state of her qubit, figuring out the amplitudes, and then telling Bob so he can re-create the state. Unfortunately, as I explained elsewhere, in general Alice can get only very limited information about those amplitudes. There's certainly no way she can get anything like enough information for Bob to re-create ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ or a state very close to it. So what can Alice and Bob do? The solution – the quantum teleportation protocol – is sufficiently simple that I'm just going to lay out the steps for you. After I've laid out the steps, it'll be pretty easy for us to verify that it works, and to discuss some implications. Note that you shouldn't expect to immediately see why the protocol works, or why we're using these steps in particular. Indeed, it would be shocking if you could! Rather, the point right now is to begin getting familiar with the basic mechanics of teleportation – the gates and measurements involved. Only later in the essay will we verify that these work, and gradually understand in more depth how to think about the protocol. For Alice and Bob to do the teleportation, in addition to Alice having ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ in her possession, they also need to begin by sharing a special two-qubit state, ∣ 00 ⟩ + ∣ 11 ⟩ 2 , \\frac{|00\\rangle+|11\\rangle}{\\sqrt 2}, 2 ​ ∣ 0 0 ⟩ + ∣ 1 1 ⟩ ​ , one qubit in Alice's possession, the other in Bob's possession. This can be arranged in many ways. One way, for instance, is for Bob to perform the following quantum circuit in his laboratory: With that done, Bob sends one of the two qubits to Alice. It doesn't matter which qubit he sends, since the state is symmetric. We can now depict the three qubits as: Here, the top two qubits belong to Alice. And the third qubit is in Bob's possession. This representation has the drawback that you've got to keep in mind which qubits belong to Alice, and which to Bob. We could make this easier by inserting some vertical space between the second and third qubits (and people sometimes do exactly this), but it's not that much work to remember, so we'll stick with the more compact representation. The quantum circuit language is also a nice way to depict the rest of the quantum teleportation protocol. Here it is: Most of this is pretty transparent. Alice starts with an unknown quantum state, ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ , which is to be teleported. Alice and Bob share a quantum state ∣ 00 ⟩ + ∣ 11 ⟩ 2 \\frac{|00\\rangle+|11\\rangle}{\\sqrt 2} 2 ​ ∣ 0 0 ⟩ + ∣ 1 1 ⟩ ​ . Then Alice performs two gates on her qubits, followed by measuring both of her qubits in the computational basis, with outcomes z z z and x x x . These are just conventional classical bits, each taking the value 0 0 0 or 1 1 1 . The next piece of the protocol is only implied in the circuit representation above. Having measured her qubits, Alice then sends the classical bits z z z and x x x over to Bob. She can do this however she likes – using the internet, or some other classical communication channel. Bob then applies the Pauli X X X gate (i.e., the quantum NOT gate) to his qubit if x = 1 x=1 x = 1 , and otherwise does nothing. This is the meaning of the X x X^x X x notation It's just matrix exponentiation: X 0 = I X^0 = I X 0 = I , the identity matrix, and X 1 = X X^1 = X X 1 = X , the NOT gate.. Similarly, Bob applies the Pauli Z Z Z gate to his qubit if z = 1 z = 1 z = 1 , and otherwise does nothing. The end result is that Bob's qubit is now in the same state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ that Alice started with. That is, Alice has successfully teleported her state to Bob. There's a lot to unpack here. As I said above, you certainly shouldn't expect to immediately see why teleportation works! But as you can see at least the basic mechanics are pretty simple: it's just a few gates, a few measurements, and some classical communication. And, somehow, Bob ends up with the state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ . We'll spend the rest of the essay understanding why it's true, and what some of the implications are. Before going through any of those details, I want to emphasize just how strange the result of the teleportation circuit is. Alice starts out with a state ∣ ψ ⟩ = α ∣ 0 ⟩ + β ∣ 1 ⟩ |\\psi\\rangle = \\alpha|0\\rangle+\\beta|1\\rangle ∣ ψ ⟩ = α ∣ 0 ⟩ + β ∣ 1 ⟩ . She does some stuff to her qubits, and then – without sending Bob any qubit at all – somehow Bob is able to recover ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ , using just two bits of classical information, x x x and z z z . This is quite remarkable. After all, Alice hasn't sent Bob a qubit, just two classical bits. You might say “Oh, well, those two bits must do the job of carrying the information from the qubit”. But that can't possibly be right: just to precisely specify the original amplitudes α \\alpha α and β \\beta β would require, in principle, an infinite amount of classical information. Obviously, neither Alice nor Bob receives that information. Yet Bob is somehow able to recover ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ anyway. This is, in my opinion, a most curious and surprising state of affairs. We'll understand more deeply how it works below. How to remember the teleportation protocol In the introduction I said that this essay is in a new form, a mnemonic medium. That means the medium is designed to make it essentially effortless for you to remember what you read. The way the mnemonic medium works is this: throughout the essay we'll occasionally pause to ask you a few simple questions, testing you on the material just explained. In the days and weeks ahead we'll re-test you in followup review sessions. By expanding the review schedule, we can ensure you consolidate the answers into your long-term memory, while minimizing the study time required. In particular, the expanding review schedule means that each extra minute spent studying provides more and more benefit, a kind of exponential return. The review sessions take no more than a few minutes per day, and we'll notify you when you need to review. The benefit is that instead of remembering how quantum teleportation works for a few hours or days, you'll remember for years; it'll become a much more deeply internalized part of your thinking. That may sound a strange aspiration. But if you're genuinely interested in understanding quantum computing, then having teleportation down cold is necessary. To give you a more concrete flavor of how the mnemonic medium works, let's take a look at three questions reviewing part of what you've just learned. Please indulge me by answering these questions – it'll take just a few seconds. For each question, think about what you believe the answer to be, click to reveal the actual answer, and then mark whether you remembered or not. If you can recall, that's great. But if not – and most readers don't get the answers to these questions correct(!) – I'll have some comments on how to think about it below. Didn’t remember Remembered I said above that most readers don't recall the answers to these questions. It's worth thinking about what this means. The questions ask about some of the most absolutely basic things about the quantum teleportation protocol. If someone is not getting these questions correct, what are they really learning about quantum teleportation? If you're in this boat, I challenge you to name three specific things that you've learned about quantum teleportation so far. Genuine learning requires paying close attention to what you're reading. In fact, it's not difficult to learn any of the three things tested in the questions above, if you're paying attention. I don't mean to be a downer. But I also think it's important to be realistic. Most people (myself included) learn very little from most of what we read, unless we're paying close attention. The reading may be entertaining, or produce a brief illusion of understanding. But you can only learn if you pay attention. The questions are good way of monitoring whether that's the case. And reviewing them again in the future will help you internalize this understanding for the long term. You may object that there's not much point in knowing “unimportant details” like the circuit used to generate Alice and Bob's shared state. It's tempting to take refuge in a belief that what you're looking for is a broad, conceptual understanding. Unfortunately, I've never met someone knowledgeable about quantum computing who didn't know the details of teleportation. So I don't buy the “unimportant details” argument. Imagine meeting someone who told you that they “had a broad conceptual understanding” of how to speak Spanish, but it turned out they didn't know the meanings of hola, adiós, or bien. You'd think their claim to a broad conceptual understanding of Spanish was hilarious. If you want to understand quantum computing and related subjects, you need to know the details of how the teleportation protocol works. That means knowing things like what state Alice and Bob initially share. What's more, it means not just knowing them immediately after reading. It means internalizing them for the long term. If you're interested in doing that, then I invite you to set up an account by signing in below. If you do so, your review schedule for each question in the essay will be tracked, and you'll receive reminders each day (or few days), containing a link which takes you to an online review session. That review session isn't this full essay – rather, it looks just like the question set you answered above, but contains instead all the questions which are due, so you can quickly run through them. The time commitment is no more than a few minutes per day. You can study on your phone while grabbing coffee, or standing in line, or going for a walk, or in transit. The return for that small time commitment is an internalized understanding of quantum teleportation, retained for years instead of days. To keep this promise, we'll track your review schedule for each question, and send you occasional reminders to check in, and to run through the questions which are due. You can review on your phone while grabbing coffee, or standing in line, or going for a walk, or on transit. The return for that small time commitment is an internalized understanding of quantum teleportation; it'll become a part of who you are, retained for years instead of days. Please sign in so we can save your progress and let you know the best times to review. Thank you! Your progress will be saved as you read. Does teleportation enable faster-than-light communication? Before we verify that the teleportation circuit works, let's briefly discuss one of the most common questions about quantum teleportation: does it enable faster-than-light communication? At first, it looks as though it may – after all, Alice is able to transmit her state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ to Bob, even if he's very distant from her. It'd be quite marvelous if it enabled faster-than-light communication, since that in turn would give rise to many incredible phenomena, including the ability to send information backward in time. But while it would be marvelous, it is not possible. You can see the trouble if you think closely about the protocol. Remember, for Bob to recover the state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ , Alice must send Bob two bits of classical information. The speed of that transmission is limited by the speed of light. Without that classical information, Bob can't guarantee that he recovers ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ . Instead, what he has is a distribution over four different possible states. And while I won't prove it here, it turns out to be possible to prove that with only that distribution over states, no information is transferred from Alice to Bob. It's a pity, but that's the way the world seems to work. Let's pause to quickly review a few more questions about teleportation: Didn’t remember Remembered Notice that the second question is a more qualitative style of question than the earlier questions. Your answer may not exactly match the answer given. It's up to you to decide whether you want to mark yourself correct or not. Ask yourself: have I really understood the core point? If so, mark yourself correct. If not, don't! The point of all the questions is to serve you, and it's up to you to decide how best they can do that. How partial measurements work To verify that the teleportation protocol works, we'll mostly use tools already introduced in the earlier essay Quantum Computing for the Very Curious. But there's one missing piece of background knowledge we need to fill in first. In the earlier essay I explained how to do computational basis measurements for multi-qubit systems. But I didn't explain what happens if you measure just some (but not all) of the qubits. This is relevant to quantum teleportation, since we're going to be measuring just two of three qubits. The rule for describing such partial measurements is simple, though slightly cumbersome to describe. First, I'll describe it for a two-qubit system, and then explain how it generalizes. Suppose we have a two-qubit system in the state a ∣ 00 ⟩ + b ∣ 01 ⟩ + c ∣ 10 ⟩ + d ∣ 11 ⟩ . a|00\\rangle+b|01\\rangle+c|10\\rangle+d|11\\rangle. a ∣ 0 0 ⟩ + b ∣ 0 1 ⟩ + c ∣ 1 0 ⟩ + d ∣ 1 1 ⟩ . Suppose we measure just the first qubit in the computational basis. We'd like to know (i) what the probabilities for the two measurement outcomes are; and (ii) what the corresponding resulting state of the second qubit will be. To answer these questions, we simply group terms corresponding to ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ on the first qubit, rewriting the state as ∣ 0 ⟩ ( a ∣ 0 ⟩ + b ∣ 1 ⟩ ) + ∣ 1 ⟩ ( c ∣ 0 ⟩ + d ∣ 1 ⟩ ) . |0\\rangle(a|0\\rangle+b|1\\rangle)+ |1\\rangle(c|0\\rangle+d|1\\rangle). ∣ 0 ⟩ ( a ∣ 0 ⟩ + b ∣ 1 ⟩ ) + ∣ 1 ⟩ ( c ∣ 0 ⟩ + d ∣ 1 ⟩ ) . Suppose we measure in the computational basis on the first qubit, and obtain the result 0 0 0 . It probably won't surprise you that the resulting state of the second qubit is just the corresponding piece from the above expression, namely a ∣ 0 ⟩ + b ∣ 1 ⟩ a|0\\rangle+b|1\\rangle a ∣ 0 ⟩ + b ∣ 1 ⟩ , normalized by dividing by ∣ a ∣ 2 + ∣ b ∣ 2 \\sqrt{|a|^2+|b|^2} ∣ a ∣ 2 + ∣ b ∣ 2 ​ , so it's a properly normalized quantum state: a ∣ 0 ⟩ + b ∣ 1 ⟩ ∣ a ∣ 2 + ∣ b ∣ 2 \\frac{a|0\\rangle+b|1\\rangle}{\\sqrt{|a|^2+|b|^2}} ∣ a ∣ 2 + ∣ b ∣ 2 ​ a ∣ 0 ⟩ + b ∣ 1 ⟩ ​ This result occurs with probability ∣ a ∣ 2 + ∣ b ∣ 2 |a|^2+|b|^2 ∣ a ∣ 2 + ∣ b ∣ 2 . The resulting state of the first qubit is, of course, ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ . Similarly, if the result from the computational basis measurement is 1 1 1 , then the corresponding conditional state for the second qubit is c ∣ 0 ⟩ + d ∣ 1 ⟩ ∣ c ∣ 2 + ∣ d ∣ 2 , \\frac{c|0\\rangle+d|1\\rangle}{\\sqrt{|c|^2+|d|^2}}, ∣ c ∣ 2 + ∣ d ∣ 2 ​ c ∣ 0 ⟩ + d ∣ 1 ⟩ ​ , and this occurs with probability ∣ c ∣ 2 + ∣ d ∣ 2 |c|^2+|d|^2 ∣ c ∣ 2 + ∣ d ∣ 2 . The resulting state of the first qubit is, of course, ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ . I won't write the general rule out in absolutely full generality, but hopefully it's pretty clear what the rule is. For instance, suppose we measure the first two qubits of a many-qubit system in the computational basis. To figure out the result, we express the state immediately prior to measurement as ∣ 00 ⟩ ∣ ψ 00 ⟩ + ∣ 01 ⟩ ∣ ψ 01 ⟩ + ∣ 10 ⟩ ∣ ψ 10 ⟩ + ∣ 11 ⟩ ∣ ψ 11 ⟩ . |00\\rangle|\\psi_{00}\\rangle + |01\\rangle|\\psi_{01}\\rangle +|10\\rangle|\\psi_{10}\\rangle + |11\\rangle|\\psi_{11}\\rangle. ∣ 0 0 ⟩ ∣ ψ 0 0 ​ ⟩ + ∣ 0 1 ⟩ ∣ ψ 0 1 ​ ⟩ + ∣ 1 0 ⟩ ∣ ψ 1 0 ​ ⟩ + ∣ 1 1 ⟩ ∣ ψ 1 1 ​ ⟩ . The result of measuring the first two qubits in the computational basis is 00 00 0 0 with probability ∥ ∣ ψ 00 ⟩ ∥ 2 \\| |\\psi_{00}\\rangle \\|^2 ∥ ∣ ψ 0 0 ​ ⟩ ∥ 2 , and the resulting state of the other qubits in the system is the normalized state ∣ ψ 00 ⟩ / ∥ ∣ ψ 00 ⟩ ∥ |\\psi_{00}\\rangle/\\||\\psi_{00}\\rangle\\| ∣ ψ 0 0 ​ ⟩ / ∥ ∣ ψ 0 0 ​ ⟩ ∥ . The resulting state of the first two qubits is ∣ 00 ⟩ |00\\rangle ∣ 0 0 ⟩ . It works similarly for the other possible outcomes. As I said above, this rule is a little bit cumbersome, but with some practice it becomes easy to use fluently. We'll get an opportunity very shortly, as we verify the teleportation protocol. To help you get used to the rule, it's worth taking a few minutes to work through the exercise immediately below. Unlike the review questions, the point of the exercise isn't as an aid to memory. Rather, it's here because it will help you better understand the material just introduced. Note that even if you don't work through the exercise, it's worth at least reading through it, since some of the results will be tested in the review questions. Exercise: Suppose we have a quantum state 0.8 ∣ 01 ⟩ + 0.2 ∣ 10 ⟩ \\sqrt{0.8} |01\\rangle+\\sqrt{0.2}|10\\rangle 0 . 8 ​ ∣ 0 1 ⟩ + 0 . 2 ​ ∣ 1 0 ⟩ and measure the first qubit in the computational basis. What is the probability the measurement gives 0 0 0 as outcome? What is the corresponding state of the second qubit? What is the probability the measurement gives 1 1 1 as the outcome? What is the corresponding state of the second qubit? Answer / spoilers: It's worth your time taking a shot at the exercise, since even if you get stuck, learning to cope with being stuck is a much-needed superpower in quantum computing. But after you've done that, here are the answers: the probability of the outcome 0 0 0 is 0.8 0.8 0 . 8 and the corresponding state of the second qubit is ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ ; the probability of the outcome 1 1 1 is 0.2 0.2 0 . 2 and the corresponding state of the second qubit is ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ . Didn’t remember Remembered Some of these questions perhaps seem peculiar. Obviously, there's little long-term value in remembering the specific probability of getting outcome 0 0 0 when we measure the first qubit of 0.8 ∣ 01 ⟩ + 0.2 ∣ 10 ⟩ \\sqrt{0.8}|01\\rangle+\\sqrt{0.2}|10\\rangle 0 . 8 ​ ∣ 0 1 ⟩ + 0 . 2 ​ ∣ 1 0 ⟩ in the computational basis. Rather, you should think of this question (and similar questions) as really implicitly asking if you remember how to compute the probability. And marking your answer appropriately. That procedural knowledge really is valuable to remember. Verifying the teleportation protocol works We now have all the tools necessary to verify that teleportation works. In fact, if you're feeling enthusiastic, you can do the verification yourself. I won't pretend that it's not some work. On the other hand, if you push through the calculation you can take away a lot of confidence that you can do nontrivial calculations in quantum mechanics. With that said, let me show you one approach to doing the verification. (By the way, none of the in-essay review questions will be on the details of the verification. So while you should follow along, you don't need to remember every detail. There wouldn't be much point – there are many ways of doing the verification, and what's important is that you are able to do it somehow, not that you remember any particular approach.) Let's start by recalling the circuit depicting the protocol: Writing ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ explicitly in terms of its amplitudes, ∣ ψ ⟩ = α ∣ 0 ⟩ + β ∣ 1 ⟩ |\\psi\\rangle = \\alpha|0\\rangle+\\beta|1\\rangle ∣ ψ ⟩ = α ∣ 0 ⟩ + β ∣ 1 ⟩ , we see that the initial state at the start of the teleportation protocol is: ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) ∣ 00 ⟩ + ∣ 11 ⟩ 2 . (\\alpha|0\\rangle+\\beta|1\\rangle) \\frac{|00\\rangle+|11\\rangle}{\\sqrt 2}. ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) 2 ​ ∣ 0 0 ⟩ + ∣ 1 1 ⟩ ​ . We can expand this out as α ∣ 000 ⟩ + α ∣ 011 ⟩ + β ∣ 100 ⟩ + β ∣ 111 ⟩ 2 . \\frac{\\alpha|000\\rangle+\\alpha|011\\rangle+\\beta|100\\rangle+\\beta|111\\rangle}{\\sqrt 2}. 2 ​ α ∣ 0 0 0 ⟩ + α ∣ 0 1 1 ⟩ + β ∣ 1 0 0 ⟩ + β ∣ 1 1 1 ⟩ ​ . We apply the CNOT to the first two qubits to obtain α ∣ 000 ⟩ + α ∣ 011 ⟩ + β ∣ 110 ⟩ + β ∣ 101 ⟩ 2 . \\frac{\\alpha|000\\rangle+\\alpha|011\\rangle+\\beta|110\\rangle+\\beta|101\\rangle}{\\sqrt 2}. 2 ​ α ∣ 0 0 0 ⟩ + α ∣ 0 1 1 ⟩ + β ∣ 1 1 0 ⟩ + β ∣ 1 0 1 ⟩ ​ . Then we apply the Hadamard gate to the first qubit. Recall from the earlier essay that H ∣ 0 ⟩ = ∣ 0 ⟩ + ∣ 1 ⟩ 2 H|0\\rangle = \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} H ∣ 0 ⟩ = 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ and H ∣ 1 ⟩ = ∣ 0 ⟩ − ∣ 1 ⟩ 2 H|1\\rangle = \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} H ∣ 1 ⟩ = 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ . So after the Hadamard gate on the first qubit the state is α ∣ 000 ⟩ + α ∣ 100 ⟩ + α ∣ 011 ⟩ + α ∣ 111 ⟩ + β ∣ 010 ⟩ − β ∣ 110 ⟩ + β ∣ 001 ⟩ − β ∣ 101 ⟩ 2 . \\frac{\\alpha|000\\rangle+\\alpha|100\\rangle+\\alpha|011\\rangle+\\alpha|111\\rangle+\\beta|010\\rangle-\\beta|110\\rangle+\\beta|001\\rangle-\\beta|101\\rangle}{2}. 2 α ∣ 0 0 0 ⟩ + α ∣ 1 0 0 ⟩ + α ∣ 0 1 1 ⟩ + α ∣ 1 1 1 ⟩ + β ∣ 0 1 0 ⟩ − β ∣ 1 1 0 ⟩ + β ∣ 0 0 1 ⟩ − β ∣ 1 0 1 ⟩ ​ . To analyze the computational basis measurement on the first two qubits, we group terms corresponding to each computational basis state for those qubits. The state above can be rewritten ∣ 00 ⟩ ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) + ∣ 01 ⟩ ( α ∣ 1 ⟩ + β ∣ 0 ⟩ ) + ∣ 10 ⟩ ( α ∣ 0 ⟩ − β ∣ 1 ⟩ ) + ∣ 11 ⟩ ( α ∣ 1 ⟩ − β ∣ 0 ⟩ ) 2 . \\frac{|00\\rangle(\\alpha|0\\rangle+\\beta|1\\rangle)+ |01\\rangle(\\alpha|1\\rangle+\\beta|0\\rangle)+ |10\\rangle(\\alpha|0\\rangle-\\beta|1\\rangle)+ |11\\rangle(\\alpha|1\\rangle-\\beta|0\\rangle) }{2}. 2 ∣ 0 0 ⟩ ( α ∣ 0 ⟩ + β ∣ 1 ⟩ ) + ∣ 0 1 ⟩ ( α ∣ 1 ⟩ + β ∣ 0 ⟩ ) + ∣ 1 0 ⟩ ( α ∣ 0 ⟩ − β ∣ 1 ⟩ ) + ∣ 1 1 ⟩ ( α ∣ 1 ⟩ − β ∣ 0 ⟩ ) ​ . When Alice measures in the computational basis, the outcome is 00 00 0 0 with probability given by ∣ α ∣ 2 / 4 + ∣ β ∣ 2 / 4 = 1 4 |\\alpha|^2/4+|\\beta|^2/4 = \\frac{1}{4} ∣ α ∣ 2 / 4 + ∣ β ∣ 2 / 4 = 4 1 ​ , since ∣ α ∣ 2 + ∣ β ∣ 2 = 1 |\\alpha|^2+|\\beta|^2=1 ∣ α ∣ 2 + ∣ β ∣ 2 = 1 (normalization condition for the original state). And the resulting conditional state for Bob is α ∣ 0 ⟩ + β ∣ 1 ⟩ \\alpha|0\\rangle+\\beta|1\\rangle α ∣ 0 ⟩ + β ∣ 1 ⟩ , that is, just the original state to be teleported. We can run through similar calculations for all four outcomes of the measurement in Alice's computational basis. The results are: Outcome Probability Bob's state 00 1 4 \\frac{1}{4} 4 1 ​ α ∣ 0 ⟩ + β ∣ 1 ⟩ = ∣ ψ ⟩ \\alpha|0\\rangle+\\beta|1\\rangle = |\\psi\\rangle α ∣ 0 ⟩ + β ∣ 1 ⟩ = ∣ ψ ⟩ 01 1 4 \\frac{1}{4} 4 1 ​ α ∣ 1 ⟩ + β ∣ 0 ⟩ = X ∣ ψ ⟩ \\alpha|1\\rangle+\\beta|0\\rangle = X|\\psi\\rangle α ∣ 1 ⟩ + β ∣ 0 ⟩ = X ∣ ψ ⟩ 10 1 4 \\frac{1}{4} 4 1 ​ α ∣ 0 ⟩ − β ∣ 1 ⟩ = Z ∣ ψ ⟩ \\alpha|0\\rangle-\\beta|1\\rangle = Z|\\psi\\rangle α ∣ 0 ⟩ − β ∣ 1 ⟩ = Z ∣ ψ ⟩ 11 1 4 \\frac{1}{4} 4 1 ​ α ∣ 1 ⟩ − β ∣ 0 ⟩ = X Z ∣ ψ ⟩ \\alpha|1\\rangle-\\beta|0\\rangle = XZ|\\psi\\rangle α ∣ 1 ⟩ − β ∣ 0 ⟩ = X Z ∣ ψ ⟩ This is good news: in each case Bob's state is very similar to the original state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ to be teleported. The only thing wrong is the extra Pauli matrices in Bob's conditional states. But Bob can easily fix those. To do the fix, recall that the Pauli matrices are self-inverse, X X = I XX = I X X = I and Z Z = I ZZ = I Z Z = I . If we simply run through the four possibilities in the table above, we see that Bob can recover the original state by (in the respective cases): doing nothing; applying the X X X gate; applying the Z Z Z gate; applying Z X ZX Z X . This is exactly what's done in the circuit described earlier. That completes the verification that teleportation works. Intuitively, one story you might tell yourself about why teleportation works is that somehow the measurement outcomes are “telling us something about the identity of the state to be teleported”, something that helps Bob put the state back together again. While such a story seems appealing, it's wrong. Consider that the probabilities for the computational basis measurements are 1 4 \\frac{1}{4} 4 1 ​ , no matter what the identity of the state was Contrast to a computational basis measurement of a state such as α ∣ 0 ⟩ + β ∣ 1 ⟩ \\alpha|0\\rangle+\\beta|1\\rangle α ∣ 0 ⟩ + β ∣ 1 ⟩ , where the probabilities of the outcomes very strongly depend on the amplitudes α \\alpha α and β \\beta β , and so on the identity of the state.. Imagine a third party, Carol, eavesdropped on Alice and Bob's classical communication, and learned the results of Alice's measurements. Because the distribution of those measurement results doesn't depend in any way on the identity of the state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ , Carol would still be completely in the dark about the identity of ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ . Even after years of familiarity with teleportation, I still find this marvelous and surprising. What the measurement results are saying is purely how the state has been changed – to ∣ ψ ⟩ , X ∣ ψ ⟩ , Z ∣ ψ ⟩ |\\psi\\rangle, X|\\psi\\rangle, Z|\\psi\\rangle ∣ ψ ⟩ , X ∣ ψ ⟩ , Z ∣ ψ ⟩ , or X Z ∣ ψ ⟩ XZ|\\psi\\rangle X Z ∣ ψ ⟩ – without giving any information at all about the identity of ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ . There's no immediate further moral to this story. I'm mentioning it merely to help flesh out your appreciation for the teleportation protocol. (We'll do more in this vein a little later in the essay). Although the protocol is technically simple, it's very deep. Indeed, I believe there are still unsuspected depths in the protocol, ideas that no-one has yet fathomed. One further fun fact about teleportation is that it doesn't matter where Bob is. In fact, Alice may not even know where Bob is – she can simply broadcast the classical bits out into the world, perhaps using a public address on the internet. Provided Bob can see those bits, he can recover the state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ . So teleportation is a kind of broadcast protocol. Didn’t remember Remembered Summary of the teleportation protocol We now have a basic picture of quantum teleportation, and have verified that it works. Let me summarize the key elements, followed by some questions reviewing those elements of the protocol not covered by earlier questions. The quantum circuit representation for teleportation is as follows: In more descriptive prose, teleportation is achieved in four steps: Initial state: Alice starts with a quantum state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ of a single qubit. Alice and Bob also share a quantum state of two qubits, ∣ 00 ⟩ + ∣ 11 ⟩ 2 \\frac{|00\\rangle+|11\\rangle}{\\sqrt 2} 2 ​ ∣ 0 0 ⟩ + ∣ 1 1 ⟩ ​ . What Alice does: To accomplish her part of the protocol, Alice performs a CNOT gate between ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ and her other qubit, then applies a Hadamard gate to the first qubit. Alice then measures both her qubits in the computational basis, getting results z = 0 z = 0 z = 0 or 1 1 1 and x = 0 x = 0 x = 0 or 1 1 1 . The probability for each of the four possible measurement outcomes ( 00 , 01 , 10 , 11 00, 01, 10, 11 0 0 , 0 1 , 1 0 , 1 1 ) is 1 / 4 1/4 1 / 4 . Classical communication: Alice sends both classical bits, z z z and x x x , to Bob. Bob recovers the quantum state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ : Bob applies Z z X x Z^z X^x Z z X x to his qubit, recovering the original state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ . One final point: at the end of the teleportation protocol, Alice no longer possesses the quantum state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ . In particular, her computational basis measurement leaves her qubits in one of the four states, ∣ 00 ⟩ |00\\rangle ∣ 0 0 ⟩ , ∣ 01 ⟩ |01\\rangle ∣ 0 1 ⟩ , ∣ 10 ⟩ |10\\rangle ∣ 1 0 ⟩ , or ∣ 11 ⟩ |11\\rangle ∣ 1 1 ⟩ , corresponding to the result of her computational basis measurement. And so you shouldn't think of teleportation as copying the state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ , but rather as a way of moving the state. Let's now review some questions covering those details of teleportation not covered by earlier questions. Note that I will ask several of the questions in closely related ways. This may seem redundant, but it's done because research on memory shows that encoding information in multiple related ways results in deeper and better memories being formed. This is the biggest batch of questions in the essay, and may take a couple of minutes to work through. Didn’t remember Remembered Discussion Quantum teleportation is different in several ways from what people ordinarily think of as teleportation, the kind of thing made famous by Star Trek. For one thing, it's not about teleporting complex objects, like human beings. Rather, it's about teleporting elementary quantum systems. Although in principle it is possible to teleport much more complex objects, it seems extremely unlikely in the foreseeable future. Another difference is that quantum teleportation is not about an object vanishing in one location and then reappearing instantaneously in another. It's essential that the classical information be sent, and that Bob performs the corresponding operations. This makes some people feel cheated: “it's not real teleportation!” While the name “quantum teleportation” is great marketing, it genuinely is a little misleading. On the other hand, quantum teleportation is still astonishing. It's impossible to measure a quantum state to determine its amplitudes. Intuitively, that ought to make anything like quantum teleportation impossible. Yet, somehow, it is still possible to use measurement to transport a state from one location to another. One of the originators of quantum teleportation, Asher Peres, was once asked by a reporter whether it is possible to teleport not only the [human] body but also the soul. Peres captured the essence of quantum teleportation beautifully when he replied “only the soul” Asher Peres, What is actually teleported? (2003).. Teleportation isn't really about transmitting objects over long distances. Rather, it's about a counter-intuitive way of disassembling an unknown quantum state into classical information, using a fixed shared state and measurement, and then later recovering the original quantum state. In lectures, another of the originators of teleportation, Charles Bennett, has sometimes illustrated this using an elegant informal inequality: 1 ebit + 2 cbits ≥ \\geq ≥ 1 qubit What this means, informally, is that one shared “ebit” (meaning the entangled state ∣ 00 ⟩ + ∣ 11 ⟩ 2 \\frac{|00\\rangle+|11\\rangle}{\\sqrt 2} 2 ​ ∣ 0 0 ⟩ + ∣ 1 1 ⟩ ​ ) together with two classical bits of communication is enough to enable one quantum bit of communication. It's teleportation-expressed-as-an-inequality. Bennett sometimes combines this with inequalities expressing other quantum information processing protocols, and is able to develop a kind of algebra of what's possible. Not everyone finds this to their taste, but personally I find it a fun and stimulating way of thinking about what's going on. Teleportation has been experimentally demonstrated in many systems, beginning in the late 1990s. The experiments have been somewhat contentious, with four separate teams claiming to be the first to “really” do teleportation D. Boschi, S. Branca, F. De Martini, L. Hardy, and S. Popescu, Experimental Realization of Teleporting an Unknown Pure Quantum State via Dual Classical and Einstein-Podolsky-Rosen Channels (1997); Dik Bouwmeester, Jian-Wei Pan, Klaus Mattle, Manfred Eibl, Harald Weinfurter, and Anton Zeilinger, Experimental Quantum Teleportation (1997); A. Furusawa, J. L. Sørensen, S. L. Braunstein, C. A. Fuchs, H. J. Kimble, and E. S. Polzik, Unconditional Quantum Teleportation (1998); M. A. Nielsen, E. Knill, and R. Laflamme, Complete quantum teleportation using nuclear magnetic resonance (1998).; each team adopted a somewhat different criterion for what it means to succeed. I'm not a disinterested observer: I was on one of the teams, and unsurprisingly I think our criterion was the best; others have different opinions. In the years since many more experiments have been done, improving the quality of the implementation, and also using teleportation as part of more complex quantum information processing protocols. There are many variations of the quantum teleportation protocol. I've described the simplest version, one that works to teleport qubits. But it's worth knowing that it's possible to extend the protocol to more complex quantum systems. There are also variations which use a different state shared between Alice and Bob, different operations performed by Alice and Bob, and so on. The version of teleportation I've described is, however, the one people are usually referring to when they say “quantum teleportation”. If you want a fun challenge, you might try to find some variations on the protocol. For instance, can you find a way of teleporting a qubit using the shared state ∣ 01 ⟩ + ∣ 10 ⟩ 2 \\frac{|01\\rangle+|10\\rangle}{\\sqrt 2} 2 ​ ∣ 0 1 ⟩ + ∣ 1 0 ⟩ ​ ? And are there other shared states you can find which can be used to do teleportation? I've described how teleportation works, and we've verified that it does work. But it's still somewhat mysterious. How could you have come to discover teleportation in the first place? Why would you even suspect it's possible? Quantum mechanics took its near-modern form in the 1920s, and teleportation could have been discovered then. Certainly, more technically complex discoveries were made by physicists at the time. Yet, despite that, it wasn't discovered until 1993. Although technically rather simple, I believe teleportation wasn't discovered in part because it was conceptually unexpected. While I won't give a detailed answer to the question “what could lead you to discover teleportation”, if you're interested you may enjoy this Twitter thread, which provides a partial answer, and which should be possible to follow with the background in this essay. Why does teleportation matter? While it's a simple protocol it opens up a world of questions, leading to new ideas and new applications. You could write a fun book containing nothing but ideas inspired by or building on teleportation. I won't survey all those here, but just to give you the flavor I will mention one line of development. I hope you pardon me, but it's something I played a small role in. In 1997, Ike Chuang and I M. A. Nielsen and Isaac L. Chuang, Programmable Quantum Gate Arrays (1997). showed that it's not only possible to teleport quantum states, but by modifying the protocol it's possible to teleport quantum gates from one location to another. Our gate teleportation protocol was stochastic, meaning it only worked some of the time, but in 1999, Chuang and Daniel Gottesman Daniel Gottesman and Isaac L. Chuang, Demonstrating the viability of universal quantum computation using teleportation and single-qubit operations (1999). pointed out that for some gates quantum gate teleportation could be made to work all the time. This perhaps seems nice, but of mostly theoretical interest. However, in 2001 the scientists Manny Knill, Ray Laflamme, and Gerard Milburn E. Knill, R. Laflamme, and G. J. Milburn, A scheme for efficient quantum computation with linear optics (2001)., used quantum gate teleportation to show something unexpected. At the time, experts thought particles of light (photons) were likely to be a bad choice for use as qubits in quantum computers. While photons are in many ways excellent candidates to be qubits – it's easy to do many types of manipulation, and they're quite resistant to noise – there seemed to be no way of getting photons to interact to do a CNOT gate. But Knill, Laflamme, and Milburn found a beautiful way of teleporting the CNOT gate onto photons. This provided an in-principle (though initially very complex) way of building an optical quantum computer. That construction has since been simplified by many orders of magnitude, using yet more ideas related to teleportation. Today, there are startup companies working toward optical quantum computers that build on this approach. Back in 1993, when teleportation was discovered, I doubt anyone anticipated that it would one day lead to new approaches to quantum computing. But teleportation is such a deep and powerful primitive that it keeps giving rise to new ideas, of which this is merely one example. Thanks for reading. In a few days, you’ll receive a notification containing a link to your first review session for this essay. In that review session you’ll be retested on the material you’ve learned, helping you further commit it to memory. It should only take a few minutes. In subsequent days you’ll receive more notifications linking you to re-review, gradually working toward genuine long-term memory of all the core material in the essay. Concluding thought: Thanks for reading this account of quantum teleportation. If you’d like to remember the core ideas durably, please set up an account below. We’ll track your review schedule and send you occasional reminders containing links that will take you to the review experience. Please sign in so we can save your progress and let you know the best times to review. Thank you! Your progress will be saved as you read. Acknowledgments Andy and Michael are supported in part through the generous contributions of our Patreon supporters. Early work on this essay was supported by Y Combinator Research. This essay is based in part on Michael’'s earlier lecture series on Quantum Computing for the Determined. Citing this work In academic work, please cite this as: Andy Matuschak and Michael A. Nielsen, “How Quantum Teleportation Works”, https://quantum.country/teleportation, San Francisco (2019). Authors are listed in alphabetical order. License This work is licensed under a Creative Commons Attribution-NonCommercial 3.0 Unported License. This means you’re free to copy, share, and build on this essay, but not to sell it. If you’re interested in commercial use, please contact us. Last updated November 12, 2019 See our User Agreement and Privacy Policy."
  },
  {
    "categories": [
      "Computer Science",
      "Quantum Mechanics"
    ],
    "authors": [
      "Andy Matuschak",
      "Michael Nielsen"
    ],
    "title": "Quantum mechanics distilled",
    "link": "https://quantum.country/qm",
    "description": "",
    "content": "Quantum mechanics distilled Quantum mechanics distilled Andy Matuschak and Michael Nielsen Part of a series of essays in a mnemonic medium which makes it almost effortless to remember and apply what you read. Quantum computing for the very curious How the quantum search algorithm works How quantum teleportation works Quantum mechanics distilled by Andy Matuschak and Michael Nielsen Presented in a new mnemonic medium which makes it almost effortless to remember and apply what you read. Quantum computing for the very curious How the quantum search algorithm works How quantum teleportation works Quantum mechanics distilled In-text 5 days 2 weeks 1 month 2 months Long-term Part I: The postulates of quantum mechanics Part II: Why are so many physicists so upset about quantum mechanics? Support us on Patreon Our future projects are funded in part by readers like you. Special thanks to our sponsor-level patrons, Adam Wiggins, Andrew Sutherland, Bert Muthalaly, Calvin French-Owen, Dwight Crow, fnnch, James Hill-Khurana, Lambda AI Hardware, Ludwig Petersson, Mickey McManus, Mintter, Patrick Collison, Paul Sutter, Peter Hartree, Sana Labs, Shripriya Mahesh, Tim O'Reilly. Quantum mechanics: real black magic calculus. — Albert Einstein, writing to a colleague in 1925 In fairy tales, a wizard will say the magic words of a spell, or inscribe a set of magic symbols, and the world will change. Fireballs shoot from their hands; the hero is revived from an endless slumber; a couple falls in love. We smile when we hear such stories. They seem quaint but implausible. Common sense tells us that merely speaking certain words, or inscribing certain symbols, cannot cause such changes in the world. And yet our scientific theories are not so different. By speaking the words or inscribing the symbols of such a theory, we can greatly deepen our understanding of the world. That new understanding then enables us to change the world in ways that formerly seemed impossible, even inconceivable. Consider quantum mechanics, one of our deepest theories. It has helped us create lasers, semiconductor chips, and magnetic resonance imaging. One day it will likely help us create quantum computers, perhaps even quantum intelligences. Quantum mechanics is magic that actually works. In this essay, we explain quantum mechanics in detail. We will describe all the principles of quantum mechanics in depth, nothing held back. It's not a handy-wavy treatment, of the kind often found in articles written for a general audience. While such articles can be entertaining, trying to learn quantum mechanics by reading them is like learning to play basketball by merely watching basketball being played. This essay will get you out on the mathematical court of quantum mechanics. Of course, you won't learn to slam dunk, at least not yet. But the essay will ground you in an understanding of the fundamentals of quantum mechanics, which you can later build upon and extend. To read the essay, you must first understand the quantum circuit model of quantum computation. If you're not familiar with quantum circuits, you can learn about them from our earlier essay, Quantum Computing for the Very Curious. You may wish to pause to read that essay now, if you haven't already. Once you've understood that material you shouldn't need any other prerequisites. Indeed, when you learned quantum computing, you learned in passing almost all of quantum mechanics. In Part I of this essay we distill those past ideas, collecting them up into the package known as quantum mechanics. Although quantum mechanics is not so difficult to learn, it's a theory which has disturbed many people. Here's a few classic quotes on this puzzlement: I think I can safely say that nobody understands quantum mechanics … Do not keep saying to yourself, if you can possibly avoid it, “But how can it be like that?” because you will get “down the drain”, into a blind alley from which nobody has yet escaped. Nobody knows how it can be like that. – Richard Feynman I have thought a hundred times as much about the quantum problems as I have about general relativity theory. – Albert Einstein If quantum mechanics hasn't profoundly shocked you, you haven't understood it yet. – Niels Bohr If quantum mechanics is not so difficult to learn, why has it so disturbed many great physicists? What do they mean when they say they don't understand it, or are shocked by it? How can a scientific theory be both beautiful and disturbing? In Part II of this essay we'll explore these questions. As we'll see, despite its simplicity quantum mechanics raises striking conceptual problems, problems which are among the most exciting open problems in science today. The essay is presented in an unusual form. It's an example of what we call a mnemonic essay, written in the mnemonic medium. That means it incorporates new user interface elements intended to make it almost effortless for you to remember and apply the ideas in the essay. The motivator is that most people (ourselves included) quickly forget much of what we read in books and articles. The mnemonic medium takes advantage of what cognitive scientists know about how humans learn, creating an interface which ensures you'll remember the ideas and how to apply them near permanently. More on how that works below. Part I: The postulates of quantum mechanics So, what is quantum mechanics? It's fun to wax poetic about it being magic that actually works, or to quote eminent scientists saying no-one really understands it. But while fun, those statements give no direct enlightenment. What is quantum mechanics, really? Quantum mechanics is simply this: it's a set of four postulates that provide a mathematical framework for describing the universe and everything in it. These postulates reflect ideas you've already seen in the quantum circuit model: how to describe a quantum state; how to describe the dynamics of a quantum system; and so on. But rather than talk abstractly, it's better to just see the first postulate: The first postulate: state space Postulate 1: Associated to any physical system is a complex vector space known as the state space of the system. If the system is isolated, then the system is completely described by its state vector, which is a unit vector in the system's state space. This may seem densely written, but you already know most of it. In particular, we've been working extensively with qubits, and we've seen this postulate in action. For a qubit the state space is, as you know, a two-dimensional complex vector space. The state is a unit vector in that state space (i.e., has length 1 1 1 ). And the condition that the state vector is a unit vector expresses the idea that the probabilities for the outcomes of a computational basis measurement add up to 1 1 1 . It's not just qubits that have a state space. The first postulate of quantum mechanics tells us that every physical system has a state space. An atom has a state space; a human being has a state space; even the universe as a whole has a state space. Admittedly, the first postulate doesn't tell us what the state space is, for any given physical system. That needs to be figured out on a case-by-case basis. Different types of atoms have different state spaces; a human being has a different (and much more complicated) state space; the universe has a more complicated state space still. As an example, suppose you're interested in what happens when you shine light on an atom. To give a quantum mechanical description, you'd need a state space to describe atoms and the electromagnetic field (i.e., light). A priori it's not obvious what that state space should be. How many dimensions should the state space have? How do particular physical configurations of atoms and light correspond to states in that state space? By itself, quantum mechanics doesn't answer these questions. It merely does the subtle-but-important job of instructing you to look for answers to these questions. Fortunately, there is a theory, known as quantum electrodynamics (often shortened to QED), which describes how atoms and light interact. Among other things, QED tells us which states and state spaces to use to give quantum descriptions of atoms and light. In a similar way, suppose we're trying to describe particles like quarks or the Higgs boson. Just as with the atoms-and-light example, quantum mechanics tells us we need figure out the right state spaces and state vectors. But it doesn't tell us what those are. In this case, the additional theory needed is the standard model of particle physics. Like QED, the standard model sits on top of basic quantum mechanics, fleshing it out, telling us things which aren't in the four postulates of quantum mechanics. More generally, quantum mechanics alone isn't a fully specified physical theory. Rather, it's a framework to use to construct physical theories (like QED). It's helpful to think of quantum mechanics as analogous to an operating system for a computer. On its own, the operating system doesn't do all the user needs. Rather, it's a framework that accomplishes important housekeeping functions, creating a file system, a graphical display and interface, and so on. But users need another layer of application software on top of those basic functions. That application layer is analogous to physical theories like QED and the standard model. The application layer runs within the framework provided by the operating system, but isn't itself part of the operating system This analogy was popularized in Scott Aaronson's book “Quantum Computing Since Democritus”; MN believes Scott first heard it in a talk MN gave at the Fields Institute in Toronto in 2001. This wouldn't warrant mention, except when MN uses the analogy, people often respond “oh yes, that's Scott Aaronson's way of thinking about quantum mechanics”. MN doesn't recall where he first heard it, or if the description is original.. In this essay, we won't get deeply into QED or the standard model or the other theories physicists have developed to describe particular physical systems. Rather, our focus is on quantum mechanics itself – the four postulates. For examples, we'll mostly draw on the quantum circuit model, which makes simple and reasonable assumptions about state spaces and state vectors. This model is already an extremely rich arena for studying quantum mechanics. If you wish, you can learn later about QED, the standard model, and so on. Of course, this means there is still much more to learn, beyond the scope of this essay. Each physical system requires learning its own particular set of recipes for state spaces and state vectors. You already know some such recipes: if we say “let's consider a 3-qubit system”, then we know the state space is just the 8-dimensional complex vector space spanned by the computational basis states ∣ 0 ⟩ ∣ 0 ⟩ ∣ 0 ⟩ |0\\rangle|0\\rangle|0\\rangle ∣ 0 ⟩ ∣ 0 ⟩ ∣ 0 ⟩ , ∣ 0 ⟩ ∣ 0 ⟩ ∣ 1 ⟩ , … |0\\rangle|0\\rangle|1\\rangle, \\ldots ∣ 0 ⟩ ∣ 0 ⟩ ∣ 1 ⟩ , … . By contrast, you probably don't know the recipe QED gives telling us how to construct the state space for a couple of hydrogen atoms interacting with the electromagnetic field A little more strictly speaking, QED tells us how charged elementary particles like electrons interact with the electromagnetic field. After QED was invented, atomic physicists and quantum opticians figured out how to use it to describe atoms and electromagnetic fields. And so the recipe you'd use would likely come from atomic physics or quantum optics.. While that recipe is different than in the three-qubit example, it's really much the same kind of thing. You learn the rules of the recipe, and then you can figure out the state spaces and quantum states. Where do the recipes for things like the state space of an atom (or the electromagnetic field) come from? The unglamorous answer is that figuring those things out for the first time was incredibly hard. As in: Nobel prize hard, or even multiple Nobel prize hard. People made lots of guesses, tried lots of different things, trying to figure out states and state spaces (and all the other things), constructed lots of bad theories, and lots of good-but-not-good-enough theories along the way. And, eventually, they came up with some great recipes. Most of the time today we can use one of those recipes off-the-shelf. You go pick up a book about atomic physics, say, and it'll just tell you: use such-and-such a state space, and away you go. But in the background is thousands of instances of trial-and-error by often-frustrated physicists. A point we've glossed over is the use of the term isolated in the first postulate. In particular, the first postulate tells us that every physical system has a state space, but inserts the qualifier isolated when saying which physical systems have a state vector. By isolated we mean a system that's not interacting with any other system. Of course, most physical systems aren't isolated. An atom will interact with its surroundings (for instance, it may be hit by a photon, or perhaps be affected by the charge of a nearby electron). A human being isn't an isolated physical system either – we're constantly being bombarded by light, cosmic rays, and all sorts of other things. In general such non-isolated systems don't have their own quantum state! In fact, we saw an example in the earlier essay. Suppose we use the following quantum circuit: The output is the state: ∣ 00 ⟩ + ∣ 11 ⟩ 2 . \\frac{|00\\rangle+|11\\rangle}{\\sqrt 2}. 2 ​ ∣ 0 0 ⟩ + ∣ 1 1 ⟩ ​ . A good question to ask is: what is the state of the first qubit? But as discussed in the earlier essay, this is an entangled state of the two qubits, and there is no sensible way to assign a quantum state to the first qubit In more advanced treatments you'll learn about an idea called the reduced density matrix, which can be used to describe part of an entangled quantum system. This is a useful mathematical tool derived from the four postulates we describe, but is not an essentially new idea. It's worth being aware of as a calculational convenience to learn about in the future, but discussing it is beyond our immediate scope.. Now, you might be bothered by this. Doesn't the circuit above start with the first qubit “in” the state ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ ? How does that square with the idea that a non-isolated system doesn't have a state? This is a good question, one we'll see how to address later, in the discussion of the fourth postulate. Okay, that's the first postulate covered! Hopefully, you found it pretty easy. Perhaps you're wondering if that's all there is to it? Or perhaps some element of the postulate is bothering you because you feel we haven't unpacked it sufficiently and played it out. It's fine if that's where you're at. Deeply understanding the consequences of the postulates is a lifetime's work. It is not, for instance, obvious from the postulates that phenomena such as quantum teleportation and quantum search are already implicit inside them. You have to work hard to see that! Indeed, if you believe the universe is quantum mechanical – as we do – then everything around you is implicit in the postulates: the colors of the rainbow; the shape of a mountain; the touch of the wind. And so in learning the postulates you are learning the starting rules of an extraordinarily complex game. But unlike an ordinary game, the stakes are not winning or losing; rather, they are about how the universe actually is. This essay is an example of what we call a mnemonic essay. That means it incorporates new interface elements intended to make it almost effortless for you to remember and apply the content of the essay, over the long term. The idea is this: throughout the essay we occasionally pause to ask a few simple questions, testing you on the material just explained. In the days and weeks ahead we'll re-test you in followup review sessions. By carefully expanding the testing schedule, we can ensure you consolidate the answers into your long-term memory, while minimizing the review time required. The review sessions take no more than a few minutes per day, and we'll notify you when you need to review. The benefit is that instead of remembering how quantum mechanics works for a few hours or days, you'll remember for years. It'll become a much more deeply internalized part of your thinking. This long-term internalization is particularly valuable for the core ideas of quantum mechanics, which are at the foundation of many other subjects. Of course, you can just read this as a conventional essay. But we hope you’ll try out the mnemonic medium. To do so please sign up below. This will enable us to track the best review schedule for each question, and to remind you to sign in for occasional short review sessions. And if you’d like to learn more about how the mnemonic medium works, please see A medium which makes memory a choice, How to approach this [earlier] essay?, and How to use (or not use!) the questions. Please sign in so we can save your progress and let you know the best times to review. Thank you! Your progress will be saved as you read. To give you a more concrete flavor of how the mnemonic medium works, let's look at three questions reviewing part of what you've just learned. Please indulge us by answering these questions – it'll take just a few seconds. For each question, think about what you believe the answer to be, click to reveal the actual answer, and then mark whether you remembered or not. If you can recall, that's great. But if not, that's fine, just mentally note the correct answer, and continue. Didn’t remember Remembered The second postulate: unitary dynamics The first postulate of quantum mechanics told us how we describe states. What about how states change, that is, the dynamics of a quantum system? That's where the second postulate comes in. In the earlier essay we saw that quantum gates are described by unitary matrices acting on the state space of a quantum system. The second postulate tells us that something very similar is true for any isolated quantum system: Postulate 2: The evolution of an isolated quantum system is described by a unitary matrix acting on the state space of the system. That is, the state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ of the system at a time t 1 t_1 t 1 ​ is related to the state ∣ ψ ′ ⟩ |\\psi'\\rangle ∣ ψ ′ ⟩ at a later time t 2 t_2 t 2 ​ by a unitary matrix, U U U : ∣ ψ ′ ⟩ = U ∣ ψ ⟩ |\\psi'\\rangle = U |\\psi\\rangle ∣ ψ ′ ⟩ = U ∣ ψ ⟩ . That matrix U U U may depend on the times t 1 t_1 t 1 ​ and t 2 t_2 t 2 ​ , but does not depend on the states ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ and ∣ ψ ′ ⟩ |\\psi'\\rangle ∣ ψ ′ ⟩ . Our quantum gates demonstrate this postulate in action. So, for instance, the Pauli X X X gate, also known as the quantum NOT gate, is an example. Here it is shown in the quantum circuit and matrix representations, as well as the explicit action on states: So too is the Hadamard gate, H H H : And so on, through the controlled-NOT gate, the Toffoli gate, and all the other quantum gates we met earlier. Why is it unitary matrices which appear in the second postulate? If you try writing a few matrices down on paper, you quickly find that most matrices aren't unitary, or even close to it. Why can't we have a general matrix in the second postulate? One partial explanation, discussed in depth in the earlier essay, is that unitary matrices are the only matrices which preserve length. If we want the quantum state to remain normalized, then unitary matrices are the only matrices which do the trick, since any other matrix will result in the norm changing. That normalization is in turn connected to the requirement that the probabilities of measurement outcomes sum to one. In this sense, the postulates of quantum mechanics form a tightly interconnected web, with requirements like unitarity from one postulate reflecting requirements elsewhere, like normalization of the state vector, or probabilities summing to one. How to figure out which unitary transformation is needed to describe any particular physical situation? As you might guess from our discussion of the first postulate, the second postulate is silent on this question. It needs to be figured out case by case. Theories like QED and the standard model supply additional rules specifying the exact (unitary) dynamics of the systems they describe. It's as before: quantum mechanics is a framework, not a complete physical theory in its own right. But being told that the correct way to describe dynamics is using unitary transformations on state space is already an incredibly prescriptive statement. And, as before, the quantum circuit model is a useful source of examples, and working with it is a good way to build intuition. More broadly: although quantum mechanics reached its final form in the 1920s, physicists spent much of the remainder of the twentieth century figuring out what unitary dynamics, state spaces, and quantum states are needed to describe this or that system. You can't just solve this problem once: optical physicists had to do it for light, atomic physicists for atoms, particle physicists have been doing it for the entire pantheon of particles described in the standard model of particle physics. Still, although there's much more to learn about the application of these two postulates, already they give us a remarkably constraining framework for thinking about what the world is and how it can change. Let's pause and run through some more questions: Didn’t remember Remembered If you've read earlier essays in this series, you may recall that we described the mnemonic medium as making it almost effortless to remember the essay contents. We believed this would make it much easier to deeply understand the material. To figure out how well or poorly the medium was working, we interviewed many users. Many of the conversations started out encouragingly: “Yeah, I can remember the content for a long time. And it's weird and fun that it's almost effortless”. So far, so good. But when we dig into how well people understand the material, a pretty common line is: “Before using the mnemonic medium, I didn't think memory was all that important to understanding. I've changed my mind about that: it helps much more than I thought. But it's not a panacea Of course, this isn't a real quote, just a synthesis. No-one actually said “panacea”! either. For instance, I'm not sure I really understand the Hadamard gate. Yeah, I know quite a bit about the gate, and it's remarkable to remember all that. But I'm not sure I could use it in another context. And I'm not sure I could explain it to someone else. It somehow falls short of a full understanding.” They're right that memory is only part of understanding. Understanding also involves many other skills, among them the ability to use what you've learned in new contexts. And so in this essay we're extending the mnemonic medium to include questions where we ask you to apply what you've learned. Here's an example question: Couldn’t answer Answered At first glance that looks much like earlier cards. But in important ways it's different. When you do review sessions in the future you won't see that particular question again. Instead, you'll see variation questions that are similar, but not quite the same. We'll cheat a bit now, and show you two variants that you'll see in future review sessions. We won't ask you to work these out here – in fact, it's probably best if you actively refrain from thinking too hard about these questions! Just take a look, and notice that they're asking you to solve the same kind of very simple problem as in the question above: So this is different to the earlier cards, in that we're not asking you to remember the answer to the question. Rather, we're asking you to work out an answer to the question, which is very different. That is, we're asking you to use the understanding that you've gained. Furthermore, it's not just these three variations: you'll be asked a whole sequence of variations. Our hope and expectation is that this will enable you to gradually internalize not just declarative knowledge, but also procedural knowledge. In this way, you'll learn to reliably transfer your understanding to new contexts. If you're familiar with conventional spaced repetition flashcard software, then you know that this kind of process knowledge isn't something it's designed to support. Such systems are certainly not designed for multiple variations on a question, testing the ability to fluently apply understanding in new contexts. This is a small design change, but we believe it greatly expands the type of learning the mnemonic medium supports, and will result in a qualitative jump in what you get from the medium. Over the longer term, there are umpteen ways the medium could be greatly improved, or radically changed. We think of these experiments as part of developing an answer to the question: what's an ideal, very high-growth personal environment? Of course, we're a long way from having developed such an environment! When we talk with people about the mnemonic medium, they sometimes described it as an educational experiment. This is a limited ambition, as education is commonly conceived. Imagine taking a month of public speaking lessons in school. Now compare that to a situation where you've been asked to present onstage at a major Apple product launch, and will have a team of people from Apple helping you practice for a month. Which do you think would be a higher growth environment? This is a fanciful example, but illustrates the point: there's tremendous scope to develop extraordinarily high-growth environments. This medium is one (tiny) experiment in exploring that scope, and we hope to develop many more such ideas in the future. But for now we'll explore the limits of the current medium. Alright, let's get back to quantum stuff. Here's a couple more examples: Couldn’t answer Answered To reiterate, in future review sessions you'll be asked small variations on the questions above. You can't simply memorize the answers, since you won't be shown the questions again. Rather, you must work them out. You may notice that in the second of the above questions there was an explanation available. You aren't being tested on these explanations, they're just intended to help out if you get stuck. In particular, it doesn't matter if you use the same approach as described in the explanation – anything which gives the correct answer is good. Let's return to the details of the second postulate. You probably noticed that, like the first postulate, it contained a caveat about describing the dynamics of isolated systems using unitary matrices. Of course, in nature most physical systems aren't isolated or even near-isolated. Suppose, for instance, you're trying to build a quantum computer, using atoms to store the states of your qubits. For instance, we could use two different orbital shells for an electron to correspond to the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ states. And suppose you effect a quantum gate by briefly shining laser light on the atom, causing the electron to move between the shells. Such an atom is not isolated: it's interacting strongly with the laser. But it turns out the dynamics of the atom are still closely described by a unitary transformation, even when laser light is being shone on it. This isn't obvious; it requires detailed investigation. But the upshot is that while the second postulate only directly applies to isolated systems, surprisingly often it's possible to describe the dynamics of non-isolated systems by unitary dynamics. Indeed, when we said above that quantum gates are examples of the second postulate, that's not quite right. Usually, quantum gates involve non-isolated systems. But with careful design they implement unitary operations anyway. Didn’t remember Remembered Let's also work through a few questions where you're asked to apply what you've learned. For these questions we'll return to the first postulate, since we didn't have any questions in that style in our earlier discussion, and it's useful being able to apply those ideas. Couldn’t answer Answered We presented the second postulate in the form most often used in quantum circuit contexts, with discrete gates being applied to cause discrete changes in the quantum state. We'll now briefly show you an alternate form, in which the evolution of the quantum state varies smoothly in continuous time, with the change described by a differential equation. We're not going to use this much, and so you don't need to understand all the details in depth – this is just so you get the gist. Here's the equation controlling the time rate of change of the quantum state: i d ∣ ψ ⟩ d t = H ∣ ψ ⟩ . i \\frac{d|\\psi\\rangle}{dt} = H |\\psi\\rangle. i d t d ∣ ψ ⟩ ​ = H ∣ ψ ⟩ . This equation is known as the Schroedinger equation. The matrix H H H is a (fixed) hermitian matrix known as the Hamiltonian of the system. (This is not the same as the Hadamard gate, it's merely an unfortunate notational coincidence). You can think of the Hamiltonian H H H as telling the quantum state how to change. The Schroedinger equation thus provides a continuous-time description of the dynamics of the quantum state, whereas the second postulate was focused on a discrete time description. By solving the Schroedinger equation we can relate the state of the system at time t 2 t_2 t 2 ​ to the state at time t 1 t_1 t 1 ​ . The solution is: ∣ ψ t 2 ⟩ = e − i H ( t 2 − t 1 ) ∣ ψ t 1 ⟩ . |\\psi_{t_2}\\rangle = e^{-iH(t_2-t_1)} |\\psi_{t_1}\\rangle. ∣ ψ t 2 ​ ​ ⟩ = e − i H ( t 2 ​ − t 1 ​ ) ∣ ψ t 1 ​ ​ ⟩ . Of course, while it's easy enough to write down the matrix exponential, it is often quite a bit of work to compute in practice! Traditional treatments of quantum mechanics spend a lot of time discussing Hamiltonians used to describe different physical systems, and figuring out (often rather tediously) the value of the matrix exponential. These are important and worthy technical problems, but not necessary for understanding the fundamentals of quantum mechanics. Still, it's worth noting that the above solution is consistent with the second postulate: in particular, it can be shown that e − i H ( t 2 − t 1 ) e^{-iH(t_2-t_1)} e − i H ( t 2 ​ − t 1 ​ ) is a unitary matrix. Indeed, if you're wondering about questions like “where does that factor i i i come from in the Schroedinger equation” or “why is the Hamiltonian H H H required to be Hermitian”, they turn out to be required to get unitary evolution. So it all hangs together nicely. In fact, there's also an informal argument which lets you turn the second postulate around and (with a few extra assumptions) derive the Schroedinger equation. We won't get into details here, but it seems worth being aware this is possible. What is worth taking away from this discussion is a few basics: the term Hamiltonian, what the Schroedinger equation does, qualitatively, and a broad feel for how the Schroedinger equation relates to the second postulate. We won't ask you to remember the details of the Schroedinger equation – those really need to be unpacked in more depth, and with some detailed examples. On the other hand, it is valuable to know what the Schroedinger equation does: it describes the time rate of change of the quantum state of an isolated physical system. The questions below are somewhat outside the main scope of this essay. But they're included here so that if in later reading you come across terms like “Hamiltonian”, you'll have a basic understanding to build upon. Didn’t remember Remembered The third postulate: measurement The third postulate is the strangest of the postulates of quantum mechanics. To explain its content, it helps to recap from Quantum Computing for the Very Curious : Suppose a (hypothetical!) quantum physicist named Alice prepares a qubit in her laboratory, in a quantum state α ∣ 0 ⟩ + β ∣ 1 ⟩ \\alpha|0\\rangle+\\beta|1\\rangle α ∣ 0 ⟩ + β ∣ 1 ⟩ . Then she gives her qubit to another quantum physicist, Bob, but doesn’t tell him the values of α \\alpha α and β \\beta β . Is there some way Bob can figure out α \\alpha α and β \\beta β ? That is, is there some experiment Bob can do to figure out the identity of the quantum state? The surprising answer to this question turns out to be NO! There is, in fact, no way to figure out α \\alpha α and β \\beta β if they start out unknown. To put it a slightly different way, the quantum state of any system – whether it be a qubit or a some other system – is not directly observable. [We] say this is surprising, because it's very different from our usual everyday way of thinking about how the world works. If there’s something wrong with your car, a mechanic can use diagnostic tools to learn about the internal state of the engine. The better the diagnostic tools, the more they can learn. Of course, there may be parts of the engine that would be impractical to access – maybe they’d have to break a part, or use a microscope, for instance. But you’d probably be rather suspicious if the mechanic told you the laws of physics prohibited them from figuring out the internal state of the engine. In the earlier essay we described measurement in the computational basis. That process governs how you get information out of a quantum computer. Recall the simplest version, measuring just a single qubit in the computational basis. In that process, if you measure a qubit in the state α ∣ 0 ⟩ + β ∣ 1 ⟩ \\alpha|0\\rangle+\\beta|1\\rangle α ∣ 0 ⟩ + β ∣ 1 ⟩ then you get the outcome 0 0 0 with probability ∣ α ∣ 2 |\\alpha|^2 ∣ α ∣ 2 and the outcome 1 1 1 with probability ∣ β ∣ 2 |\\beta|^2 ∣ β ∣ 2 . The posterior state in the two cases is ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ , respectively. The third postulate generalizes this idea of measurement: Postulate 3: Quantum measurements are described by a collection { M m } \\{ M_m \\} { M m ​ } of measurement operators The term “operator” is often used essentially interchangeably with “matrix” in elementary linear algebra. The two terms mean somewhat different things in infinite-dimensional spaces, but in our finite-dimensional context can be interchanged. The term measurement operator seems more common than measurement matrix, and so we go with that.. Each M m M_m M m ​ is a matrix acting on the state space of the system being measured. The index m m m takes values corresponding to the measurement outcomes that may occur in the experiment. If the state of the quantum system is ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ immediately before the measurement then the probability that result m m m occurs is given by p ( m ) = ⟨ ψ ∣ M m † M m ∣ ψ ⟩ , p(m) = \\langle \\psi| M_m^\\dagger M_m |\\psi\\rangle, p ( m ) = ⟨ ψ ∣ M m † ​ M m ​ ∣ ψ ⟩ , and the state of the system after the measurement, often called the posterior state, is M m ∣ ψ ⟩ ⟨ ψ ∣ M m † M m ∣ ψ ⟩ . \\frac{M_m|\\psi\\rangle}{\\sqrt{\\langle \\psi|M_m^\\dagger M_m |\\psi\\rangle}}. ⟨ ψ ∣ M m † ​ M m ​ ∣ ψ ⟩ ​ M m ​ ∣ ψ ⟩ ​ . (It's worth noting that: (a) the denominator is just the square root of the probability p ( m ) p(m) p ( m ) ; and (b) this is a properly normalized quantum state.) The measurement operators satisfy the completeness relation The postulate we've stated is a generalization of the statement given in some quantum mechanics textbooks, based on what are often called projective measurements. The two statements are, however, equivalent, when combined with the other postulates. We've chosen the present formulation as it's mathematically simpler and more powerful than the presentation in terms of projective measurements. We believe that presentation should be deprecated from quantum mechanics texts., ∑ m M m † M m = I . \\sum_m M_m^\\dagger M_m = I. m ∑ ​ M m † ​ M m ​ = I . There's a lot going on in this postulate. But again it mostly expresses ideas we've already seen, albeit disguised. We'll give a concrete example in a moment, but first want to address the meaning of the completeness relation. It simply expresses the idea that probabilities sum to 1 1 1 . We can see this by pre-multiplying the completeness relation by ⟨ ψ ∣ \\langle \\psi| ⟨ ψ ∣ and post-multiplying by ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ . Comparing with the equation for the probability p ( m ) p(m) p ( m ) we see this gives ∑ m p ( m ) = ∑ m ⟨ ψ ∣ M m † M m ∣ ψ ⟩ = ⟨ ψ ∣ I ∣ ψ ⟩ = ⟨ ψ ∣ ψ ⟩ = 1 , \\sum_m p(m) = \\sum_m \\langle \\psi|M_m^\\dagger M_m|\\psi\\rangle = \\langle \\psi| I |\\psi\\rangle = \\langle \\psi|\\psi\\rangle = 1, m ∑ ​ p ( m ) = m ∑ ​ ⟨ ψ ∣ M m † ​ M m ​ ∣ ψ ⟩ = ⟨ ψ ∣ I ∣ ψ ⟩ = ⟨ ψ ∣ ψ ⟩ = 1 , where we assumed ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ is a quantum state, and thus normalized to have length 1 1 1 . To understand the meaning of the third postulate, it helps to work through the particular case of measuring a single qubit in the computational basis. In that case, the measurement operators are M 0 = ∣ 0 ⟩ ⟨ 0 ∣ M_0 = |0\\rangle \\langle 0| M 0 ​ = ∣ 0 ⟩ ⟨ 0 ∣ and M 1 = ∣ 1 ⟩ ⟨ 1 ∣ M_1 = |1\\rangle \\langle 1| M 1 ​ = ∣ 1 ⟩ ⟨ 1 ∣ . It's easiest to apply the third postulate if we write out explicit amplitudes for the quantum state, ∣ ψ ⟩ = α ∣ 0 ⟩ + β ∣ 1 ⟩ |\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle ∣ ψ ⟩ = α ∣ 0 ⟩ + β ∣ 1 ⟩ . According to the third postulate, the probability for result 0 0 0 is: p ( 0 ) = ⟨ ψ ∣ M 0 † M 0 ∣ ψ ⟩ = ⟨ ψ ∣ 0 ⟩ ⟨ 0 ∣ 0 ⟩ ⟨ 0 ∣ ψ ⟩ . \\begin{aligned} p(0) & = \\langle \\psi| M_0^\\dagger M_0 |\\psi\\rangle \\\\ & = \\langle \\psi|0\\rangle \\langle 0| 0\\rangle \\langle 0| \\psi\\rangle. \\end{aligned} p ( 0 ) ​ = ⟨ ψ ∣ M 0 † ​ M 0 ​ ∣ ψ ⟩ = ⟨ ψ ∣ 0 ⟩ ⟨ 0 ∣ 0 ⟩ ⟨ 0 ∣ ψ ⟩ . ​ Of course, ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ is normalized and so ⟨ 0 ∣ 0 ⟩ = 1 \\langle 0|0\\rangle = 1 ⟨ 0 ∣ 0 ⟩ = 1 , and this becomes: p ( 0 ) = ⟨ ψ ∣ 0 ⟩ ⟨ 0 ∣ ψ ⟩ = ∣ ⟨ 0 ∣ ψ ⟩ ∣ 2 . \\begin{aligned} p(0) & = \\langle \\psi|0\\rangle \\langle 0| \\psi\\rangle = |\\langle 0|\\psi\\rangle|^2. \\end{aligned} p ( 0 ) ​ = ⟨ ψ ∣ 0 ⟩ ⟨ 0 ∣ ψ ⟩ = ∣ ⟨ 0 ∣ ψ ⟩ ∣ 2 . ​ But ⟨ 0 ∣ ψ ⟩ = α \\langle 0|\\psi\\rangle = \\alpha ⟨ 0 ∣ ψ ⟩ = α , and so this tells us the probability of measurement outcome 0 0 0 is p ( 0 ) = ∣ α ∣ 2 p(0) = |\\alpha|^2 p ( 0 ) = ∣ α ∣ 2 , just as we expected. The exact same calculation, but with 0 0 0 s replaced by 1 1 1 s, shows us that p ( 1 ) = ∣ β ∣ 2 p(1) = |\\beta|^2 p ( 1 ) = ∣ β ∣ 2 , also as expected. What about the posterior states? Let's consider the case of measurement result m = 0 m = 0 m = 0 . In that case the third postulate tells us the posterior state is ∣ 0 ⟩ ⟨ 0 ∣ ψ ⟩ p ( 0 ) . \\frac{|0\\rangle \\langle 0|\\psi\\rangle}{\\sqrt{p(0)}}. p ( 0 ) ​ ∣ 0 ⟩ ⟨ 0 ∣ ψ ⟩ ​ . We've already seen p ( 0 ) = ∣ α ∣ 2 p(0) = |\\alpha|^2 p ( 0 ) = ∣ α ∣ 2 . And, of course, ⟨ 0 ∣ ψ ⟩ = α \\langle 0|\\psi\\rangle = \\alpha ⟨ 0 ∣ ψ ⟩ = α . And so the posterior state is: α ∣ α ∣ ∣ 0 ⟩ . \\frac{\\alpha}{|\\alpha|} |0\\rangle. ∣ α ∣ α ​ ∣ 0 ⟩ . This is a peculiar looking state! The pre-factor α / ∣ α ∣ \\alpha/|\\alpha| α / ∣ α ∣ appears complicated, but it's just a global phase factor. Back in Quantum Computing for the Very Curious we observed that such global phase factors never affect measurement probabilities, and as a result it's conventional to regard quantum states which differ only by a global phase factor as identical. And so the state above can be regarded as equivalent to the quantum state ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ , just as we expected for a measurement in the computational basis. In a similar fashion, we can show that the posterior state if the measurement result was 1 1 1 is ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ (again, up to a global phase factor, which we ignore). The argument for ignoring global phase factors in Quantum Computing for the Very Curious was hand wavy. In fact, the postulates can be used to make the argument watertight. Suppose we have two quantum states related by a global phase factor, ∣ ψ ′ ⟩ = e i θ ∣ ψ ⟩ |\\psi'\\rangle = e^{i\\theta}|\\psi\\rangle ∣ ψ ′ ⟩ = e i θ ∣ ψ ⟩ . Then the measurement probabilities for the two states are always identical: p ′ ( m ) = ⟨ ψ ′ ∣ M m † M m ∣ ψ ′ ⟩ = ⟨ ψ ∣ M m † M m ∣ ψ ⟩ = p ( m ) . p'(m) = \\langle \\psi'| M_m^{\\dagger} M_m |\\psi'\\rangle = \\langle \\psi|M_m^\\dagger M_m |\\psi\\rangle = p(m). p ′ ( m ) = ⟨ ψ ′ ∣ M m † ​ M m ​ ∣ ψ ′ ⟩ = ⟨ ψ ∣ M m † ​ M m ​ ∣ ψ ⟩ = p ( m ) . And, although we won't show it explicitly, the global phase factor also propagates unchanged through both unitary dynamics and posterior state calculations. And so such a phase factor makes no difference at all to anything observable. In fact, people sometimes re-state the postulates using a different formulation which entirely eliminates such global phase factors. The reason we haven't done this is that it significantly complicates the presentation. It's easier and also more conventional to just use the informal argument we've made. Incidentally, you may wonder what happens in the third postulate when the denominator ⟨ ψ ∣ M m † M m ∣ ψ ⟩ \\sqrt{\\langle \\psi|M_m^\\dagger M_m|\\psi\\rangle} ⟨ ψ ∣ M m † ​ M m ​ ∣ ψ ⟩ ​ in the posterior state vanishes. Wouldn't that make the posterior state undefined? Fortunately, that can only happen if the probability of the measurement outcome is zero. That also means the measurement outcome will never occur, so we don't need to worry about it. We've gone through a lot of work just to analyze a single quantum measurement! The good news is that we rarely need to go through such work. Instead, you build up a library of common patterns for measurement operators. Indeed, in quantum computing you'll mainly be interested in measurements in the computational basis. And there are a few other common measurements as well, some of which we'll talk about later in the essay. In practice, you only rarely need to do calculations like those above. Let's run through a few questions testing your memory of the third postulate. These may be more challenging than earlier questions, and you may find it helpful to first return to the statement of the third postulate, and check that you understand it. This is the most detailed part of the essay – if you can get past it, you're over the main hump. Didn’t remember Remembered Let's also work through a couple of cards giving you practice computing the outcomes of measurements in the computational basis: Couldn’t answer Answered Let's introduce a new type of measurement, one we haven't seen in any earlier essays. It's a single-qubit measurement, but not a measurement in the computational basis. Recall the equal superposition states: ∣ + ⟩ = ∣ 0 ⟩ + ∣ 1 ⟩ 2 ; ∣ − ⟩ = ∣ 0 ⟩ − ∣ 1 ⟩ 2 . |+\\rangle = \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2}; \\,\\,\\,\\,\\,\\, |-\\rangle = \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2}. ∣ + ⟩ = 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ ; ∣ − ⟩ = 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ . The measurement we'll now define is what people call “measuring in the ∣ + ⟩ , ∣ − ⟩ |+\\rangle, |-\\rangle ∣ + ⟩ , ∣ − ⟩ basis”. In particular, we define measurement operators M + = ∣ + ⟩ ⟨ + ∣ M_+ = |+\\rangle\\langle +| M + ​ = ∣ + ⟩ ⟨ + ∣ and M − = ∣ − ⟩ ⟨ − ∣ M_- = |-\\rangle \\langle -| M − ​ = ∣ − ⟩ ⟨ − ∣ . We won't work through the details of the calculation, but if you wish you can check the completeness relation M + † M + + M − † M − = I M_+^\\dagger M_+ + M_-^\\dagger M_- = I M + † ​ M + ​ + M − † ​ M − ​ = I . Making this check is just a little matrix calculation. What we're going to focus on is understanding the measurement probabilities and posterior states. It's tempting – in fact, it's a good idea! – to begin by expanding the state being measured as ∣ ψ ⟩ = α ∣ 0 ⟩ + β ∣ 1 ⟩ |\\psi\\rangle = \\alpha |0\\rangle+\\beta |1\\rangle ∣ ψ ⟩ = α ∣ 0 ⟩ + β ∣ 1 ⟩ . If you do this, you can push through and figure out the measurement probabilities and posterior states. But there's a trick you can use to simplify matters. It's worth pausing a moment, and seeing if you can think of what that trick might be. Any ideas? The trick is this: it's to observe that we could equally well have expanded ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ in terms of the ∣ + ⟩ |+\\rangle ∣ + ⟩ and ∣ − ⟩ |-\\rangle ∣ − ⟩ states, as ∣ ψ ⟩ = γ ∣ + ⟩ + δ ∣ − ⟩ |\\psi\\rangle = \\gamma|+\\rangle + \\delta|-\\rangle ∣ ψ ⟩ = γ ∣ + ⟩ + δ ∣ − ⟩ . We can certainly do this, since ∣ + ⟩ |+\\rangle ∣ + ⟩ and ∣ − ⟩ |-\\rangle ∣ − ⟩ are linearly independent (in fact, orthonormal) states in a two-dimensional vector space. Can you now guess what the measurement probabilities and posterior states are, by analogy with measurement in the computational basis? We'll work through the explicit calculation in a moment, but it's worth stating the result up front: the probability of the + + + outcome is ∣ γ ∣ 2 |\\gamma|^2 ∣ γ ∣ 2 , with posterior state ∣ + ⟩ |+\\rangle ∣ + ⟩ , and the probability of the − - − outcome is ∣ δ ∣ 2 |\\delta|^2 ∣ δ ∣ 2 , with posterior state ∣ − ⟩ |-\\rangle ∣ − ⟩ . This is much like the computational basis measurement, and the calculation showing it is much like the calculation earlier for the computational basis measurement. In particular, for the + + + measurement we have: p ( + ) = ⟨ ψ ∣ M + † M + ∣ ψ ⟩ = ( γ ∗ ⟨ + ∣ + δ ∗ ⟨ − ∣ ) ∣ + ⟩ ⟨ + ∣ + ⟩ ⟨ + ∣ ( γ ∣ + ⟩ + δ ∣ − ⟩ ) . p(+) = \\langle \\psi| M_+^\\dagger M_+ |\\psi\\rangle = (\\gamma^* \\langle +|+\\delta^*\\langle -|) |+\\rangle \\langle +|+\\rangle \\langle +|(\\gamma |+\\rangle+\\delta|-\\rangle). p ( + ) = ⟨ ψ ∣ M + † ​ M + ​ ∣ ψ ⟩ = ( γ ∗ ⟨ + ∣ + δ ∗ ⟨ − ∣ ) ∣ + ⟩ ⟨ + ∣ + ⟩ ⟨ + ∣ ( γ ∣ + ⟩ + δ ∣ − ⟩ ) . Of course, ⟨ + ∣ + ⟩ = 1 \\langle +|+\\rangle = 1 ⟨ + ∣ + ⟩ = 1 . And, in case you've forgotten, a quick calculation shows ⟨ + ∣ − ⟩ = 0 \\langle +|-\\rangle = 0 ⟨ + ∣ − ⟩ = 0 , i.e., the ∣ + ⟩ |+\\rangle ∣ + ⟩ and ∣ − ⟩ |-\\rangle ∣ − ⟩ states are orthogonal. So the above becomes: p ( + ) = ( γ ∗ ⟨ + ∣ + δ ∗ ⟨ − ∣ ) ∣ + ⟩ ⟨ + ∣ ( γ ∣ + ⟩ + δ ∣ − ⟩ ) = ∣ γ ∣ 2 . \\begin{aligned} p(+) & = (\\gamma^* \\langle +|+\\delta^*\\langle -|) |+\\rangle\\langle +| (\\gamma |+\\rangle+\\delta |-\\rangle) \\\\ & = |\\gamma|^2. \\end{aligned} p ( + ) ​ = ( γ ∗ ⟨ + ∣ + δ ∗ ⟨ − ∣ ) ∣ + ⟩ ⟨ + ∣ ( γ ∣ + ⟩ + δ ∣ − ⟩ ) = ∣ γ ∣ 2 . ​ A similar but simpler calculation shows M + ∣ ψ ⟩ = γ ∣ + ⟩ M_+ |\\psi\\rangle = \\gamma|+\\rangle M + ​ ∣ ψ ⟩ = γ ∣ + ⟩ , and so the corresponding posterior state is γ / ∣ γ ∣ ∣ + ⟩ \\gamma/|\\gamma| |+\\rangle γ / ∣ γ ∣ ∣ + ⟩ , which up to a global phase factor is the ∣ + ⟩ |+\\rangle ∣ + ⟩ state. Similar calculations show that the probability of a − - − outcome is ∣ δ ∣ 2 |\\delta|^2 ∣ δ ∣ 2 , with posterior state ∣ − ⟩ |-\\rangle ∣ − ⟩ (up to a global phase). In practice we just say the posterior states are ∣ + ⟩ |+\\rangle ∣ + ⟩ and ∣ − ⟩ |-\\rangle ∣ − ⟩ , respectively. What would have happened if instead we'd stuck with ∣ ψ ⟩ = α ∣ 0 ⟩ + β ∣ 1 ⟩ |\\psi\\rangle = \\alpha|0\\rangle+\\beta|1\\rangle ∣ ψ ⟩ = α ∣ 0 ⟩ + β ∣ 1 ⟩ ? The calculation would have been a little more complicated, but of course we would have gotten the same result in the end, albeit expressed in terms of α \\alpha α and β \\beta β instead of γ \\gamma γ and δ \\delta δ . We've been asking you to guess or intuit many results. If you successfully guessed the answers that may have been fun, but if not it may have been frustrating. If you'll allow us to generalize, people who've been raised in a culture that values “quick” and “smart” sometimes feel bad if they can't solve this kind of problem. This is unfortunate. Much of getting good at mathematics or physics is how you respond to this kind of stuckness. Superb mathematicians get stuck in similar ways, sometimes on things which later may seem headslappingly obvious. But they develop lots of heuristics for trying new things, even when stuck; and for keeping morale up. Developing that ability to continue when stuck is a crucial part of learning to do mathematics or physics. Couldn’t answer Answered There's an alternate way of looking at the third postulate where it appears almost to be a generalization of the second postulate. Suppose we were to do a measurement with just a single measurement outcome. That sounds peculiar – if a measuring device always showed the same answer you'd think it was broken – but it at least makes logical sense. According to the third postulate such a measurement would be described by a single measurement operator, which we can just call M M M (no label needed, since the outcome is always the same). In this situation the completeness relation becomes M † M = I M^\\dagger M = I M † M = I . That's just the condition that M M M be unitary. And, of course, the probability of the outcome would be ⟨ ψ ∣ M † M ∣ ψ ⟩ = ⟨ ψ ∣ I ∣ ψ ⟩ = 1 \\langle \\psi|M^\\dagger M |\\psi\\rangle = \\langle \\psi|I|\\psi\\rangle = 1 ⟨ ψ ∣ M † M ∣ ψ ⟩ = ⟨ ψ ∣ I ∣ ψ ⟩ = 1 , so that outcome would occur with probability 1 1 1 , i.e., all the time. This is as we'd expect, given that there is only one possible outcome! So the second postulate appears to be a special case of the third postulate where there is just a single outcome. We say “appears”. It's not quite as simple as that. After all, the third postulate is talking about a system which is being measured, presumably by interacting with some measuring device, while the second postulate is talking about the dynamics of isolated physical systems. So the two postulates are talking about quite distinct physical situations. To say the second postulate is a special case of the third it's not enough for the mathematics to check out: you need a unified and conceptually sensible physical picture too. Still, you can imagine trying to reformulate the postulates so that the second and third postulates are unified aspects of a single postulate. It's fun to try to find conceptual unifications making that work. We don't do it here, but you may enjoy pondering how to do it. Didn’t remember Remembered The fourth postulate: composite systems We're almost done! The fourth postulate is the final postulate of quantum mechanics. It's also the easiest to understand. Remember that the first postulate told us that quantum systems have state spaces. Suppose we take two quantum systems, A A A and B B B . What's the state space of the combined system containing both A A A and B B B ? The fourth postulate tells us the answer to this question. It involves the use of an idea from linear algebra known as the tensor product, which we haven't explicitly met before. We'll discuss the tensor product more below; for now, let's see the postulate. Postulate 4: The state space of a composite physical system is the tensor product of the state spaces of the component physical systems. Moreover, if we have systems numbered 1 1 1 through n n n , and system number j j j is prepared in the state ∣ ψ j ⟩ |\\psi_j\\rangle ∣ ψ j ​ ⟩ , then the joint state of the total system is just the tensor product of the individual states, ∣ ψ 1 ⟩ ⊗ ∣ ψ 2 ⟩ ⊗ … ⊗ ∣ ψ n ⟩ |\\psi_1\\rangle \\otimes |\\psi_2\\rangle \\otimes \\ldots \\otimes |\\psi_n\\rangle ∣ ψ 1 ​ ⟩ ⊗ ∣ ψ 2 ​ ⟩ ⊗ … ⊗ ∣ ψ n ​ ⟩ . We've already met many examples of this postulate, when we worked with many-qubit systems in the earlier essays. For instance, we dealt with two-qubit states like: ∣ 00 ⟩ + ∣ 11 ⟩ 2 . \\frac{|00\\rangle+|11\\rangle}{\\sqrt 2}. 2 ​ ∣ 0 0 ⟩ + ∣ 1 1 ⟩ ​ . If you read the quantum search essay, then in that essay we dealt with n n n - qubit systems. So, implicitly we've already worked quite a bit with the tensor product. Sometimes people will write states like ∣ 00 ⟩ |00\\rangle ∣ 0 0 ⟩ as ∣ 0 ⟩ ⊗ ∣ 0 ⟩ |0\\rangle \\otimes |0\\rangle ∣ 0 ⟩ ⊗ ∣ 0 ⟩ , and speak of “the tensor product of ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ ”. (The symbol ⊗ \\otimes ⊗ is known as the tensor product symbol.) In fact, ∣ 00 ⟩ , ∣ 0 ⟩ ∣ 0 ⟩ |00\\rangle, |0\\rangle|0\\rangle ∣ 0 0 ⟩ , ∣ 0 ⟩ ∣ 0 ⟩ and ∣ 0 ⟩ ⊗ ∣ 0 ⟩ |0\\rangle \\otimes |0\\rangle ∣ 0 ⟩ ⊗ ∣ 0 ⟩ are all just different notations for exactly the same thing. This proliferation of notations can seem a little confusing at first, but it's worth remembering that ∣ 00 ⟩ = ∣ 0 ⟩ ∣ 0 ⟩ = ∣ 0 ⟩ ⊗ ∣ 0 ⟩ |00\\rangle = |0\\rangle|0\\rangle = |0\\rangle \\otimes |0\\rangle ∣ 0 0 ⟩ = ∣ 0 ⟩ ∣ 0 ⟩ = ∣ 0 ⟩ ⊗ ∣ 0 ⟩ . There is, of course, a formal definition for the tensor product of two vector spaces. We thought about including that formal definition here, but decided against it. If you're bothered, you might pause for a moment to consider how comfortable you feel with the natural numbers 1 , 2 , 3 , … 1, 2, 3, \\ldots 1 , 2 , 3 , … . Chances are you didn't begin your acquaintance with the natural numbers with a rigorous definition. Rather, you probably used the numbers for years before ever seeing a definition (if you ever saw such a definition). In the same way: we believe you learn more about the tensor product by working through things like quantum circuits, the quantum search algorithm, and quantum teleportation than by working through a formal definition. There are a couple of additional elements left implicit in the fourth postulate – physicists sometimes regard these elements as so obvious as to not merit mentioning. We'd like to mention them here. They're best illustrated concretely. Suppose we apply a quantum gate like the Hadamard gate H H H to the first qubit in a quantum circuit: Usually, we think of the Hadamard gate as a 2 × 2 2 \\times 2 2 × 2 unitary matrix, H = 1 2 [ 1 1 1 − 1 ] . H = \\frac{1}{\\sqrt 2} \\left[ \\begin{array}{cc} 1 & 1 \\\\ 1 & -1 \\end{array} \\right]. H = 2 ​ 1 ​ [ 1 1 ​ 1 − 1 ​ ] . But in the context of a two-qubit circuit like that shown above, the gate must somehow be acting on the entire four-dimensional state space. Mathematically, we define the tensor product operation H ⊗ I H \\otimes I H ⊗ I . This does what you'd guess: it acts on tensor product states ∣ ψ ⟩ ⊗ ∣ ϕ ⟩ |\\psi\\rangle \\otimes |\\phi\\rangle ∣ ψ ⟩ ⊗ ∣ ϕ ⟩ by letting H H H act on the ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ piece, and the 2 × 2 2 \\times 2 2 × 2 identity I I I act on ∣ ϕ ⟩ |\\phi\\rangle ∣ ϕ ⟩ . Writing this all out very explicitly, it just means that the Hadamard gate acts on the two-qubit system exactly as we expect: ( H ⊗ I ) ( ∣ ψ ⟩ ⊗ ∣ ϕ ⟩ ) = ( H ∣ ψ ⟩ ⊗ ∣ ϕ ⟩ ) . (H \\otimes I)(|\\psi\\rangle \\otimes |\\phi\\rangle) = (H|\\psi\\rangle \\otimes |\\phi\\rangle). ( H ⊗ I ) ( ∣ ψ ⟩ ⊗ ∣ ϕ ⟩ ) = ( H ∣ ψ ⟩ ⊗ ∣ ϕ ⟩ ) . Similar ideas apply more generally. For instance, if the Hadamard gate had acted on the second qubit instead, the operation on the entire state space would have been I ⊗ H I \\otimes H I ⊗ H , and we're sure you can figure out how that would be defined! Much more generally still, when we apply unitary matrices or measurement operators to a system which is part of a larger quantum system, we extend the relevant unitary matrices and measurement operators by tensoring with the identity matrix acting on the state space for the rest of the system. It's worth knowing that this is implicitly (and sometimes explicitly) what's going on. Didn’t remember Remembered Quantum mechanics in a nutshell You've now seen quantum mechanics in its entirety! Let's briefly sum up what it does and does not do. It does a few things: it tells us what a quantum state is, it tells us where quantum states live (state space), it tells us how they change, both when the system is isolated (unitary dynamics) and when it's measured (measurement operators). And it tells us how the state space of a composite physical system relates to the component systems. The exact details – what state, what state space, what unitary dynamics or measurement operators – need to be figured out case by case. That's additional to the core content of quantum mechanics. Here's that core content in a single diagram: All the mathematical ideas are simple. But they're not a priori obvious. Still, once the framework has been discovered, it's easy to use. If you're confident you understand all the elements of that diagram, then you're confident you understand quantum mechanics. In a similar vein, it's possible to summarize in just a few lines the quantum computing model, the quantum search algorithm, and quantum teleportation. If you understand all those summaries, then you've got a good mastery of the core of quantum computing, quantum mechanics, and two deep applications of those subjects. It's worth examining the summaries, and asking yourself where you're comfortable – pausing to appreciate your achievement – and also to identify spots where you're uncomfortable, and to think about how you might improve your understanding, perhaps through re-reading. We'll conclude Part I with a few questions that integrate what we've covered. In Part II we finish off Quantum Country by discussing what makes quantum mechanics such a strange theory, and some major open scientific challenges it reveals. Didn’t remember Remembered Part II: Why are so many physicists so upset about quantum mechanics? If quantum mechanics is just four easily-understood postulates, then why did Richard Feynman say “nobody understands quantum mechanics”? What was Einstein referring to when he said he'd thought “a hundred times” as much about the quantum problems as about general relativity? What had them so upset? On the one hand, no matter what Feynman says, tens of thousands of people genuinely understand quantum mechanics. They understand it well enough to use it to design and build new devices – for instance, quantum mechanics played a crucial role in developing the first transistor, and nowadays is a working tool for anyone thinking seriously about semiconductors. Or consider the tens of thousands of people who understand quantum search and quantum teleportation. All these people certainly understand quantum mechanics. On the other hand, Feynman wasn't a yo-yo. When people say no-one understands quantum mechanics, they're using “understand” in an unconventional but meaningful sense. In particular, there are a few fundamental puzzles about the meaning of the theory that have never been entirely resolved, despite decades of thought. In Part II of this essay, we explain one of these puzzles in detail, the Bell inequality. And we'll briefly survey some of the other puzzles about quantum mechanics, and consider what would be entailed by a deeper understanding of the theory. The Bell inequality Bell's theorem is the most profound discovery of science. — Henry Stapp Do you really believe that the moon isn’t there when nobody looks? — Albert Einstein Suppose someone flips a coin, letting it land on their arm, but covers it up before you have a chance to see it. You will, naturally, model the state of the coin as being either heads or tails. And while that state is not immediately accessible to you (at least, not until they move their hand), you certainly assume that the coin has an objective state which could in principle be measured. This all sounds rather trite. But it's actually not obvious that the same thing is true in quantum mechanics. Suppose you have a qubit in some state, and perform a measurement in the computational basis, with outcome 0 0 0 or 1 1 1 . What is being revealed by such a measurement? Is it revealing some objective, independent property of the qubit, a property that existed inside the qubit prior to the measurement? Or is it doing something else? In fact, does the qubit have any objective, independent properties at all? These may seem like amusing philosophical questions, good for late-night conversation, but not of much importance. But in a remarkable 1964 paper, the physicist John Bell sharpened the questions up a very great deal. Indeed, it's not going too far to say Bell's results show a need to completely revise our thinking about what reality is. In this section, we explain Bell's result. In fact, we'll explain a slightly stronger result, usually attributed to a 1969 paper of John Clauser, Michael Horne, Abner Shimony, and Richard Holt (CHSH) John Bell, On the Einstein Podolsky Rosen Paradox (1964), and John Clauser, Michael Horne, Abner Shimony, and Richard Holt, Proposed Experiment to Test Local Hidden-Variable Theories (1969).. This result is sometimes called the CHSH inequality, but we'll call it the Bell inequality (even though it's not quite the same as Bell's), since Bell had the core underlying insight. To explain the Bell inequality, we first have to get ourselves out of the headspace of quantum mechanics. Nothing we say through the remainder of this section will have anything directly to do with quantum mechanics. There will be no qubits, no state vectors, no unitary dynamics, and certainly no measurement operators. Instead, we're going to consider a real-world experimental setup, and we'll think about what happens in that setup from (more or less) first principles The material that follows in this section is adapted from MN's essay Why the world needs quantum mechanics (2008).. In particular, we're going to think about light, and about photons, the elementary particles making up light. Photons have many physical properties – things like position, color, and so on. One property that may be a little less familiar to you is something called polarization. We say “less familiar”, but chances are you've at least met polarization, though you may not have realized it. If you take a pair of sunglasses, and hold them up toward the surface of the ocean or a pool on a sunny day, you'll notice that different amounts of light come through, depending on which angle you hold the sunglasses at. What this means is that depending on the angle, different numbers of photons are coming through Not all sunglasses are polarizing in this way. But many are. You can check whether your sunglasses are polarizing by holding them up toward pretty much any surface that reflects glare. The ocean or a pool on a sunny day work well. If the surface appearance changes as you rotate the sunglasses, they're polarizing; if not, they're not.. Imagine, for example, that you hold the sunglasses horizontally: The photons that make it through the sunglasses have what is called horizontal polarization. Not all photons coming toward the sunglasses have this polarization, which is why not all the photons make it through. There are other physical properties that can be measured in a similar way. For example, imagine holding the sunglasses at 45 degrees to horizontal: The photons that make it through the sunglasses have a polarization at 45 degrees to horizontal. You may wonder: is there any relationship between a photon having a horizontal polarization and having polarization at 45 degrees (or some other angle) to horizontal? The answer is that there is a relationship, but it's a little complicated to get into now: we'll come back to it. Physicists routinely measure photon polarization in their laboratories. They don't use sunglasses; they use polarization-sensitive photodetectors instead. Despite that intimidating sounding name, these are essentially just like sunglasses, but have a more convenient shape and size for laboratory use, are more accurate, less fashionable, and far more expensive. We'll now describe an experiment involving photon polarization that physicists can do in their laboratories. We'll build up the description of the experiment piece by piece. Along the way there's a few details that may seem ad hoc – some angles of polarization measurement, and things like that. Don't worry too much about those ad hoc details, just try to get the basic picture straight. Let's begin by imagining an experimentalist named Alice. Alice is measuring a single photon to determine whether or not it has horizontal polarization. Alice will record A = 1 A = 1 A = 1 when it has horizontal polarization, and A = − 1 A = -1 A = − 1 when it does not. Of course, Alice might have decided to measure a different polarization, say at an angle of 45 degrees to the horizontal. Alice will record B = 1 B = 1 B = 1 when it has a polarization at 45 degrees to the horizontal, and B = − 1 B = -1 B = − 1 when it does not. Here's a diagram summarizing the different things we want you to imagine Alice doing. By the way, we haven't put the photon she's measuring into the diagram, but you should imagine it coming into the screen, toward the sunglasses: Now, remember what we were saying earlier – about our everyday assumption that objects have objective, intrinsic properties that can be measured. By analogy, you'd expect that a photon intrinsically “knows” whether it has a horizontal polarization or not. And it should also know whether it has a polarization at 45 degrees to horizontal or not. It turns out the world doesn't work that way! What we'll now explain is that there are fundamental physical properties that don't have an independent existence like this. In particular, we'll see that prior to Alice measuring the A A A or B B B polarization, the photon itself actually doesn't know what the value for A A A or B B B is going to be. This is utterly unlike our everyday experience. It's as though a coin doesn't decide whether to be heads or tails until we've measured it. Or, in terms of the epigraph with which we began this section, that the moon isn't really there when nobody looks. (That last paragraph may have sounded like gobbledygook. If it didn't give you pause, you should reread it. It's difficult to understand because it's really a declaration of non-understanding, a declaration that the world is radically different from our ordinary, intuitive understanding.) To see why this is the case, we'll first proceed on the assumption that our everyday view of the world is correct. That is, we'll assume photons really do know whether they have horizontal polarization or not, i.e., they have intrinsic values A = 1 A = 1 A = 1 or A = − 1 A = -1 A = − 1 , and also B = 1 B = 1 B = 1 or B = − 1 B = -1 B = − 1 . We'll find that this assumption leads us to a conclusion that is contradicted by real experiments. The only way this could be the case was if our original assumption was in fact wrong, i.e., photons don't have intrinsic properties in this way. This strategy may sound complicated, but it's a common approach to everyday reasoning. Imagine your Uncle has shown you how to bake a cake. You decide to bake it on your own, but realize partway through that you've forgotten whether he said to put one or two cups of flour into the cake. You decide to proceed on the assumption that it was one cup of flour. Unfortunately, the cake falls and is a disaster; you conclude that your original assumption was wrong, and the recipe must have called for two cups. In a similar way, if we proceed on the assumption that photons do have intrinsic, objective values for A A A and B B B , but then arrive at a conclusion which is contradicted by experiment, we'll know our original assumptions must have been wrong. Alright, let's finish describing the experiment. In addition to Alice, the experiment involves another experimentalist, Bob, and a third person, Eve, who prepares two photons and then sends one to Alice and one to Bob. When the photon gets to Alice, she measures either the A A A or the B B B polarization, as described above. She makes the choice of which polarization to measure at random (e.g., by flipping a coin), for reasons which we'll understand later. When the photon gets to Bob, he decides at random to measure either a polarization C C C at 22.5 degrees to horizontal, or D D D , at 67.5 67.5 6 7 . 5 degrees to horizontal. Here's a picture summarizing most of what's going on, although it leaves out Eve: To make this all more concrete, let's think about what might happen in a typical instance of the experiment. Over on Alice's side, she decides to measure the B B B polarization of her photon, and gets the result 1 1 1 , i.e., the photon is polarized at 45 45 4 5 degrees to horizontal. Over on Bob's side, he decides to measure the C C C polarization of his photon, and gets the result − 1 -1 − 1 , i.e., the photon does not have polarization at 22.5 22.5 2 2 . 5 degrees to horizontal. You might imagine Alice, Bob, and Eve doing this experiment many times. If they did, then you could conveniently represent the sequence of runs of the experiment in a table: A B C D 1 1 -1 -1 1 1 -1 1 Each row of the table shows the results from a single run of the experiment, so this table shows a case where the experiment was done four times. Looking at the first row of the table, we see that in the first run of the experiment Alice chose to measure A A A , and got the result 1 1 1 , while Bob chose to measure D D D , and also got the result 1 1 1 . Running further down the table we can see for each experimental run which polarization directions Alice and Bob chose, and what result they got. Now that we've understood the basic experimental setup, let's move onto the analysis leading to the Bell inequality. Remember, we're starting from the assumption that both photons in the experiment have independently existing and well-defined values for A A A , B B B , C C C , and D D D . Two of these four values are revealed in any given instance of the experiment, depending on what Alice and Bob choose to measure. However, because all four quantities have (by assumption) an independent existence, we can consider quantities which involve all four, like the quantity Q Q Q defined by the equation: Q : = A C + B C + B D – A D . Q := AC + BC + BD – AD. Q : = A C + B C + B D – A D . (To be really explicit, quantities like A C AC A C mean A × C A \\times C A × C , i.e., we're omitting the multiplication sign.) Although Q Q Q ' s definition appears to have come from out of the blue, it's at least easy to calculate for any given set of values for A A A , B B B , C C C , and D D D . For example, when A = 1 A = 1 A = 1 , B = − 1 B = -1 B = − 1 , C = 1 C = 1 C = 1 , and D = 1 D = 1 D = 1 , we get: Q = 1 × 1 + ( − 1 ) × 1 + ( − 1 ) × 1 − 1 × 1 = − 2. Q = 1 \\times 1 + (-1) \\times 1 + (-1) \\times 1 - 1 \\times 1 = -2. Q = 1 × 1 + ( − 1 ) × 1 + ( − 1 ) × 1 − 1 × 1 = − 2 . In fact, it turns out that no matter what value A A A , B B B , C C C , and D D D have, the value of Q Q Q is always equal to either 2 2 2 or − 2 -2 − 2 . Perhaps the easiest way to see this is simply to run through all 16 16 1 6 sets of possible values for A A A , B B B , C C C , and D D D , and verify that Q Q Q is indeed always either 2 2 2 or − 2 -2 − 2 . Of course, it's a little tedious to run through all 16 16 1 6 cases, and we don't think there's much point in writing them all out here (though you may wish to run through the cases). A slicker way of seeing Q Q Q is always 2 2 2 or − 2 -2 − 2 is to rewrite Q = ( A + B ) C + ( B − A ) D Q = (A+B)C + (B-A)D Q = ( A + B ) C + ( B − A ) D . We can then split our analysis up into two cases. In the first case A = B A = B A = B , causing the ( B − A ) (B-A) ( B − A ) terms in Q Q Q to vanish, leaving just the ( A + B ) C (A+B)C ( A + B ) C term. A bit of thought and experimentation should convince you this is either 2 2 2 or − 2 -2 − 2 . In the second case, A = − B A = -B A = − B , causing the ( A + B ) (A+B) ( A + B ) terms in Q Q Q to vanish, and leaving just the ( B − A ) D (B-A)D ( B − A ) D term. Again, a bit of thought should convince you this is either 2 2 2 or − 2 -2 − 2 . Now, when Alice and Bob actually do an experiment, Alice chooses to measure just one of A A A or B B B , and Bob chooses to measure just one of C C C or D D D . This means they have no way of measuring Q Q Q directly, although on any given run they can determine one of the four terms that make up Q Q Q , that is, they can always determine one of A C AC A C , B C BC B C , B D BD B D , or − A D -AD − A D . But if they repeat the experiment many times, Alice and Bob can estimate average values for each of the four quantities, A C AC A C , B C BC B C , B D BD B D , and − A D -AD − A D . Because the sum of these four quantities is always 2 2 2 or − 2 -2 − 2 , as we've seen, the sum of their averages over multiple runs of the experiment can not possibly be more than 2 2 2 : Avg ( A C ) + Avg ( B C ) + Avg ( B D ) − Avg ( A D ) ≤ 2. \\text{Avg}(AC)+\\text{Avg}(BC)+\\text{Avg}(BD)-\\text{Avg}(AD) \\leq 2. Avg ( A C ) + Avg ( B C ) + Avg ( B D ) − Avg ( A D ) ≤ 2 . This result is the Bell inequality (sometimes also called the Clauser-Horne-Shimony-Holt or CHSH inequality). You might wonder why we need averages in the Bell inequality. Why can't Alice measure both A A A and B B B , and Bob measure both C C C and D D D , so they can determine Q Q Q directly? To understand this, remember that the idea we're exploring is the idea that the photon has an actual intrinsic value for A A A , and an actual intrinsic value for B B B , each of which is merely revealed by the measurement. A single photon is quite delicate, and if Alice measured both A A A and B B B , there's a good chance the measurement of A A A would interfere with the measurement of B B B , and vice versa, and so mess up the measurement of Q Q Q . To keep things clean we force Alice to choose which one she wants to measure in any given run of the experiment, and stick to it. Similarly for Bob and choosing between C C C and D D D . That's why we have to work with averages over many experiments. If you're a bit more paranoid, you might wonder if maybe Alice's measurement could interfere with what Bob sees. This may seem unlikely, but it's at least plausible. But Einstein's relativity tells us that no influence can travel faster than the speed of light. If Alice and Bob do their measurements near-simultaneously and very quickly, nothing Alice does can possibly affect what Bob sees. As a result, if Alice and Bob do the experiment many times, they can estimate the averages Avg ( A C ) \\text{Avg}(AC) Avg ( A C ) , Avg ( B C ) \\text{Avg}(BC) Avg ( B C ) , and so on, and check that the Bell inequality does, in fact, hold. In the early 1980s, Alain Aspect did a series of experiments to determine whether the Bell inequality holds in nature. And Aspect found that if Eve prepares the two photons in just the right way, then what Alice and Bob see after many runs of the experiment is Aspect and his collaborators published a series of papers on the subject in 1981 and 1982. We will simply link to the final paper: Alain Aspect, Jean Dalibard, and Gerard Roger, Experimental Test of Bell's Inequalities Using Time-Varying Analyzers (1982). Note that the paper considers a slight variation on the Bell inequality we have considered here. In particular, it considers a quantity S S S which is related to our Q Q Q by Q = 2 + 4 S Q = 2+4S Q = 2 + 4 S , and the Bell inequality and its violations are rewritten in terms of S S S . : Avg ( A C ) + Avg ( B C ) + Avg ( B D ) − Avg ( A D ) ≈ 2.4. \\text{Avg}(AC)+\\text{Avg}(BC)+\\text{Avg}(BD)-\\text{Avg}(AD) \\approx 2.4. Avg ( A C ) + Avg ( B C ) + Avg ( B D ) − Avg ( A D ) ≈ 2 . 4 . That is, Aspect and collaborators found that the Bell inequality fails to hold in the real world! And since it doesn't hold in the real world, there must be some assumption we made in deriving the Bell inequality that fails to be true in the real world. Perhaps most obviously, our argument for the Bell inequality relies on a two-part set of assumptions often known as local realism. The first part is the idea that the universe is local, that is, influences can't propagate instantaneously. And second, as we've mentioned ad nauseum, the idea that the universe is realistic, meaning that properties of physical systems like polarization have an intrinsic, independent existence, an existence which is merely revealed by the results of measurement. Physicists thus often summarize the experimental violation of the Bell inequality as proving that the universe cannot be locally realistic. This is an astonishing conclusion! In our opinion, the Aspect 1981/1982 experiments are crucial experiments in the history of science. They show the universe violates human intuition about reality in some extraordinarily strong sense. Indeed, if you accept locality – as most physicists do – then the violation of the Bell inequalities by nature forces you to give up the idea of an independent, objective reality. Of course, people have torn apart every piece of both the experiments where the Bell inequality is violated, and also every implicit assumption underlying the proof. Every assumption is fair game: you can go as deep as you like, ultimately asking questions about whether you accept ideas like the independence of different runs of the experiment, or whether it is possible for Alice and Bob to truly choose which polarization to measure. That said, the conventional (though not universal) wisdom among physicists who have studied the Bell inequality is that: (a) the Bell inequality is violated in nature; and (b) this forces you to abandon realism. Although there are ways of escaping these conclusions, most people feel the proposed cures are worse than the disease. We don't want to go all New Age woo on you, but it's only a slight exaggeration to say the moral is that reality isn't real; it's not reality which needs an update, it's our notion of what is real. It's an unfortunate fact that Einstein died in 1955, 9 years before Bell obtained the Bell inequalities, and 27 years before the Aspect experiment. So we don't know what he would have thought about all this. The physicist David Mermin has a nice account of asking a colleague how he thought Einstein would have reacted to Bell's theorem. He said that Einstein would have gone home and thought about it hard for several weeks that he couldn't guess what he would then have said, except that it would have been extremely interesting. He was sure that Einstein would have been very bothered by Bell's theorem. To conclude, it's worth re-emphasizing that the Bell inequality does not directly involve quantum mechanics in any way – it's quite a common confusion that it's a result of quantum mechanics. Nothing could be further from the truth! The Bell inequality is, rather, a consequence of adopting a local realistic view of the world. But since that view has been rebutted by nature, we are forced to seek an alternate way of understanding the world, a way radically different from our conventional way of thinking. As we'll see in the next section, one solution is to use quantum mechanics. Didn’t remember Remembered Using quantum mechanics to describe Bell inequality violations In this section, we develop an example where quantum mechanics violates the Bell inequality. This implies that quantum mechanics is not a locally realistic theory, and also means that it has some chance of describing experimental results like the Aspect experiment. To develop this example, we're going to use our familiar language of qubits, computational basis measurements, and so on, instead of photons and polarization. It can all be mapped back onto the photon experiments, but explaining that mapping up front is more trouble than it's worth, given the experience we already have with qubits. To do all this, we need to compute the average values associated to certain quantum measurements. There are quite a few details, so we'll just sketch the calculation. It's fine to follow along in more or less detail, to your own taste: you can drill down into details if you wish, or you can simply read quickly and focus on the conclusions, if that's your preference. The questions at the end of the section will thus be more conceptual in nature, and not about the details of the calculation. As background, it helps to first explain a mathematical trick which makes it easier to compute the average outcome from a quantum measurement. As a concrete example, suppose we consider a measurement on a single qubit with two possible outcomes, + 1 +1 + 1 and − 1 -1 − 1 . The + 1 +1 + 1 outcome will be associated to a measurement operator M 1 = ∣ 0 ⟩ ⟨ 0 ∣ M_1 = |0\\rangle \\langle 0| M 1 ​ = ∣ 0 ⟩ ⟨ 0 ∣ , while the − 1 -1 − 1 outcome has measurement operator M − 1 = ∣ 1 ⟩ ⟨ 1 ∣ M_{-1} = |1\\rangle \\langle 1| M − 1 ​ = ∣ 1 ⟩ ⟨ 1 ∣ . Imagine you prepare a state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ repeatedly, and perform this measurement over and over. What is the average outcome? By definition, the average is just p ( 1 ) ∗ 1 + p ( − 1 ) ∗ ( − 1 ) p(1)*1+p(-1)*(-1) p ( 1 ) ∗ 1 + p ( − 1 ) ∗ ( − 1 ) . More generally, if we have a set of measurement operators { M m } \\{M_m\\} { M m ​ } then the average outcome is ∑ m p ( m ) m = ∑ m ⟨ ψ ∣ M m † M m ∣ ψ ⟩ m , \\sum_m p(m) m = \\sum_m \\langle \\psi| M_m^\\dagger M_m |\\psi\\rangle m, m ∑ ​ p ( m ) m = m ∑ ​ ⟨ ψ ∣ M m † ​ M m ​ ∣ ψ ⟩ m , where the term on the left is just the definition of the average, and the term on the right comes from the third postulate. But we can take the sum inside and we see that the average is just ⟨ ψ ∣ M ∣ ψ ⟩ \\langle \\psi| M | \\psi\\rangle ⟨ ψ ∣ M ∣ ψ ⟩ where M : = ∑ m m M m † M m . M := \\sum_m m M_m^\\dagger M_m. M : = m ∑ ​ m M m † ​ M m ​ . M M M is known as the observable corresponding to the measurement { M m } \\{ M_m \\} { M m ​ } . This observable is a single, fixed operator which depends only on the measurement outcomes m m m and measurement operators M m M_m M m ​ . Yet if you know the observable then it is often easy to compute the average measurement outcome: it's just ⟨ ψ ∣ M ∣ ψ ⟩ \\langle \\psi| M |\\psi\\rangle ⟨ ψ ∣ M ∣ ψ ⟩ . Indeed, this is so convenient that it's often written in an abbreviated form as ⟨ M ⟩ : = ⟨ ψ ∣ M ∣ ψ ⟩ \\langle M \\rangle := \\langle \\psi| M |\\psi\\rangle ⟨ M ⟩ : = ⟨ ψ ∣ M ∣ ψ ⟩ . And so people often use ⟨ M ⟩ \\langle M \\rangle ⟨ M ⟩ as the notation for an average in quantum mechanics. Let's come back to the example mentioned above, with measurement operators M 1 = ∣ 0 ⟩ ⟨ 0 ∣ M_1 = |0\\rangle \\langle 0| M 1 ​ = ∣ 0 ⟩ ⟨ 0 ∣ , and M − 1 = ∣ 1 ⟩ ⟨ 1 ∣ M_{-1} = |1\\rangle \\langle 1| M − 1 ​ = ∣ 1 ⟩ ⟨ 1 ∣ . The corresponding observable is then: M = ∣ 0 ⟩ ⟨ 0 ∣ 0 ⟩ ⟨ 0 ∣ − ∣ 1 ⟩ ⟨ 1 ∣ 1 ⟩ ⟨ 1 ∣ = ∣ 0 ⟩ ⟨ 0 ∣ − ∣ 1 ⟩ ⟨ 1 ∣ . \\begin{aligned} M & = |0\\rangle\\langle 0|0\\rangle \\langle 0| -|1\\rangle\\langle 1|1\\rangle\\langle 1| \\\\ & = |0\\rangle \\langle 0| -|1\\rangle \\langle 1|. \\end{aligned} M ​ = ∣ 0 ⟩ ⟨ 0 ∣ 0 ⟩ ⟨ 0 ∣ − ∣ 1 ⟩ ⟨ 1 ∣ 1 ⟩ ⟨ 1 ∣ = ∣ 0 ⟩ ⟨ 0 ∣ − ∣ 1 ⟩ ⟨ 1 ∣ . ​ Rewriting in matrix form, this is: M = [ 1 0 0 − 1 ] . M = \\left[ \\begin{array}{cc} 1 & 0 \\\\ 0 & -1 \\end{array} \\right]. M = [ 1 0 ​ 0 − 1 ​ ] . This is just the Pauli Z Z Z matrix. And so if we want to compute the average value from such a measurement, it's just ⟨ ψ ∣ Z ∣ ψ ⟩ \\langle \\psi|Z|\\psi\\rangle ⟨ ψ ∣ Z ∣ ψ ⟩ . More generally, we can tip this process upside down, using an observable to define a quantum measurement. In particular, suppose M M M is a Hermitian matrix acting on the quantum system's state space. Then a result known as the spectral theorem from linear algebra A somewhat peculiar but enlightening account may be found here. Unfortunately, the treatment on Wikipedia buries many of the key ideas in a mass of detail. guarantees that M M M can be decomposed as: M = ∑ λ λ ∣ λ ⟩ ⟨ λ ∣ , M = \\sum_{\\lambda} \\lambda |\\lambda\\rangle \\langle \\lambda|, M = λ ∑ ​ λ ∣ λ ⟩ ⟨ λ ∣ , where the sum is over all eigenvalues λ \\lambda λ and corresponding (normalized) eigenvectors ∣ λ ⟩ |\\lambda\\rangle ∣ λ ⟩ of M M M . We can then define a quantum measurement with measurement operators M λ = ∣ λ ⟩ ⟨ λ ∣ M_\\lambda = |\\lambda\\rangle \\langle \\lambda| M λ ​ = ∣ λ ⟩ ⟨ λ ∣ . The completeness relation is easily checked to be true, and so this is a valid quantum measurement. What's more, you can check that M M M is the observable corresponding to those choices. This perhaps seems rather abstract and indirect. It's done because sometimes it's easier to specify a quantum measurement simply by specifying the observable. This can be particularly convenient when we're most interested in the average outcome from a measurement, as in the case of the Bell inequality. Indeed, in the case of the Bell inequality violations, the four observables we'll choose will correspond to: A = Z B = X + Z 2 C = X D = X − Z 2 \\begin{aligned} A & = Z \\\\ B & = \\frac{X+Z}{\\sqrt 2} \\\\ C & = X \\\\ D & = \\frac{X-Z}{\\sqrt 2} \\end{aligned} A B C D ​ = Z = 2 ​ X + Z ​ = X = 2 ​ X − Z ​ ​ It's simple to check that all these are Hermitian matrices, with eigenvalues + 1 +1 + 1 and − 1 -1 − 1 , and so define corresponding quantum measurements. You can, if you like, explicitly work out the corresponding eigenvectors and measurement operators. Doing so is likely good for your quantum mechanical soul (or at least your quantum mechanical work ethic) If you do so you'll discover that the X X X observable corresponds to measuring in the ∣ + ⟩ , ∣ − ⟩ |+\\rangle, |-\\rangle ∣ + ⟩ , ∣ − ⟩ basis, as we discussed earlier.. But there's also a sense in which it's good practice to leave those things undetermined. After all, we're only going to be computing averages: we don't need to explicitly know the individual measurement operators! To get a Bell inequality violation in quantum mechanics, we're going to imagine that our two experimental parties, Alice and Bob, are sharing a two-qubit state. In fact, it's an entangled state we met both in Quantum computing for the very curious and (if you read it) How quantum teleportation works : ∣ ψ ⟩ = ∣ 00 ⟩ + ∣ 11 ⟩ 2 . |\\psi\\rangle = \\frac{|00\\rangle+|11\\rangle}{\\sqrt 2}. ∣ ψ ⟩ = 2 ​ ∣ 0 0 ⟩ + ∣ 1 1 ⟩ ​ . To reiterate, we suppose the quantum measurements Alice and Bob do correspond to the observables defined above: A = Z B = X + Z 2 C = X D = X − Z 2 \\begin{aligned} A & = Z \\\\ B & = \\frac{X+Z}{\\sqrt 2} \\\\ C & = X \\\\ D & = \\frac{X-Z}{\\sqrt 2} \\end{aligned} A B C D ​ = Z = 2 ​ X + Z ​ = X = 2 ​ X − Z ​ ​ To check whether the Bell inequalities hold or are violated, we want to compute: Avg ( A B ) + Avg ( C B ) + Avg ( C D ) − Avg ( A D ) . \\text{Avg}(AB)+\\text{Avg}(CB)+\\text{Avg}(CD)-\\text{Avg}(AD). Avg ( A B ) + Avg ( C B ) + Avg ( C D ) − Avg ( A D ) . With our choices for A , B , C , D A, B, C, D A , B , C , D this average can be shown to be We're skipping over some plausible-but-need-to-be-checked steps here. In particular, we need to show that if Alice does the measurement corresponding to the observable A A A , then immediately afterward Bob does the measurement corresponding to the measurement B B B , the average of the product of the measurement results will just be ⟨ ψ ∣ A ⊗ B ∣ ψ ⟩ \\langle \\psi| A \\otimes B |\\psi\\rangle ⟨ ψ ∣ A ⊗ B ∣ ψ ⟩ . Proving this is a good exercise. : ⟨ Z ⊗ X + Z 2 ⟩ + ⟨ X ⊗ X + Z 2 ⟩ + ⟨ X ⊗ X − Z 2 ⟩ − ⟨ Z ⊗ X − Z 2 ⟩ . \\langle Z \\otimes \\frac{X+Z}{\\sqrt 2} \\rangle + \\langle X \\otimes \\frac{X+Z}{\\sqrt 2} \\rangle + \\langle X \\otimes \\frac{X-Z}{\\sqrt 2} \\rangle - \\langle Z \\otimes \\frac{X-Z}{\\sqrt 2} \\rangle. ⟨ Z ⊗ 2 ​ X + Z ​ ⟩ + ⟨ X ⊗ 2 ​ X + Z ​ ⟩ + ⟨ X ⊗ 2 ​ X − Z ​ ⟩ − ⟨ Z ⊗ 2 ​ X − Z ​ ⟩ . To figure this out, let's start with the first term: ⟨ Z ⊗ X + Z 2 ⟩ = ⟨ ψ ∣ Z ⊗ X ∣ ψ ⟩ + ⟨ ψ ∣ Z ⊗ Z ∣ ψ ⟩ 2 . \\begin{aligned} \\langle Z \\otimes \\frac{X+Z}{\\sqrt 2} \\rangle & = \\frac{\\langle \\psi| Z \\otimes X |\\psi\\rangle+\\langle \\psi|Z \\otimes Z|\\psi\\rangle}{\\sqrt 2}. \\end{aligned} ⟨ Z ⊗ 2 ​ X + Z ​ ⟩ ​ = 2 ​ ⟨ ψ ∣ Z ⊗ X ∣ ψ ⟩ + ⟨ ψ ∣ Z ⊗ Z ∣ ψ ⟩ ​ . ​ We can see that determining the averages is going to involve many computations that look like ⟨ ψ ∣ ⋅ ⊗ ⋅ ∣ ψ ⟩ \\langle \\psi| \\cdot \\otimes \\cdot |\\psi\\rangle ⟨ ψ ∣ ⋅ ⊗ ⋅ ∣ ψ ⟩ , where the missing bits are Pauli operators. This is tedious to compute, but not especially difficult. If you are fluent with the Pauli operators it can be done pretty quickly. If you're less fluent, you can treat it as an exercise in becoming more fluent (the Pauli matrices are excellent things to become fluent with, but you should stop before you get bored). Let's compute a few such terms. We'll start with ⟨ ψ ∣ Z ⊗ X ∣ ψ ⟩ \\langle \\psi|Z \\otimes X |\\psi\\rangle ⟨ ψ ∣ Z ⊗ X ∣ ψ ⟩ . Substituting in ∣ ψ ⟩ = ∣ 00 ⟩ + ∣ 11 ⟩ 2 |\\psi\\rangle = \\frac{|00\\rangle+|11\\rangle}{\\sqrt 2} ∣ ψ ⟩ = 2 ​ ∣ 0 0 ⟩ + ∣ 1 1 ⟩ ​ this becomes: ⟨ Z ⊗ X ⟩ = ( ⟨ 00 ∣ + ⟨ 11 ∣ ) ( Z ⊗ X ) ( ∣ 00 ⟩ + ∣ 11 ⟩ ) 2 . \\begin{aligned} \\langle Z \\otimes X \\rangle & = \\frac{(\\langle 00|+\\langle 11|)(Z \\otimes X) (|00\\rangle+|11\\rangle)}{2}. \\end{aligned} ⟨ Z ⊗ X ⟩ ​ = 2 ( ⟨ 0 0 ∣ + ⟨ 1 1 ∣ ) ( Z ⊗ X ) ( ∣ 0 0 ⟩ + ∣ 1 1 ⟩ ) ​ . ​ But ( Z ⊗ X ) ∣ 00 ⟩ = ∣ 01 ⟩ (Z \\otimes X)|00\\rangle = |01\\rangle ( Z ⊗ X ) ∣ 0 0 ⟩ = ∣ 0 1 ⟩ , since the Z Z Z leaves the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state alone, and X X X flips ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ to ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ . Similarly, ( Z ⊗ X ) ∣ 11 ⟩ = − ∣ 10 ⟩ (Z\\otimes X) |11\\rangle = -|10\\rangle ( Z ⊗ X ) ∣ 1 1 ⟩ = − ∣ 1 0 ⟩ . And so: ⟨ Z ⊗ X ⟩ = ( ⟨ 00 ∣ + ⟨ 11 ∣ ) ( ∣ 01 ⟩ − ∣ 10 ⟩ ) 2 . \\begin{aligned} \\langle Z \\otimes X \\rangle & = \\frac{(\\langle 00|+\\langle 11|)(|01\\rangle-|10\\rangle)}{2}. \\end{aligned} ⟨ Z ⊗ X ⟩ ​ = 2 ( ⟨ 0 0 ∣ + ⟨ 1 1 ∣ ) ( ∣ 0 1 ⟩ − ∣ 1 0 ⟩ ) ​ . ​ Of course, all the inner products vanish, and so ⟨ Z ⊗ X ⟩ = 0 \\langle Z \\otimes X \\rangle = 0 ⟨ Z ⊗ X ⟩ = 0 . What about ⟨ Z ⊗ Z ⟩ \\langle Z \\otimes Z \\rangle ⟨ Z ⊗ Z ⟩ ? The calculation is very similar. Going through it with a little less explanation we obtain: ⟨ Z ⊗ Z ⟩ = ( ⟨ 00 ∣ + ⟨ 11 ∣ ) ( Z ⊗ Z ) ( ∣ 00 ⟩ + ∣ 11 ⟩ ) 2 = ( ⟨ 00 ∣ + ⟨ 11 ∣ ) ( ∣ 00 ⟩ + ∣ 11 ⟩ ) 2 = 2 2 = 1. \\begin{aligned} \\langle Z \\otimes Z \\rangle & = \\frac{(\\langle 00|+\\langle 11|) (Z \\otimes Z) (|00\\rangle+|11\\rangle)}{2} \\\\ & = \\frac{(\\langle 00|+\\langle 11|)(|00\\rangle+|11\\rangle)}{2} \\\\ & = \\frac{2}{2} \\\\ & = 1. \\end{aligned} ⟨ Z ⊗ Z ⟩ ​ = 2 ( ⟨ 0 0 ∣ + ⟨ 1 1 ∣ ) ( Z ⊗ Z ) ( ∣ 0 0 ⟩ + ∣ 1 1 ⟩ ) ​ = 2 ( ⟨ 0 0 ∣ + ⟨ 1 1 ∣ ) ( ∣ 0 0 ⟩ + ∣ 1 1 ⟩ ) ​ = 2 2 ​ = 1 . ​ So we see ⟨ Z ⊗ Z ⟩ = 1 \\langle Z \\otimes Z \\rangle = 1 ⟨ Z ⊗ Z ⟩ = 1 , and as a result ⟨ A ⊗ B ⟩ = 1 2 \\langle A \\otimes B \\rangle = \\frac{1}{\\sqrt 2} ⟨ A ⊗ B ⟩ = 2 ​ 1 ​ . All the remaining terms on the left-hand side of the Bell inequality can be computed very similarly. We won't explicitly work through them here. When you do, you learn that Avg ( C B ) = 1 2 \\text{Avg}(CB) = \\frac{1}{\\sqrt 2} Avg ( C B ) = 2 ​ 1 ​ , Avg ( C D ) = 1 2 \\text{Avg}(CD) = \\frac{1}{\\sqrt 2} Avg ( C D ) = 2 ​ 1 ​ , and Avg ( A D ) = − 1 2 \\text{Avg}(AD) = -\\frac{1}{\\sqrt 2} Avg ( A D ) = − 2 ​ 1 ​ . And so: Avg ( A B ) + Avg ( C B ) + Avg ( B D ) − Avg ( A D ) = 1 2 + 1 2 + 1 2 − − 1 2 = 4 2 = 2 2 ≈ 2.8. \\begin{aligned} \\text{Avg}(AB)+\\text{Avg}(CB)+\\text{Avg}(BD)-\\text{Avg}(AD) & = \\frac{1}{\\sqrt 2}+\\frac{1}{\\sqrt 2}+\\frac{1}{\\sqrt 2}-\\frac{-1}{\\sqrt 2} \\\\ & = \\frac{4}{\\sqrt 2} \\\\ & = 2\\sqrt{2}\\\\ & \\approx 2.8. \\end{aligned} Avg ( A B ) + Avg ( C B ) + Avg ( B D ) − Avg ( A D ) ​ = 2 ​ 1 ​ + 2 ​ 1 ​ + 2 ​ 1 ​ − 2 ​ − 1 ​ = 2 ​ 4 ​ = 2 2 ​ ≈ 2 . 8 . ​ In other words, just as we claimed, quantum mechanics violates the Bell inequality! And thus quantum mechanics is not a locally realistic theory. This is all for qubits. In fact, in quantum optics a photon's polarization is modeled essentially as a qubit, with horizontal polarization corresponding to the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state, the 45 45 4 5 degree polarization to the ∣ 0 ⟩ + ∣ 1 ⟩ 2 \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ state, and so on. We won't go through the detailed correspondence, but our quantum mechanical calculation above really does correspond to a calculation about a photon polarization experiment. You may recall that the Aspect experiment gave a value for the sum of averages of 2.4 2.4 2 . 4 , not 2.8 2.8 2 . 8 . The reason is that the photodetectors in the Aspect experiment aren't perfectly efficient, and so don't capture all photons involved; the Aspect paper claims a quantum calculation with all the detector efficiencies accounted for gives a prediction just a little higher than the 2.4 2.4 2 . 4 observed, well within experimental error Unfortunately, the paper is unclear on these points, mostly making claims without much detailed explanation or calculation; our comments are based on what seems a reasonable read, and deserve further checking.. More recent experiments have used much better photodetectors, and more closely approximate the 2.8 2.8 2 . 8 value. And so, not only does the Bell inequality and Aspect experiment lead us to reject the intuitive local realistic view of the world, it also confirms the predictions of quantum mechanics. Why care about the Bell inequality, and its violation by quantum mechanics? Sure, it's weird. It's the kind of thing which provokes a “Whoah!” response. But astonishment aside, there's lots of weird things in the world. So what? One reason to care is that fundamental physics is still incomplete. For instance, there's still no accepted, well-developed quantum theory of gravity. In fact, general relativity – our best current theory of gravity – is a local realistic theory: the Bell inequality isn't violated in general relativity. So in order to develop a full quantum theory of gravitation, among many other challenges we'd need to reproduce the existing (local realistic) predictions where general relativity has been well tested, while also reproducing the (not always local realistic) predictions of quantum mechanics where that has been well tested. Doing both is not trivial! Another way of looking at this is that Bell inequality violations tell you there's not going to be any way to get a quantum theory of gravity that is locally realistic. This straight away rules out many possible extensions of general relativity. Indeed, over the past decade much work on quantum gravity has focused on understanding the role entangled states play in quantum gravity A fun overview is this popular essay.. This focus is natural, in the light of Bell inequality violations. How would you ever have come up with the Bell inequality and with the quantity Q Q Q ? In our presentation, it appeared out of nowhere. As did the particular experimental construction – all those polarization angles(!) – violating the Bell inequality. Of course, the Bell inequality didn't come out of nowhere. It was motivated by the desire to find a sharp criterion separating the local realistic view of the world from the quantum mechanical view of the world. It's instructive to think about possible variations and simplifications. For instance, in the Bell inequality we've described it's important that Alice and Bob each make a choice of what to measure. This gives rise to four possible combinations of measurement. What would happen if Alice and Bob each make a fixed measurement, say A A A and B B B ? Is it possible to find some Bell-like inequality satisfied by the product A B AB A B , but violated by a prediction of quantum mechanics? If you try this, you quickly discover it's surprisingly difficult. And so you might decide to allow Bob two choices of possible measurement, say B B B or C C C , and perhaps consider a quantity like A B + A C AB+AC A B + A C , and whether there's a Bell-like inequality which can be violated by quantum mechanics. We actually don't know the answer here – it's fun to think about, and to try playing with examples. Indeed, professional physicists have done quite a bit of work in this vein – trying to simplify and better understand different variations of the Bell inequalities. Of course, it's quite time-intensive: you need to try lots of things, and mostly it's hard to see how they might work. But it's good fun, and good for deepening your understanding of both quantum mechanics and the Bell inequality. Indeed, you're not so very far from the research frontier here. Of course, physicists have pushed on from the early Bell inequalities and the Aspect experiment. But there's certainly still more to discover. Didn’t remember Remembered Why then are so many physicists so upset about quantum mechanics? The Bell inequality is just one of many phenomena that bother people about quantum mechanics. We'll now briefly describe a couple more, to give you a flavor for why people are bothered. If you'd like more depth a good introduction is Asher Peres's superb book “Quantum Theory: Concepts and Methods”. The book is now somewhat dated (it's from 1995), but still provides a clear account of many foundational issues, and is good preparation for understanding more recent work. Why are there two types of dynamics? One oddity of quantum mechanics is that it has two different ways of describing the way states change: unitary dynamics, and the dynamics associated to measurement. This gives rise to a striking set of puzzles. Suppose we have a quantum system – let's call it Q Q Q – that's being measured. The measuring device is, of course, itself a quantum system. It seems therefore that it should be possible to find some larger quantum system which is isolated and that includes both Q Q Q and the measuring device. According to the second postulate, that larger system is undergoing unitary evolution. What this means is that quantum mechanics now offers two seemingly different ways of describing the evolution of the system being measured: either through measurement operators and probabilities, as in the third postulate. Or in a completely deterministic way, under the unitary evolution of the larger system. Having two different ways of describing the same situation prompts many questions: Can we guarantee the two descriptions are always consistent with one another? Is it possible to derive one description from the other? How can probabilities arise out of what seems like a deterministic description of the larger system? In particular, doesn't it seem strange that the quantum state for the combined system is evolving in a purely deterministic fashion, while the quantum state for the measured system changes at random? Shouldn't there be some way for us to derive the measurement probabilities from the deterministic evolution of the state of the combined system? In short: how can we reconcile these two points of view? These are good questions. Together, these and other related questions are often known as the measurement problem in quantum mechanics. Many resolutions of the measurement problem have been published by individual physicists. Unfortunately, the physics community as a whole has not agreed upon a complete resolution. We've heard variations on: “this has all been solved by Bohr; by von Neumann; by Everett; by Bohm; etc ”. Unfortunately, for all the dozens or hundreds of resolutions which have been published there are also standard counterarguments. Indeed, a fun idea for a (very large!) project is to gather up the entire argument tree, containing all the strongest arguments and counterarguments, and cruxes of disagreement. What does the quantum state mean? The interpretation of quantum mechanics: We've learned much about what you can do with quantum states, but never quite said what a quantum state means. We've been using it as a calculational device, a sort of ghost which helps us predict the probability distribution for results of measurements, and which can be used to solve problems like rapidly searching a large search space. But how should we think about the state? What does it mean? This is the problem of interpretation of the quantum state, and, more generally, of quantum mechanics. As with the measurement problem, it's not difficult to find physicists who will confidently tell you how to correctly interpret the quantum state. Often they will follow up by telling you how their interpretation resolves the measurement problem, throws light on the violation of the Bell inequality, and how it will clean your living room as well. Again, the trouble is: there's a lot of disagreement between experts, standard arguments and counterarguments. With all that said, many people have thought long and well about the meaning of the quantum state. As a starting point, we recommend reading Hugh Everett and David Deutsch on the many-worlds interpretation of quantum mechanics; Chris Fuchs on the idea that the quantum state is a state of knowledge; David Bohm on the idea that it's a sort of pilot wave, guiding particles in the system; and Rob Spekkens's broad, thoughtful work on what quantum mechanics means. Going further back in time, there is also much earlier work, some of it by physicists such as Einstein and Bohr who were deeply involved in creating quantum mechanics. A taste may be found in the excellent collection of papers curated by Wheeler and Zurek. Many of these papers are accessible and extremely stimulating, dealing with fundamental questions about the nature of reality. Finally, although it's not exactly an interpretation of the quantum state, we like Richard Feynman's paper recasting quantum mechanics in terms of (sometimes negative!) probability distributions, rather than quantum states. This is just a tiny sample of the many ideas out there. Be aware that many of these people disagree (or disagreed, while alive) strongly with one another. But in controversy there is opportunity, and perhaps exploring that melange of ideas will even get you working on the problem yourself. Why do any of these questions matter? After all, with our existing understanding of quantum mechanics we can develop ideas like quantum search and quantum teleportation. And, although we haven't gone through the details, it can also be used to make predictions about superconductors and lasers and semiconductors and so on. None of these is directly affected by concerns about the quantum measurement problem, about the meaning of the quantum state, or whether the world is locally realistic. This returns us to the question from the introduction: what does it mean to understand quantum mechanics? If the purpose of a scientific theory is merely to predict the results of experiments, then quantum mechanics is doing an outstanding job. Not just in a practical sense, as with lasers and superconductors, but also with predicting unexpected phenomena like the violation of the Bell inequality, or quantum teleportation and quantum search. Put another way: in some very practical sense you have already understood this Real Black Magic calculus. You know how to work the symbols, you can make astonishing predictions about the world. You really do understand quantum mechanics. But while that's true, we demand more of our scientific theories. Consider Darwin's theory of evolution by natural selection. It's an astonishingly powerful theory, and explained much about the world that had hitherto been incomprehensible. But, as formulated by Darwin, it was not yet complete. It did not provide a detailed understanding of how variation arose in a population, and thus provided a limited understanding of selection. Genetics and successor ideas – from molecular biology to our still-nascent ideas about morphogenesis – have helped address many questions that Darwin left open. In a similar way, while quantum mechanics is an astonishingly successful theory, it still leaves open fundamental questions such as the measurement problem and the interpretation of the quantum state. One day we'll know how to convincingly address those questions, and improve upon our existing understanding of quantum mechanics, in much the same way as modern biology has improved upon Darwin. Furthermore, it may be that solving such problems will help resolve problems such as the search for a quantum theory of gravity. Concluding thought: This essay concludes a series of four essays. It's worth pausing to consider what the complete series gives you: not only an understanding of all the basic principles of quantum mechanics and quantum computing, but also of two major applications ( search and teleportation). If you've worked through the details, then you're well placed to understand more; you're no longer a beginner, but rather a person who understands the fundamentals of quantum mechanics and quantum computing. Along the way, we've developed the mnemonic medium. If you've used the medium then you've participated in a small experiment in changing the way human beings understand new subjects. Of course, the medium has many limitations: it is by no means a silver bullet for understanding! But we believe it shows promise for changing the role of memory in understanding, and for developing fluency in the application of new skills. And that challenges us to ask: how much more powerful can we make such a medium? Acknowledgments Andy and Michael are supported in part through the generous contributions of our Patreon supporters. Andy is supported in part by a grant from Emergent Ventures. Citing this work In academic work, please cite this as: Andy Matuschak and Michael A. Nielsen, “Quantum Mechanics Distilled”, https://quantum.country/qm, San Francisco (2020). Authors are listed in alphabetical order. License This work is licensed under a Creative Commons Attribution-NonCommercial 3.0 Unported License. This means you’re free to copy, share, and build on this essay, but not to sell it. If you’re interested in commercial use, please contact us. Last updated March 15, 2020 See our User Agreement and Privacy Policy."
  },
  {
    "categories": [
      "Computer Science",
      "Quantum Mechanics"
    ],
    "authors": [
      "Andy Matuschak",
      "Michael Nielsen"
    ],
    "title": "How the quantum search algorithm works",
    "link": "https://quantum.country/search",
    "description": "",
    "content": "How the quantum search algorithm works How the quantum search algorithm works Andy Matuschak and Michael Nielsen Part of a series of essays in a mnemonic medium which makes it almost effortless to remember what you read. Quantum computing for the very curious How the quantum search algorithm works How quantum teleportation works Quantum mechanics distilled by Andy Matuschak and Michael Nielsen Presented in a new mnemonic medium which makes it almost effortless to remember what you read. Quantum computing for the very curious How the quantum search algorithm works In-text 5 days 2 weeks 1 month 2 months Long-term The building blocks of the quantum search algorithm Details of the quantum search algorithm How to reflect about the ∣ s ⟩ |s\\rangle ∣ s ⟩ and ∣ E ⟩ |E\\rangle ∣ E ⟩ states? Measuring the output Summary of the quantum search algorithm What can we learn from the quantum search algorithm? How quantum teleportation works Quantum mechanics distilled Support us on Patreon Our future projects are funded in part by readers like you. Special thanks to our sponsor-level patrons, Adam Wiggins, Andrew Sutherland, Bert Muthalaly, Calvin French-Owen, Dwight Crow, fnnch, James Hill-Khurana, Lambda AI Hardware, Ludwig Petersson, Mickey McManus, Mintter, Patrick Collison, Paul Sutter, Peter Hartree, Sana Labs, Shripriya Mahesh, Tim O'Reilly. Imagine you’re the star of an action movie about a kidnapping. As part of the story, you come into possession of a secret message, which says where the victim is hidden. Unfortunately, the message is encrypted using a 12-digit secret key, i.e., a string of digits such as 8409 … 8409\\ldots\\ 8 4 0 9 … . But you don’t know the secret key. The only way to unlock the message and find the victim is by searching through the N = 1 0 12 N = 10^{12} N = 1 0 1 2 (one trillion) possible keys. While you may get lucky and find the right key early on, on average you’ll need to try N / 2 N/2 N / 2 different keys, and in the worst case you’ll need to try all N N N . I’ve painted a fanciful picture, but similar search-based algorithms are used frequently in computing. They’re often the first approach we try when solving a problem. For example, suppose you’re trying to attack the famous traveling salesperson problem (TSP), that is, trying to find the shortest route that visits every city in a list of cities, while returning to the origin city. A simple approach is to search through all the possible routes, while keeping track of the minimal route found. Of course, it’s possible to develop more sophisticated algorithms for TSP, algorithms that make it unnecessary to search through every route. But that doesn’t take away from the core point: for many problems in computing a search-based approach is a good first-cut way to attack the problem. Indeed, search is sometimes a good final-cut approach, or even provably optimal. Overall, search is an exceptionally useful general-purpose algorithm. As mentioned above, on a conventional classical computer, if we have a search space of N N N items, we need to examine the search space on the order of N N N times to find the item we’re looking for. Remarkably, quantum computers can do far better. It turns out that you can use a quantum computer to solve the search problem after examining the search space roughly N \\sqrt{N} N ​ times! A little more precisely, it needs to examine the search space about π N / 4 \\pi\\sqrt{N}/4 π N ​ / 4 times. That square root factor makes a big difference. If N N N was a trillion, as in our opening scenario, then a classical computer will need to examine the search space a trillion times, while the quantum computer will need to examine it fewer than 800 thousand times. That’s an improvement of more than a factor of a million. When I first heard about the quantum search algorithm I thought it sounded impossible. I just couldn’t imagine any way it could be true. But it is true. In this essay I explain in detail how the quantum search algorithm works. I’ll also explain some limitations of the quantum search algorithm, and discuss what we can learn about quantum computing in general from the quantum search algorithm. To read this essay you need to be familiar with the quantum circuit model of computation. If you’re not, you can learn the elements from the earlier essay Quantum Computing for the Very Curious. It may be tempting to think “Oh, I'm not that interested in the problem of search, why should I bother learning about it?” But the point of this essay is deeper than search. It's to begin answering the question: how can we use quantum computers to do things which are genuinely different and better than a conventional classical computer? The particular problem (search) is almost incidental. And so the essay is about learning to think in the quantum realm, finding non-classical heuristics that let us beat classical computers. This turns out to be immensely challenging, but also immensely fun. Because of these aspirations, I won’t just explain how the search algorithm works. We'll dig down and try to understand why it works, and how you might have discovered the algorithm in the first place. That takes more time than just laying out the quantum circuit, but is also more rewarding. Along the way we’ll learn many other techniques widely used in quantum algorithm design, ideas such as clean computation, the phase trick, quantum parallelism, and others. All this is great experience in learning how to think about quantum algorithm design in general. This essay is an example of what Andy Matuschak and I have dubbed a mnemonic medium – it’s like a regular essay, but incorporates new user interface elements intended to make it almost effortless for you to remember the content of the essay. The motivator is that most people (myself included) quickly forget much of what we read in books and articles. But cognitive scientists studying human memory have understood how to guarantee you will remember something permanently. This mnemonic medium builds those ideas into the essay, making it easy to remember the material for the long term. The core idea of the mnemonic medium is this: throughout the essay we occasionally pause to ask you a few simple questions, testing you on the material just explained. In the days and weeks ahead we’ll re-test you in followup review sessions. By carefully expanding the testing schedule, we can ensure you consolidate the answers into your long-term memory, while minimizing the study time required. The review sessions take no more than a few minutes per day, and we’ll notify you when you need to review. The benefit is that instead of remembering how the quantum search algorithm works for a few hours or days, you’ll remember for years; it’ll become a much more deeply internalized part of your thinking. Of course, you can just read this as a conventional essay. But I hope you’ll at least try out the mnemonic medium. To do so please sign up below. This will enable us to track the best review schedule for each question, and to remind you to sign in for occasional short review sessions. And if you’d like to learn more about how the mnemonic medium works, please see A medium which makes memory a choice, How to approach this essay?, and How to use (or not use!) the questions. Please sign in so we can save your progress and let you know the best times to review. Thank you! Your progress will be saved as you read. As an example, let’s take a look at a couple of simple questions reviewing what you’ve just learned. Please indulge me by answering the questions just below. It’ll only take a few seconds – for both questions, think about what you believe the answer to be, click to reveal the actual answer, and then mark whether you remembered or not. If you can recall, that’s great. If not, that’s also fine, just mentally note the correct answer, and continue. Since you probably weren't expecting to be tested like this, it seems only fair to give you a hint for the second question: the somewhat hard-to-remember prefactor in the answer is π / 4 \\pi/4 π / 4 . Later in the essay I won't always provide such reminders, so you'll need to be paying attention! Didn’t remember Remembered The building blocks of the quantum search algorithm In the introduction I gave an informal description of what the quantum search algorithm achieves. To make the search algorithm more concrete, let’s think about the special case of using search to attack the traveling salesperson problem (TSP). Of course, there are better approaches to TSP than search, but the purpose of this section is to show the overall building blocks that go into the search algorithm. For that purpose, TSP is a useful concrete example. In the next section we’ll understand the details of how the buildings blocks work. It’ll help to consider a variation on TSP, namely, searching for a route shorter than some specified threshold distance, T T T . In other words, we’ll be using search to solve problems like: Here’s a list of cities – Hobbiton, Minas Tirith, Edoras, Bree, Dale, … – and the distances between them (which I won’t attempt to specify, but you can imagine!) Is there a route through all the cities that is less than 2,000 kilometers [or the equivalent in miles] in length? This isn’t quite the same as find-the-minimal-route, but this variation turns out to be a little easier to connect to the quantum search algorithm. Variation noted, here’s what a quantum search algorithm might look like: The search register contains candidate solutions ∣ x ⟩ = ∣ x 1 , x 2 , … , x n ⟩ |x\\rangle = |x_1, x_2, \\ldots, x_n\\rangle ∣ x ⟩ = ∣ x 1 ​ , x 2 ​ , … , x n ​ ⟩ to the search problem. In this case, our search register will contain potential routes through the cities, written out as bit strings x = x 1 , x 2 , … x = x_1, x_2, \\ldots x = x 1 ​ , x 2 ​ , … . I won’t get into the bit string representation explicitly – there are many ways to make such a representation, and the details don’t much matter. The key point is that you should think of the search register as being in some superposition ∑ x α x ∣ x ⟩ \\sum_x \\alpha_x |x\\rangle ∑ x ​ α x ​ ∣ x ⟩ of different possible routes through the cities, and x x x as being some bit string representation of a route. For definiteness, I’ll also assume the search register starts in the all ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state. That’s just a convention: we need to start somewhere. Step 1 of the quantum search algorithm will just be some fixed quantum circuit, made up of standard quantum gates – things like the Hadamard and CNOT gates, as discussed in the previous essay. Of course, eventually we need to figure out what those gates should be. We’ll do that in later sections. But for now we’re just sticking at a broad conceptual level, trying to figure out what a quantum search algorithm might look like. The next step is to check if the search register state ∣ x ⟩ |x\\rangle ∣ x ⟩ corresponds to what we’ll call a short route through the cities, i.e., a route of less distance than the threshold T T T . To do this, we introduce a check qubit to store the results of this step, initialized in the state ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ . So we start in the state ∣ x ⟩ ∣ 0 ⟩ |x\\rangle|0\\rangle ∣ x ⟩ ∣ 0 ⟩ , and change to ∣ x ⟩ ∣ 1 ⟩ |x\\rangle|1\\rangle ∣ x ⟩ ∣ 1 ⟩ if x x x represents a short route through the cities, and otherwise are left as ∣ x ⟩ ∣ 0 ⟩ |x\\rangle|0\\rangle ∣ x ⟩ ∣ 0 ⟩ , when x x x doesn’t represent a short route. We can write this compactly as ∣ x ⟩ ∣ 0 ⟩ → ∣ x ⟩ ∣ s ( x ) ⟩ |x\\rangle|0\\rangle \\rightarrow |x\\rangle|s(x)\\rangle ∣ x ⟩ ∣ 0 ⟩ → ∣ x ⟩ ∣ s ( x ) ⟩ , where the search function s ( x ) s(x) s ( x ) is equal to 1 1 1 if x x x is a solution to the problem (i.e., a route of length less than T T T ), and 0 0 0 if x x x is not a solution. Of course, in general the search register is in a superposition ∑ x α x ∣ x ⟩ \\sum_x \\alpha_x |x\\rangle ∑ x ​ α x ​ ∣ x ⟩ . We’ll assume (and justify later) that this checking-if-short-route step acts linearly, taking ∑ x α x ∣ x ⟩ ∣ 0 ⟩ \\sum_x \\alpha_x |x\\rangle |0\\rangle ∑ x ​ α x ​ ∣ x ⟩ ∣ 0 ⟩ to ∑ x α x ∣ x ⟩ ∣ s ( x ) ⟩ \\sum_x \\alpha_x |x\\rangle |s(x)\\rangle ∑ x ​ α x ​ ∣ x ⟩ ∣ s ( x ) ⟩ . How is this checking-if-short-route step implemented? Of course, in principle it’s easy to construct a conventional classical circuit which does the trick – the circuit would just check that the bit string x = x 1 x 2 … x = x_1 x_2 \\ldots x = x 1 ​ x 2 ​ … is a valid route through all the cities, and if so would add up the corresponding distances, and compare it to the threshold T T T . We can just take that classical circuit – whatever it is – and translate it into the equivalent quantum circuit. I explained how to do such translations using Toffoli and NOT gates in the earlier essay, and I won’t re-explain it here. Of course, we still need to figure out the exact details of the classical circuit, but: (a) that’s part of classical computing, not quantum computing; and (b) in any case is a detail unrelated to making search work. With one slight caveat (to be discussed shortly), we’ll take for granted we have a quantum circuit which can do the job. After that is Step 2 in the quantum search algorithm. Again, we need to figure out exactly what quantum gates to use here, and we’ll do that in the next section. Next, we check again if the search register state ∣ x ⟩ |x\\rangle ∣ x ⟩ is a short route. It works just as before, with a check qubit and so on. We continue in this way, alternating steps in our search algorithm with checking whether or not the search register state is a solution to our search problem, i.e., a short route through the cities. At the end of the algorithm we measure the search register. If we’ve designed the search algorithm well, then the result of the measurement will be a solution s s s to the search problem, in this case a route through the cities of distance less than T T T . A challenge is that sometimes such a solution may not exist. In our example, that’ll happen when there is no route through the cities of distance less than T T T . In that case, whatever measurement result we get at the end of the search algorithm, it won’t be a solution to the search problem. That’s okay, though, since it’s easy to just check and make sure we’ve got a legitimate solution. I’ve been talking about the problem of searching for short routes in TSP. But there’s little here that has to do with the details of TSP. We can imagine a general quantum search algorithm which works along the same lines: Everything is the same, except that we’ve replaced the check-if-short-route step by C s C_s C s ​ . We can think of this as a subroutine or black box which checks whether or not the search register is a solution s s s to the search problem. In particular, we’ll assume that C s C_s C s ​ takes ∑ x α x ∣ x ⟩ ∣ 0 ⟩ \\sum_x \\alpha_x |x\\rangle|0\\rangle ∑ x ​ α x ​ ∣ x ⟩ ∣ 0 ⟩ to ∑ x α x ∣ x ⟩ ∣ s ( x ) ⟩ \\sum_x \\alpha_x |x\\rangle|s(x)\\rangle ∑ x ​ α x ​ ∣ x ⟩ ∣ s ( x ) ⟩ , where (to recap) the search function s ( x ) = 0 s(x) = 0 s ( x ) = 0 when x x x is not a solution to the search problem, and s ( x ) = 1 s(x) = 1 s ( x ) = 1 when x x x is a solution to the search problem. More informally, we can think of C s C_s C s ​ as examining the search space to see if the search register contains a solution to the search problem. The hope motivating the quantum search algorithm is that we can reduce the number of times we need to do such examinations. In particular, we’ll try to minimize the number of times the search black box C s C_s C s ​ needs to be applied. As another example, suppose the search problem is the one I opened the essay with – searching for a key to decode a kidnapper’s note. In that case, you’d design C s C_s C s ​ so it does two things: (1) decodes the kidnapper’s note, assuming the search register contains a possible key; and (2) examines the decoded text from step 1 to see whether or not it’s plausibly a message in English. If it is a plausibly an English message then almost certainly it’s the correct text, since for most ciphers decodings for anything other than the correct key will look like gibberish. All of this is easily done using classical circuits, and those classical circuits can then be converted into a suitable quantum circuit for C s C_s C s ​ . As still another example, consider the protein folding problem – the problem of figuring out what shapes proteins take on in nature. A way of phrasing this in our framework is as a search for a way of spatially arranging the protein’s amino acids so the protein’s energy is below some threshold energy, E E E ? If you can answer this question reliably, then by gradually lowering the threshold E E E you can find the lowest-energy states for the protein. These lowest-energy states correspond to the shapes we find in nature. Again, it’s easy to figure out a circuit C s C_s C s ​ which checks whether or not some potential spatial arrangement of the amino acids has energy less than E E E . For the purpose of designing the quantum search algorithm we’re not going to worry about how the search black box C s C_s C s ​ works. We’ll just assume you’ve got access to a suitable C s C_s C s ​ . Indeed, much of the utility of the quantum search algorithm comes from the fact that it works with any C s C_s C s ​ . Of course, to actually implement the quantum search algorithm in practice we’d need to have an actual implementation of a suitable C s C_s C s ​ . But to design a useful quantum search algorithm, we can treat C s C_s C s ​ as a black box. So our main job through the remainder of this essay is to figure out how to design the quantum circuits for step 1, step 2, and so on, in order to minimize the total number of times we need to apply the search black box. We’ll design those quantum circuits in the next section. Incidentally, people new to the quantum search algorithms sometimes get a little hung up because of the slightly mysterious-sounding term “black box”. They worry that it implies there’s some sort of sleight-of-hand or magic going on, that quantum search must require some sort of genie wandering around giving out black boxes. Of course, it’s not magical at all. To repeat what I said above: if you were actually running the search algorithm, you’d need an implementation of the black box for your particular problem. But the point is to design a search algorithm which works no matter the internal details of the search black box – it abstracts those away. Another common misconception is that to implement the search black box C s C_s C s ​ we would need to know the value of s s s in advance. That’s not necessary because there’s a big difference between a circuit which can recognize a solution and which knows the solution. All the search black box needs is to be able to recognize a solution. For instance, it’s obviously possible to design a circuit which can recognize a short tour through a list of cities, without explicitly knowing a short tour in advance. Similarly for recognizing low-energy protein shapes, recognizing a decoded kidnapper’s note, and so on. Having spent so much time saying that we’re not going to worry about the details of C s C_s C s ​ I’ll now turn around and say that it simplifies things a little if we make one extra assumption about the search black box: we’ll suppose there is exactly one solution s s s to the search problem. This assumption is ultimately not essential – the search algorithm can be extended to the case of multiple (or zero) solutions. But for now it simplifies life to assume there’s exactly one single solution, which we’ll label s s s . That, by the way, is why I labeled the black box C s C_s C s ​ . (Incidentally, the search black box C s C_s C s ​ is sometimes called a search oracle, since it’s this oracular thing which tells us whether we have a solution to the search problem or not. I use the term black box in this essay, but many people use the term “oracle”, and it’s worth being aware of both terms.) Didn’t remember Remembered Getting a clean black box: Earlier, I blithely asserted you can take a classical circuit for computing the search function s ( x ) s(x) s ( x ) , and turn it into a quantum circuit which has the effect C s ∣ x ⟩ ∣ 0 ⟩ = ∣ x ⟩ ∣ s ( x ) ⟩ C_s|x\\rangle|0\\rangle = |x\\rangle|s(x)\\rangle C s ​ ∣ x ⟩ ∣ 0 ⟩ = ∣ x ⟩ ∣ s ( x ) ⟩ . Actually, there’s a slight complication. To illustrate the issue concretely, suppose you’re trying to compute s ( x ) = x 1 ∧ x 2 ∧ x 3 s(x) = x_1 \\wedge x_2 \\wedge x_3 s ( x ) = x 1 ​ ∧ x 2 ​ ∧ x 3 ​ , that is, the AND of three bits (corresponding to a search solution s = 111 s = 111 s = 1 1 1 , in binary). To do this, we’d start by using a Toffoli gate to compute the AND of the first two bits, x 1 ∧ x 2 x_1 \\wedge x_2 x 1 ​ ∧ x 2 ​ : Then we’d use another Toffoli gate to AND the result with x 3 x_3 x 3 ​ : So we’ve indeed computed s ( x ) = x 1 ∧ x 2 ∧ x 3 s(x) = x_1 \\wedge x_2 \\wedge x_3 s ( x ) = x 1 ​ ∧ x 2 ​ ∧ x 3 ​ , but along the way we’ve also generated an intermediate working qubit in the state ∣ x 1 ∧ x 2 ⟩ |x_1 \\wedge x_2\\rangle ∣ x 1 ​ ∧ x 2 ​ ⟩ . That working state wasn’t part of our original specification. Put another way, we wanted to compute ∣ x 1 , x 2 , x 3 ⟩ ∣ 0 ⟩ → ∣ x 1 , x 2 , x 3 ⟩ ∣ x 1 ∧ x 2 ∧ x 3 ⟩ , |x_1, x_2, x_3\\rangle|0\\rangle \\rightarrow |x_1, x_2, x_3\\rangle|x_1 \\wedge x_2 \\wedge x_3\\rangle, ∣ x 1 ​ , x 2 ​ , x 3 ​ ⟩ ∣ 0 ⟩ → ∣ x 1 ​ , x 2 ​ , x 3 ​ ⟩ ∣ x 1 ​ ∧ x 2 ​ ∧ x 3 ​ ⟩ , and instead we ended up computing ∣ x 1 , x 2 , x 3 ⟩ ∣ 0 ⟩ ∣ 0 ⟩ → ∣ x 1 , x 2 , x 3 ⟩ ∣ x 1 ∧ x 2 ⟩ ∣ x 1 ∧ x 2 ∧ x 3 ⟩ . |x_1, x_2, x_3\\rangle|0\\rangle|0\\rangle \\rightarrow |x_1, x_2, x_3\\rangle|x_1\\wedge x_2\\rangle |x_1 \\wedge x_2 \\wedge x_3\\rangle. ∣ x 1 ​ , x 2 ​ , x 3 ​ ⟩ ∣ 0 ⟩ ∣ 0 ⟩ → ∣ x 1 ​ , x 2 ​ , x 3 ​ ⟩ ∣ x 1 ​ ∧ x 2 ​ ⟩ ∣ x 1 ​ ∧ x 2 ​ ∧ x 3 ​ ⟩ . More generally, suppose we try to convert a classical circuit computing the search function s ( x ) s(x) s ( x ) into a quantum circuit. If we do it using the recipe described in the last essay – converting AND gates to Toffoli gates, and classical NOT gates to quantum NOT gates – it won’t take ∣ x ⟩ ∣ 0 ⟩ |x\\rangle|0\\rangle ∣ x ⟩ ∣ 0 ⟩ to ∣ x ⟩ ∣ s ( x ) ⟩ |x\\rangle|s(x)\\rangle ∣ x ⟩ ∣ s ( x ) ⟩ . There will be extra qubits involved, arising as intermediaries during the computation. The result will be something more like ∣ x ⟩ ∣ 0 ⟩ ∣ 0 ⟩ → ∣ x ⟩ ∣ s ( x ) ⟩ ∣ w ( x ) ⟩ , |x\\rangle|0\\rangle|0\\rangle \\rightarrow |x\\rangle|s(x)\\rangle |w(x)\\rangle, ∣ x ⟩ ∣ 0 ⟩ ∣ 0 ⟩ → ∣ x ⟩ ∣ s ( x ) ⟩ ∣ w ( x ) ⟩ , where the extra register is a supply of one-or-more working qubits, and they end up in some state ∣ w ( x ) ⟩ |w(x)\\rangle ∣ w ( x ) ⟩ produced along the way. The difference might seem small. We’re certainly close to having our search black box. But it turns out to be crucial to the quantum search algorithm that we get that clean behavior, ∣ x ⟩ ∣ 0 ⟩ → ∣ x ⟩ ∣ s ( x ) ⟩ |x\\rangle|0\\rangle \\rightarrow |x\\rangle|s(x)\\rangle ∣ x ⟩ ∣ 0 ⟩ → ∣ x ⟩ ∣ s ( x ) ⟩ . We’ll discuss later why this clean form for the computation is needed. For right now, though, let’s figure out how to do it. Fortunately, there’s a simple trick called uncomputation which works. It involves three steps. The first is more or less what you’d expect, but the second and third are quite clever: Compute ∣ x ⟩ ∣ 0 ⟩ ∣ 0 ⟩ → ∣ x ⟩ ∣ s ( x ) ⟩ ∣ w ( x ) ⟩ |x\\rangle |0\\rangle |0\\rangle \\rightarrow |x\\rangle|s(x)\\rangle |w(x)\\rangle ∣ x ⟩ ∣ 0 ⟩ ∣ 0 ⟩ → ∣ x ⟩ ∣ s ( x ) ⟩ ∣ w ( x ) ⟩ , using the standard approach of converting classical AND gates to Toffoli gates, and classical NOT gates to quantum NOT gates. Add on an extra qubit in the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state, and do a CNOT with ∣ s ( x ) ⟩ |s(x)\\rangle ∣ s ( x ) ⟩ as the control. This effectively copies the result, and we obtain: ∣ x ⟩ ∣ s ( x ) ⟩ ∣ w ( x ) ⟩ ∣ s ( x ) ⟩ |x\\rangle|s(x)\\rangle|w(x)\\rangle|s(x)\\rangle ∣ x ⟩ ∣ s ( x ) ⟩ ∣ w ( x ) ⟩ ∣ s ( x ) ⟩ . Now apply all the gates from step 1, but in reverse order, and applying at each step the inverse gate. The result is to undo or uncompute what happened in step 1, resulting in ∣ x ⟩ ∣ 0 ⟩ ∣ 0 ⟩ ∣ s ( x ) ⟩ |x\\rangle|0\\rangle|0\\rangle|s(x)\\rangle ∣ x ⟩ ∣ 0 ⟩ ∣ 0 ⟩ ∣ s ( x ) ⟩ . At the end, we can ignore the ∣ 0 ⟩ ∣ 0 ⟩ |0\\rangle|0\\rangle ∣ 0 ⟩ ∣ 0 ⟩ state, which isn’t changed at all by the entire process. And so the net result of these steps is the desired transformation, ∣ x ⟩ ∣ 0 ⟩ → ∣ x ⟩ ∣ s ( x ) ⟩ |x\\rangle|0\\rangle \\rightarrow |x\\rangle|s(x)\\rangle ∣ x ⟩ ∣ 0 ⟩ → ∣ x ⟩ ∣ s ( x ) ⟩ . Summing up, if we have a classical circuit to compute a function s ( ⋅ ) s(\\cdot) s ( ⋅ ) , you can think of the three stages in the corresponding clean quantum circuit as: compute s ( ⋅ ) s(\\cdot) s ( ⋅ ) , by converting classical gates to quantum; copy the answer using a CNOT; uncompute, by reversing the gates and inverting them. So, for instance, it’s easy to convert a computation of s ( x ) = x 1 ∧ x 2 ∧ x 3 s(x) = x_1 \\wedge x_2 \\wedge x_3 s ( x ) = x 1 ​ ∧ x 2 ​ ∧ x 3 ​ into the clean form using uncomputation. We just literally follow the steps above, and remember that the inverse of a Toffoli gate is a Toffoli gate: I’ve written the results of the clean computation as ∣ x ⟩ ∣ 0 ⟩ → ∣ x ⟩ ∣ s ( x ) ⟩ |x\\rangle|0\\rangle \\rightarrow |x\\rangle|s(x)\\rangle ∣ x ⟩ ∣ 0 ⟩ → ∣ x ⟩ ∣ s ( x ) ⟩ . What would have happened if the second register had been in the state 1 1 1 (or, more generally, an unknown state z = 0 z = 0 z = 0 or 1 1 1 ), instead of 0 0 0 ? You can easily trace through the above steps to see that the net result is ∣ x ⟩ ∣ z ⟩ → ∣ x ⟩ ∣ z ⊕ s ( x ) ⟩ , |x\\rangle|z\\rangle \\rightarrow |x\\rangle|z \\oplus s(x) \\rangle, ∣ x ⟩ ∣ z ⟩ → ∣ x ⟩ ∣ z ⊕ s ( x ) ⟩ , where the addition is done modulo 2 2 2 . This type of clean computation turns out to be useful in many quantum computations, not just quantum search, and the form just shown is the standard form in which it is presented. In particular, we can do a clean computation of any function f ( x ) f(x) f ( x ) for which we have a classical circuit, not just search functions. In any case, we will assume that the form given in the equation just above is the effect of the search black box C s C_s C s ​ . It’s worth noting that there is a price to pay in converting a classical circuit to its equivalent clean form: the uncomputation step doubles the number of gates required, and the copying step adds an extra CNOT on top of that doubling. So there is a genuine overhead in getting to the clean form. Still, for the speedup we’ll get from the quantum search algorithm this is a tiny price to pay. Exercise: Find a quantum circuit which computes ∣ x 1 , x 2 ⟩ ∣ 0 ⟩ → ∣ x 1 , x 2 ⟩ ∣ x 1 ∨ x 2 ⟩ |x_1, x_2\\rangle|0\\rangle \\rightarrow |x_1, x_2\\rangle |x_1 \\vee x_2\\rangle ∣ x 1 ​ , x 2 ​ ⟩ ∣ 0 ⟩ → ∣ x 1 ​ , x 2 ​ ⟩ ∣ x 1 ​ ∨ x 2 ​ ⟩ , where ∨ \\vee ∨ denotes the logical OR. Exercise: Find a quantum circuit which performs a clean computation of the classical function s ( x 1 , x 2 , x 3 ) = x 1 ∨ x 2 ∨ x 3 s(x_1, x_2, x_3) = x_1 \\vee x_2 \\vee x_3 s ( x 1 ​ , x 2 ​ , x 3 ​ ) = x 1 ​ ∨ x 2 ​ ∨ x 3 ​ . Let me finish the discussion of clean computation by introducing some extra pieces of quantum circuit notation that will come in handy later. The notation I’ll introduce generalizes the CNOT and Toffoli gates to involve more control qubits. For instance, here’s an example involving three control qubits: It behaves as you’d expect, NOTting the target qubit when all three control qubits are set, and otherwise leaving it alone. We just saw how to implement this using Toffoli gates and uncomputation: If we want to break this down even further, we can use techniques from the last essay to break the Toffoli gates into one- and two-qubit quantum gates. Very similar ideas can be used to synthesize even more complicated controlled gates, e.g. gates controlled by four qubits such as: In this notation, an open circle on a control qubit means gates are applied conditional on those control qubits being set to 0 0 0 . In this case, it means the NOT on the target qubit is applied conditional on the first two qubits being set to 0 0 0 and the third and fourth being set to 1 1 1 . I’ll leave it to you to figure out the details of how to break this down into Toffoli and other standard quantum gates – it’s a good exercise in applying the ideas we’ve been learning. Exercise: Find a way of breaking the controlled gate shown just above (with four control qubits) down into Toffoli and one- and two-qubit quantum gates. Didn’t remember Remembered Database search? The quantum search algorithm is sometimes described as a database search algorithm. This is often done in popular media accounts, and sometimes even in research papers. Unfortunately, it’s not a terribly helpful way of thinking about it. For one thing, databases are usually ordered, and that ordering makes them extremely fast to search. For instance, suppose you have an alphabetically ordered list of surnames: Calder Davies Jones Ng Prothero Richards… To find out if a name is on the list you wouldn’t run through the entire list. Rather, you’d exploit the ordering to do some kind of binary search. The result is that instead of needing to examine the database N N N times, you only need to examine it on the order of log ⁡ 2 ( N ) \\log_2(N) lo g 2 ​ ( N ) times. That’s vastly faster than the order N \\sqrt{N} N ​ times required by the quantum search algorithm. If someone needs to examine a database N N N times in order to search it, it probably means they need to think harder about how they’re indexing their database. Why is the notion of a quantum database search used so often in explanations? My guess is that it’s because searching a database is the most obvious really concrete way of thinking about search. But it’s that very concreteness which makes it easy to build database indices, which usually make database search a trivial problem in practice. Search is vastly more challenging when it’s hard to find or exploit any structure in the search space, in problems like decoding a code or the TSP or protein folding. It’s in such cases that the quantum search algorithm will shine. More precisely: the quantum search algorithm is useful when: (a) you’re doing a search where there’s little exploitable structure in the search space; but (b) you have an algorithm which lets you recognize solutions to the search problem, and so you can build the search black box. Didn’t remember Remembered Details of the quantum search algorithm Now that we have an overall picture, what quantum circuits actually make the quantum search algorithm work? Rather than simply present the final algorithm, I’m going to describe a line of thinking you might imagine using to discover the quantum search algorithm. That means we’ll be making guesses, and occasionally backtracking as we realize something doesn’t work. It has the disadvantage that the presentation is longer than if I just showed you the final algorithm. But it also makes it easier to understand where the quantum search algorithm comes from, and why it works. It’s often surprisingly instructive to see reasonable ideas tried (and fail), and how it’s possible to learn from those failures. Now, we’re looking for a truly quantum algorithm, one that exploits quantum mechanics to operate faster than a classical computer. So even though we start in a computational basis state, we should quickly move out of that state. After all, if we stayed in the computational basis we could do everything on a classical computer, and there would be no possibility of an advantage for a quantum computer. What state might we move into? In our circuit model, one of the gates that produces non-classical states is the Hadamard gate. Remember that the Hadamard gate takes the state ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ to ∣ 0 ⟩ + ∣ 1 ⟩ 2 \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ to ∣ 0 ⟩ − ∣ 1 ⟩ 2 \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ . A nice thing about the state ∣ 0 ⟩ + ∣ 1 ⟩ 2 \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ is that it’s a truly quantum state which is as agnostic as it’s possible to be about the value of the bit. Suppose we applied a Hadamard gate to all the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ ’ s at the start: Then we’d end up with the quantum state ∑ x 1 , x 2 , … , x n = 0 , 1 ∣ x 1 , x 2 , … , x n ⟩ 2 n , \\frac{\\sum_{x_1, x_2, \\ldots, x_n = 0, 1} |x_1, x_2, \\ldots, x_n\\rangle}{\\sqrt 2^n}, 2 ​ n ∑ x 1 ​ , x 2 ​ , … , x n ​ = 0 , 1 ​ ∣ x 1 ​ , x 2 ​ , … , x n ​ ⟩ ​ , where we sum over both 0 0 0 and 1 1 1 for each qubit. Put another way, we end up with an equal superposition of all possible solutions to the search problem. It’s a starting state that’s completely agnostic about the solution. We can write this more compactly by setting N : = 2 n N := 2^n N : = 2 n to be the size of the search space, and writing the last state as an equal superposition over all possible search solutions, ∑ x ∣ x ⟩ N . \\frac{\\sum_x |x\\rangle}{\\sqrt{N}}. N ​ ∑ x ​ ∣ x ⟩ ​ . This state will appear often in what follows, and it’s helpful to have some notation for it: we’ll call it ∣ E ⟩ |E\\rangle ∣ E ⟩ , for e qual superposition of possible solutions, ∣ E ⟩ : = ∑ x ∣ x ⟩ / N |E\\rangle := \\sum_x |x\\rangle/\\sqrt{N} ∣ E ⟩ : = ∑ x ​ ∣ x ⟩ / N ​ . Of course, this is just a guess as to how we might start out. In fact – spoiler alert! – we’ll eventually find that starting in pretty much any superposition state works. But the equal superposition ∣ E ⟩ |E\\rangle ∣ E ⟩ is easy to prepare, and turns out to work well. It’s got the additional bonus that this state turns up in lots of quantum algorithms, so it’s good to get comfortable with it. By the way, I said above that N = 2 n N = 2^n N = 2 n is the size of the search space. This isn’t always literally true. For instance, if we’re using x = x 1 x 2 … x = x_1 x_2 \\ldots x = x 1 ​ x 2 ​ … to describe routes in the traveling salesperson problem, it might be that some bit strings don’t represent valid routes, so the actual size of the search space may be smaller than 2 n 2^n 2 n . I won’t consider that possibility in any detail, although the algorithm we’ll find is easily modified to cope with that possibility. Now, suppose we introduce a check qubit and apply the search black box to our equal superposition state. We get the state: ∑ x ∣ x ⟩ ∣ s ( x ) ⟩ N . \\sum_x \\frac{|x\\rangle|s(x)\\rangle}{\\sqrt{N}}. x ∑ ​ N ​ ∣ x ⟩ ∣ s ( x ) ⟩ ​ . That doesn’t immediately help much: if we were to do a measurement in the computational basis, we get a result x , s ( x ) x, s(x) x , s ( x ) where s ( x ) = 1 s(x) = 1 s ( x ) = 1 (i.e., the solution to the search problem) with probability 1 / N 1/N 1 / N . We’re essentially just guessing a solution. What could we do instead if not a measurement? The most obvious thing is to apply the search black box again. Unfortunately, this adds s ( x ) s(x) s ( x ) to itself, modulo 2 2 2 , and so we end up in the state: ∑ x ∣ x ⟩ ∣ 0 ⟩ N . \\sum_x \\frac{|x\\rangle|0\\rangle}{\\sqrt N}. x ∑ ​ N ​ ∣ x ⟩ ∣ 0 ⟩ ​ . This isn’t progress – we’re back where we were earlier! Another thing to try is using a new check qubit. The simplest thing would be to apply the search black box over and over, each time with a new check qubit, so you end up with the state: ∑ x ∣ x ⟩ ∣ s ( x ) ⟩ ∣ s ( x ) ⟩ … N . \\sum_x \\frac{|x\\rangle|s(x)\\rangle|s(x)\\rangle \\ldots}{\\sqrt N}. x ∑ ​ N ​ ∣ x ⟩ ∣ s ( x ) ⟩ ∣ s ( x ) ⟩ … ​ . Again, this doesn’t seem all that promising. If you measured in the computational basis you’d again get a solution with probability 1 / N 1/N 1 / N , which is too low to be useful. What we want is to somehow increase the amplitudes in the terms with a 1 1 1 in the check qubit, and decrease the amplitudes when there is a 0 0 0 in the check qubit, a way of concentrating the amplitude in the right place. Imagine, for instance, we could do the following: if the check bit is 0 0 0 , then shrink the amplitude of the term by a factor 2 2 2 . And if the check bit is 1 1 1 , then double the amplitude by a factor 2 2 2 . Actually, that can’t work – the state would quickly become unnormalized. But maybe something like this could work, shrinking the “bad” amplitudes and growing the ”good” amplitudes, balancing things so state normalization is preserved. Unfortunately, this isn’t possible either, at least not directly! The trouble is that quantum gates are linear. That means they don’t directly “see” the amplitudes at all. For instance, for any gate described by a unitary matrix U U U and superposition of states, U ( α ∣ ψ ⟩ + β ∣ ϕ ⟩ ) = α U ∣ ψ ⟩ + β U ∣ ϕ ⟩ . U (\\alpha|\\psi\\rangle+\\beta|\\phi\\rangle) = \\alpha U|\\psi\\rangle + \\beta U|\\phi\\rangle. U ( α ∣ ψ ⟩ + β ∣ ϕ ⟩ ) = α U ∣ ψ ⟩ + β U ∣ ϕ ⟩ . That is, the gate doesn’t directly respond to the values of the amplitudes α \\alpha α and β \\beta β at all, and so there’s no shrinking or growing of amplitudes. Didn’t remember Remembered Well, we haven't made much progress! Since we haven’t gotten very far with algebra, let’s instead try to visualize what we’re hoping for geometrically. We can think of ourselves as starting out in a state ∣ E ⟩ |E\\rangle ∣ E ⟩ , and somehow trying to swing around to the solution ∣ s ⟩ |s\\rangle ∣ s ⟩ , perhaps passing through some intermediate states ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ along the way: Of course, if only we knew the identity s s s of the search solution, we could simply swing around directly. Indeed, we could solve the problem in just a single step! But we don’t know ∣ s ⟩ |s\\rangle ∣ s ⟩ . Instead, we’re hoping to use the search black box to somehow move closer. I want to draw your attention to one particular feature of the above diagram. I’ve shown ∣ E ⟩ |E\\rangle ∣ E ⟩ and ∣ s ⟩ |s\\rangle ∣ s ⟩ as being nearly orthogonal. That’s actually a pretty accurate representation of reality, since no matter what the value of ∣ s ⟩ |s\\rangle ∣ s ⟩ , its amplitude in the equal superposition ∣ E ⟩ |E\\rangle ∣ E ⟩ is 1 / N 1/\\sqrt{N} 1 / N ​ . It’ll be useful later to have a name for the corresponding angle, so let me draw it here: In particular, observe that the component of ∣ E ⟩ |E\\rangle ∣ E ⟩ in the ∣ s ⟩ |s\\rangle ∣ s ⟩ direction is just sin ⁡ ( Δ ) = 1 / N \\sin(\\Delta) = 1/\\sqrt{N} sin ( Δ ) = 1 / N ​ , and so Δ = arcsin ⁡ ( 1 / N ) ≈ 1 / N \\Delta = \\arcsin(1/\\sqrt{N}) \\approx 1/\\sqrt{N} Δ = arcsin ( 1 / N ​ ) ≈ 1 / N ​ . As an aside, I’ll be expressing all angles in radians, not degrees. So a right angle is π / 2 \\pi/2 π / 2 , a half rotation is π \\pi π , a full rotation is 2 π 2\\pi 2 π , and so on. I know some people prefer to think about angles in degrees, and using radians may frustrate them. On the other hand, if I worked in degrees, that’d be equally frustrating for people who prefer radians. Actually, it’d be more frustrating (and make the presentation more complex), because certain facts about trigonometry are simpler when angles are expressed in radians. An example, which I used in the last paragraph, is that arcsin ⁡ ( x ) ≈ x \\arcsin(x) \\approx x arcsin ( x ) ≈ x for small x x x . That becomes the much uglier arcsin ⁡ ( x ) ≈ 180 x / π \\arcsin(x) \\approx 180\\, x/\\pi arcsin ( x ) ≈ 1 8 0 x / π if we work in degrees. So it’s better just to work in radians. End of aside. At this point, I’m going to engage in some deus ex machina, and ask a question: what if we could somehow reflect about the solution vector ∣ s ⟩ |s\\rangle ∣ s ⟩ ? In fact, that turns out to be possible, and I’ll show you in a bit how to do it. For now though let’s just assume we can do it. Here’s what happens: In this diagram, θ \\theta θ is the angle between ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ and ∣ s ⟩ |s\\rangle ∣ s ⟩ , so 2 θ 2\\theta 2 θ is the total angle between ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ and its reflection. You may recall from elementary plane geometry that if we do two consecutive reflections of the plane about different axes, the net result is a rotation of the plane. That seems encouraging. The obvious other vector to try reflecting about is the equal superposition ∣ E ⟩ |E\\rangle ∣ E ⟩ . It seems plausible that if we could reflect about ∣ s ⟩ |s\\rangle ∣ s ⟩ then we could also figure out how to reflect about ∣ E ⟩ |E\\rangle ∣ E ⟩ . For now let’s just assume we can. The result is: We can see from the above diagram that we’ve rotated from the original ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ by an angle 2 θ + 2 ϕ 2\\theta + 2\\phi 2 θ + 2 ϕ , where ϕ \\phi ϕ is the angle between the equal superposition ∣ E ⟩ |E\\rangle ∣ E ⟩ and the vector ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ . Looking at the diagram, after the two reflections the quantum state is pointing in almost the opposite direction to where we started, i.e., it’s close to − ∣ ψ ⟩ -|\\psi\\rangle − ∣ ψ ⟩ , but with a slight extra rotation. To see why this is true, imagine you’re in a plane, and reflect a vector about two exactly orthogonal axes – say, the usual x x x and y y y axes. Of course, the result is just that the vector ends up pointing in the opposite direction. In this case, we’re not reflecting about exactly orthogonal axes, but rather about two almost-orthogonal axes. So we’d expect the net rotation to be approximately π \\pi π , but with a small deviation. What’s more, we’d expect that deviation to be related to the angle Δ \\Delta Δ by which the axes failed to be orthogonal. And that’s exactly right: we have Δ = π 2 − θ − ϕ \\Delta = \\frac{\\pi}{2}-\\theta-\\phi Δ = 2 π ​ − θ − ϕ , and so a little algebra shows that the rotation is 2 θ + 2 ϕ = π − 2 Δ 2\\theta + 2\\phi = \\pi-2\\Delta 2 θ + 2 ϕ = π − 2 Δ . This rotation of π − 2 Δ \\pi-2\\Delta π − 2 Δ is almost what we’re looking for. One thing that makes it a little hard to think about is the π \\pi π . In fact, a rotation by π \\pi π just flips a vector in the plane back and forth about the origin, effectively multiplying it by − 1 -1 − 1 . But in the previous essay we saw that such global phase factors make no difference whatsoever to outcomes at the end of a quantum computation. So after the double reflection it's exactly as though we're working with the state ∣ ψ ′ ⟩ |\\psi'\\rangle ∣ ψ ′ ⟩ shown below Ignoring such global phase factors sometimes bother people getting into quantum computing. If it bugs you, just insert a single-qubit gate − I -I − I on one of the qubits. : We can now see what’s going on very clearly: flipping about ∣ s ⟩ |s\\rangle ∣ s ⟩ and then ∣ E ⟩ |E\\rangle ∣ E ⟩ is the same as doing a rotation by 2 Δ 2\\Delta 2 Δ (up to the global phase factor, which can be ignored). Summing up the result in one diagram, and omitting the intermediate states we have: This is exciting news! It means we have a way of rotating from the starting state ∣ E ⟩ |E\\rangle ∣ E ⟩ an angle 2 Δ 2 \\Delta 2 Δ closer to the search solution ∣ s ⟩ |s\\rangle ∣ s ⟩ . What’s more, we can just keep repeating this operation. Maybe if we repeat it enough times we can rotate close to ∣ s ⟩ |s\\rangle ∣ s ⟩ ? How many times do we need to rotate to get close to ∣ s ⟩ |s\\rangle ∣ s ⟩ ? And how close can we get? Well, we’re rotating each time by 2 Δ 2\\Delta 2 Δ , and ideally we’d like to rotate by a total angle of π / 2 − Δ \\pi/2-\\Delta π / 2 − Δ . To get as close as possible to that total angle, the number of times we should rotate is just the integer closest to the ratio of the total angle π / 2 − Δ \\pi/2-\\Delta π / 2 − Δ with the angle of each rotation 2 Δ 2\\Delta 2 Δ , i.e.: round ( π 4 Δ − 1 / 2 ) \\text{round}\\left( \\frac{\\pi}{4 \\Delta} - 1/2 \\right) round ( 4 Δ π ​ − 1 / 2 ) When we do so, we end up within an angle Δ \\Delta Δ of ∣ s ⟩ |s\\rangle ∣ s ⟩ . Remember that Δ \\Delta Δ is small, so we’re very near the state ∣ s ⟩ |s\\rangle ∣ s ⟩ . It should be plausible that if you measure the quantum system you’ll get the result s s s with pretty high probability. We’ll figure out just how high that probability is shortly, but intuitively the overall picture is encouraging. The expression above for the number of times to do the rotation has many details in it, which makes it hard to think about. The key behavior to focus on is that the number of rotations required scales with 1 / Δ 1/\\Delta 1 / Δ . But we saw earlier that Δ ≈ 1 / N \\Delta \\approx 1/\\sqrt{N} Δ ≈ 1 / N ​ , so 1 / Δ 1/\\Delta 1 / Δ scales with N \\sqrt{N} N ​ . The result is that if you perform roughly π N / 4 \\pi \\sqrt{N}/4 π N ​ / 4 rotations, you’ll end up very near to the desired search solution. (By the way, I’ve used the phrase “roughly” there because to get Δ ≈ 1 / N \\Delta \\approx 1/\\sqrt{N} Δ ≈ 1 / N ​ we used the approximation arcsin ⁡ ( x ) ≈ x \\arcsin(x) \\approx x arcsin ( x ) ≈ x for small x x x . In fact, a bit of fiddling around with trigonometry and algebra shows that more than π N / 4 + 1 \\pi \\sqrt{N}/4 + 1 π N ​ / 4 + 1 rotations are never required. In practice, you’d use the exact formula with the arcsine in it. But that formula is a little complicated and somewhat opaque – the kind of thing you’d be unlikely to memorize, but would instead look up, unless for some reason you needed it often. On the other hand, π N / 4 \\pi \\sqrt{N}/4 π N ​ / 4 is a good shorthand, capturing the essential behavior, and worth remembering, along with the caveat that the actual expression is a little more complex.) That’s the essence of the quantum search algorithm! There are still details to be filled in, but the basic outline is as follows: Starting in the all- ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state, apply a Hadamard gate to each qubit to enter the equal superposition state ∣ E ⟩ = ∑ x ∣ x ⟩ N |E\\rangle = \\frac{\\sum_x |x\\rangle}{\\sqrt N} ∣ E ⟩ = N ​ ∑ x ​ ∣ x ⟩ ​ . Repeat the following steps, known as the Grover iteration, a number of times equal to: round ( π / 4 arcsin ⁡ ( 1 / N ) − 1 / 2 ) ≈ π N 4 \\text{round}(\\pi/4\\arcsin(1/\\sqrt{N})-1/2) \\approx \\pi\\frac{\\sqrt{N}}{4} round ( π / 4 arcsin ( 1 / N ​ ) − 1 / 2 ) ≈ π 4 N ​ ​ Reflect about the state ∣ s ⟩ |s\\rangle ∣ s ⟩ ; Reflect about the state ∣ E ⟩ |E\\rangle ∣ E ⟩ ; Measure to obtain the search solution s s s with high probability. When you consider the remarkable feat this algorithm accomplishes – searching an N N N - item search using ∼ N \\sim\\sqrt{N} ∼ N ​ examinations of that search space(!) – this is really quite simple and beautiful. The algorithm is due to Lov Grover, who introduced it in 1996, and it’s often called Grover’s quantum search algorithm in his honor. And, as mentioned above, the two steps at the core of the algorithm are sometimes called the Grover iteration. Before filling in the remaining details in the quantum search algorithm, let’s go through a few more spaced-repetition questions. These will help you remember many of the core elements of the algorithm. Note that a few details of the algorithm are still to be filled in, and we’ll discuss those in later sections. But we've got the core ideas now. Exercise: At several points in this essay I ask you to ignore global phase factors. If that makes you uncomfortable, I invite you to repeat the analysis at each place I’ve made the request, not ignoring global phase factors. Show that the states output from the computation only ever change by a factor − 1 -1 − 1 , raised to some power, and argue that measurement probabilities for the computation are not changed at all. Didn’t remember Remembered How to reflect about the ∣ s ⟩ |s\\rangle ∣ s ⟩ and ∣ E ⟩ |E\\rangle ∣ E ⟩ states? How should we achieve the desired reflections about the ∣ s ⟩ |s\\rangle ∣ s ⟩ and ∣ E ⟩ |E\\rangle ∣ E ⟩ states? To answer this question, we’ll start by focusing on the ∣ s ⟩ |s\\rangle ∣ s ⟩ state, since computational basis states are closer to our everyday way of thinking about the world. And to make it even more concrete, let’s focus on the all 0 0 0 state, ∣ 00 … 0 ⟩ |00\\ldots 0\\rangle ∣ 0 0 … 0 ⟩ . What would such a reflection actually do? It would mean leaving the ∣ 00 … 0 ⟩ |00\\ldots 0\\rangle ∣ 0 0 … 0 ⟩ state alone, and taking every other computational basis state ∣ x ⟩ |x\\rangle ∣ x ⟩ to − ∣ x ⟩ -|x\\rangle − ∣ x ⟩ . In terms of pseudocode, if the input state is ∣ x ⟩ |x\\rangle ∣ x ⟩ : if x == 00...0:\n  do nothing\nelse:\n  apply -1\nIt’s pretty easy to translate this into the quantum circuit model. You simply introduce an extra qubit that’s used as a sort of workspace for the if statement (this working qubit is often called an ancilla qubit – an unusual word in everyday speech, but easy to remember if you notice that it’s the word root for “ancillary”): This looks different to the pseudocode, but it’s really just the quantum circuit version of the pseudocode. The first controlled gate checks to see whether x x x is equal to 00 … 0 00\\ldots 0 0 0 … 0 , as in the if condition, flipping the ancilla qubit to 1 1 1 if so, and otherwise leaving it as 0 0 0 . The − Z -Z − Z gate on the ancilla then does exactly what we want, doing nothing if the ancilla is set to 1 1 1 (i.e., the if clause), and applying a factor − 1 -1 − 1 if the ancilla is set to 0 0 0 (the else clause). So the overall state is now ∣ x ⟩ ∣ 1 ⟩ |x\\rangle|1\\rangle ∣ x ⟩ ∣ 1 ⟩ when x x x is 00 … 0 00 \\ldots 0 0 0 … 0 , and − ∣ x ⟩ ∣ 0 ⟩ -|x\\rangle|0\\rangle − ∣ x ⟩ ∣ 0 ⟩ otherwise. We’re almost done. The final controlled gate is there so we can clean up the ancilla qubit, and subsequently ignore it. To do that, we apply the same controlled gate again, resetting the ancilla to 0 0 0 , no matter what the initial computational basis state was. The result is the state ∣ x ⟩ ∣ 0 ⟩ |x\\rangle|0\\rangle ∣ x ⟩ ∣ 0 ⟩ when x x x is 00 … 0 00\\ldots 0 0 0 … 0 , and − ∣ x ⟩ ∣ 0 ⟩ -|x\\rangle|0\\rangle − ∣ x ⟩ ∣ 0 ⟩ when x x x is anything else. So no matter the value of x x x the circuit leaves the ancilla in the fixed state ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ , and that ancilla can be ignored through subsequent computations. Ignoring the ancilla, we see that the ∣ x ⟩ |x\\rangle ∣ x ⟩ register has been reflected about the ∣ 00 … ⟩ |00\\ldots\\rangle ∣ 0 0 … ⟩ state, just as we wanted. There’s a rough heuristic worth noting here, which is that you can often convert if-then style thinking into quantum circuits. You introduce an ancilla qubit to store the outcome of evaluating the if condition. And then depending on the state of the ancilla, you perform the appropriate state manipulation. Finally, when possible you reverse the initial computation, resetting the ancilla to its original state so you can subsequently ignore it. For the reflection about ∣ 00 … 0 ⟩ |00\\ldots 0\\rangle ∣ 0 0 … 0 ⟩ there’s a clever trick which can be used to simplify the circuit shown above. Instead of using an ancilla in the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state, start the ancilla in the ∣ 0 ⟩ − ∣ 1 ⟩ 2 \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ state (you do this using a NOT gate followed by a Hadamard gate on ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ ), and then use the following circuit: Why does this work? If x ≠ 00 … x \\neq 00\\ldots x  ​ = 0 0 … , then nothing happens, and we end up in the state ∣ x ⟩ ∣ 0 ⟩ − ∣ 1 ⟩ 2 |x\\rangle\\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} ∣ x ⟩ 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ . If x = 00 … x = 00\\ldots x = 0 0 … the ancilla qubit is NOTted, changing it from ∣ 0 ⟩ − ∣ 1 ⟩ 2 \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ to ∣ 1 ⟩ − ∣ 0 ⟩ 2 \\frac{|1\\rangle-|0\\rangle}{\\sqrt 2} 2 ​ ∣ 1 ⟩ − ∣ 0 ⟩ ​ , which is, of course, just − ∣ 0 ⟩ − ∣ 1 ⟩ 2 -\\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} − 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ . In both cases this is exactly what we wanted, except for a global phase factor of − 1 -1 − 1 , which we can ignore. Furthermore, no matter the value of x x x the circuit leaves the ancilla in the fixed state ∣ 0 ⟩ − ∣ 1 ⟩ 2 \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ , and so the ancilla can be ignored through subsequent computations. This is a nice trick, which I sometimes call the “phase trick”. I must admit, it seems a little like magic. It’s one of those things that’s easy to verify works, but it’s not so obvious how you would have discovered it in the first place. I don’t actually know the history of the trick (the earliest mention I know is in this 1997 paper), but here’s how you might have discovered it. Suppose you’d been working hard on the original circuit I showed, thinking about each element: I don’t necessarily mean you were trying to simplify the circuit, I just mean you were messing around trying to better understand how the circuit works. And then suppose in some other context someone mentioned to you (or you noticed) that X ∣ 0 ⟩ − ∣ 1 ⟩ 2 = − ∣ 0 ⟩ − ∣ 1 ⟩ 2 X\\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} = -\\frac{|0\\rangle-|1\\rangle}{\\sqrt 2} X 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ = − 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ . If you’d been sufficiently deep into thinking about the original circuit, a lightbulb might go on and you’d think “Hey, the NOT gate can be used to generate a factor − 1 -1 − 1 , without otherwise changing the state of the qubit its being applied to. That kind of factor is just what we needed in our reflections. I wonder if I can somehow use that in my original circuit?” Having made the connection you’d eventually figure the second circuit out, though it might have required a fair bit more work before you got the circuit just right. Didn’t remember Remembered Reflection about the ∣ s ⟩ |s\\rangle ∣ s ⟩ state: Having figured out how to do the reflection for the all 0 0 0 state, it’s easy to do it for the ∣ s ⟩ |s\\rangle ∣ s ⟩ state. We just use the search black box, in exactly the same style as the circuit just shown above: It works for exactly the same reasons as the earlier circuit: the search black box is effectively applying a NOT gate to the ancilla, conditional on x x x being equal to s s s . You’ll notice, by the way, that the phase trick buys us something nice here. If we’d used the original circuit, without the phase trick, we’d need two applications of the search black box to do the reflection about ∣ s ⟩ |s\\rangle ∣ s ⟩ . So the phase trick decreases the cost of the quantum search algorithm by a factor two, a nice win. Reflection about the equal superposition state, ∣ E ⟩ |E\\rangle ∣ E ⟩ : The first time I thought about how to do this, I got a little paralyzed, thinking in essence: “Ooh, the ∣ E ⟩ |E\\rangle ∣ E ⟩ state is strange and quantum, how could we possible reflect about it?” Actually, it’s straightforward: just move the ∣ E ⟩ |E\\rangle ∣ E ⟩ state to ∣ 00 … 0 ⟩ |00\\ldots 0\\rangle ∣ 0 0 … 0 ⟩ , reflect about ∣ 00 … 0 ⟩ |00\\ldots 0\\rangle ∣ 0 0 … 0 ⟩ , and then move the ∣ 00 … 0 ⟩ |00\\ldots 0\\rangle ∣ 0 0 … 0 ⟩ state back to ∣ E ⟩ |E\\rangle ∣ E ⟩ . Here’s a circuit which does it: This circuit works because the product of Hadamard gates both moves ∣ 00 … 0 ⟩ |00\\ldots 0\\rangle ∣ 0 0 … 0 ⟩ to ∣ E ⟩ |E\\rangle ∣ E ⟩ , as we saw earlier, and also moves ∣ E ⟩ |E\\rangle ∣ E ⟩ back to ∣ 00 … 0 ⟩ |00\\ldots 0\\rangle ∣ 0 0 … 0 ⟩ , since the Hadamard gate is its own inverse. I’m not sure what lesson to draw from my initial fear of this problem, and its actual ease of solution – perhaps that sometimes things sound scary because they’re unfamiliar, but in fact they’re simple. Exercise: Prove that the circuit shown above does, indeed, reflect about ∣ E ⟩ |E\\rangle ∣ E ⟩ . To do the proof, suppose the input to the circuit is α ∣ E ⟩ + β ∣ E ⊥ ⟩ \\alpha|E\\rangle + \\beta |E_\\perp\\rangle α ∣ E ⟩ + β ∣ E ⊥ ​ ⟩ , where ∣ E ⊥ ⟩ |E_\\perp\\rangle ∣ E ⊥ ​ ⟩ is some state orthogonal to ∣ E ⟩ |E\\rangle ∣ E ⟩ . Then argue that the effect of the circuit is to take this to − ( α ∣ E ⟩ − β ∣ E ⊥ ⟩ ) -(\\alpha|E\\rangle-\\beta |E_\\perp\\rangle) − ( α ∣ E ⟩ − β ∣ E ⊥ ​ ⟩ ) . Up to a global phase factor this is the desired reflection. Didn’t remember Remembered Measuring the output As we saw earlier, the quantum search algorithm doesn’t produce the state ∣ s ⟩ |s\\rangle ∣ s ⟩ exactly as output. Instead it produces a quantum state ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ which is within an angle Δ \\Delta Δ of ∣ s ⟩ |s\\rangle ∣ s ⟩ , as shown below (in this example ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ has slightly over-rotated past ∣ s ⟩ |s\\rangle ∣ s ⟩ ) : Now, the angle Δ \\Delta Δ is small (particularly for a large search space, i.e., large N N N , which is when we’re most interested in search), which means ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ must be very close to ∣ s ⟩ |s\\rangle ∣ s ⟩ . Intuitively, you’d expect a measurement in the computational basis would produce s s s with high probability. That intuition is correct. In particular, the probability a computational basis measurement gives the result s s s is just the square of the amplitude for ∣ s ⟩ |s\\rangle ∣ s ⟩ in ∣ ψ ⟩ |\\psi\\rangle ∣ ψ ⟩ . That’s at least equal to cos ⁡ 2 ( Δ ) = 1 − sin ⁡ 2 ( Δ ) \\cos^2(\\Delta) = 1-\\sin^2(\\Delta) cos 2 ( Δ ) = 1 − sin 2 ( Δ ) , which is just 1 − 1 / N 1-1/N 1 − 1 / N . Summing up: the probability that a computational basis state measurement gives the outcome s s s is at least 1 − 1 / N 1-1/N 1 − 1 / N . So, for instance, if your search space has N = 1 , 000 N = 1,000 N = 1 , 0 0 0 or more elements, then the probability the search algorithm will find the correct outcome s s s is at least 1 − 1 / 1000 1-1/1000 1 − 1 / 1 0 0 0 , i.e., at least 99.9 99.9 9 9 . 9 percent. It works even more reliably for larger search spaces. Now, even with this high probability you might still reasonably worry about what happens if the measurement gives the wrong outcome. Fortunately, it’s possible to quickly check whether that’s happened – whatever the measurement outcome is, we can use the search black box to check whether it’s a genuine solution or not. If it’s not, we simply rerun the algorithm. That, in turn, creates a worry that you’d need to rerun the algorithm many times. But for large N N N – the case we usually care about when searching! – that’s extremely unlikely. A little probability calculation shows that on average the number of times needed to run the algorithm is never more than 1 / ( 1 − 1 / N ) 1/(1-1/N) 1 / ( 1 − 1 / N ) , which is very close to 1 1 1 . Unsurprisingly, but pleasingly, our initial intuition was good: the quantum search algorithm produces the right answer, with high probability. Didn’t remember Remembered Summary of the quantum search algorithm Let’s sum up our completed understanding of the quantum search algorithm: Starting in the all- ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state, apply a Hadamard gate to each qubit to enter the equal superposition state ∣ E ⟩ = ∑ x ∣ x ⟩ N |E\\rangle = \\frac{\\sum_x |x\\rangle}{\\sqrt N} ∣ E ⟩ = N ​ ∑ x ​ ∣ x ⟩ ​ . Repeat the following Grover iteration a number of times equal to: round ( π / 4 arcsin ⁡ ( 1 / N ) − 1 / 2 ) ≈ π N 4 \\text{round}(\\pi/4\\arcsin(1/\\sqrt{N})-1/2) \\approx \\pi\\frac{\\sqrt{N}}{4} round ( π / 4 arcsin ( 1 / N ​ ) − 1 / 2 ) ≈ π 4 N ​ ​ Reflect about the state ∣ s ⟩ |s\\rangle ∣ s ⟩ , using the circuit: Reflect about the state ∣ E ⟩ |E\\rangle ∣ E ⟩ , using the circuit: Measure to obtain the search solution s s s with probability at least 1 − 1 / N 1-1/N 1 − 1 / N . Use the search black box to check whether the measurement outcome is truly a solution to the search problem. If it is, we’re done; if not, rerun the algorithm. That’s it, the complete quantum search algorithm! I’ve tried to explain quantum search using what I call discovery fiction, a mostly-plausible series of steps you could imagine having taken to discover it, complete with occasional wrong turns and backtracking. Despite my attempts to make it legible, I believe there’s still something almost shocking about the quantum search algorithm. It’s incredible that you need only examine an N N N - item search space on the order of N \\sqrt{N} N ​ times in order to find what you’re looking for. And, from a practical point of view, we so often use brute search algorithms that it’s exciting we can get this quadratic speedup. It seems almost like a free lunch. Of course, quantum computers still being theoretical, it’s not quite a free lunch – more like a multi-billion dollar, multi-decade lunch! Variations on the basic quantum search algorithm: I’ve explained the quantum search algorithm in its simplest form. There are many variations on these ideas. Especially useful variations include extending the algorithm so it can cope with the case of multiple solutions, and extending the algorithm so it can be used to estimate the number of solutions if that number isn’t known in advance. I won’t discuss these in any detail. But if you want a challenge, try attacking these problems yourself. A good starting point is to find a search algorithm for the case where there exactly 2 2 2 search solutions, say s 1 s_1 s 1 ​ and s 2 s_2 s 2 ​ . You already have most of the ideas needed, but it’s still an instructive challenge to figure it out. If you’re looking for much more detail about variations on the quantum search algorithm, you can find it in Chapter 6 of my book with Ike Chuang. What if we used a different starting state? We simply guessed that the state ∣ E ⟩ |E\\rangle ∣ E ⟩ was a good starting state. Imagine we’d started in a different quantum state, let’s call it ∣ ϕ ⟩ |\\phi\\rangle ∣ ϕ ⟩ . And then we repeatedly reflected about ∣ s ⟩ |s\\rangle ∣ s ⟩ and about ∣ ϕ ⟩ |\\phi\\rangle ∣ ϕ ⟩ . As before, the net result of such a double reflection is a rotation, with the angle equal to π \\pi π minus double the angle between ∣ ϕ ⟩ |\\phi\\rangle ∣ ϕ ⟩ and ∣ s ⟩ |s\\rangle ∣ s ⟩ . It’s fun to think about different things one can do with such a rotation. I won’t get into it here, except to quickly mention that this observation can be used to do a type of structured search. For instance, if we know some values x x x aren’t possible solutions to the search problem, we can actually speed the search algorithm up by making sure ∣ x ⟩ |x\\rangle ∣ x ⟩ doesn’t appear in the initial superposition ∣ ϕ ⟩ |\\phi\\rangle ∣ ϕ ⟩ . Can we improve the quantum search algorithm? A good question to ask is whether it’s possible to improve the quantum search algorithm? That is, is it possible to find a quantum algorithm which requires fewer applications of the search black box? Maybe, for instance, we could solve the search problem using N 3 \\sqrt[3]{N} 3 N ​ applications of the search black box. That would be a tremendously useful improvement. If we were truly optimistic we might even hope to solve the search problem using on the order of log ⁡ ( N ) \\log(N) lo g ( N ) applications of the search black box. If that were possible, it would be revolutionary. We could use the resulting quantum algorithm to very rapidly solve problems like the traveling salesperson problem, and other NP-complete optimization problems, problems such as protein folding, satisfiability, and other famous hard problems. It would be a silver bullet, a way in which quantum computers were vastly superior to classical. Unfortunately, it turns out that the quantum search algorithm as I’ve presented it is optimal. In particular, no search algorithm with faster than ∼ N \\sim\\sqrt{N} ∼ N ​ scaling is possible. This was proved in a remarkable 1997 paper. While this is a real pity, the quantum search algorithm still provides a more limited but bona fide silver bullet for speeding up a wide class of classical computations. Exercise: Suppose that instead of performing a measurement, we instead continue performing Grover iterations. Argue that the quantum state will continue to rotate, and that we would expect the amplitude for ∣ s ⟩ |s\\rangle ∣ s ⟩ to start to decrease. Exercise: Argue that if we continue performing Grover iterations, as in the last question, we’d eventually expect the measurement probability for s s s to be no more than 1 / N 1/N 1 / N . Didn’t remember Remembered What can we learn from the quantum search algorithm? Are there any general lessons about quantum computers we can learn from the quantum search algorithm? Although dozens or hundreds of quantum algorithms have been developed, most are for relatively specialized – indeed, often rather artificial – problems. Apart from simulating quantum systems, only a handful of inarguably extremely useful quantum algorithms are known (quantum search is one). In particular, there is as yet no good general-purpose recipe for saying when a problem can be fruitfully attacked using a quantum computer, or how. Quantum algorithm design is still bespoke. For this reason, we should beware any too-pat explanation of why the quantum search algorithm works. If an explanation is really good, it should enable us to find interesting new algorithms, not merely provide eagle-eyed hindsight. With that caveat in mind, I do want to make one observation. Below is an animation showing the amplitudes for all the computational basis states as the quantum search algorithm runs. In particular, it shows the amplitudes after each Grover iteration, starting at iteration 0 0 0 and running up to the final iteration – in this case, iteration number 7 7 7 : You can see that the effect of the Grover iteration is to take amplitude from non-solutions and gradually concentrate it in the solution. This is just what I said earlier couldn’t be done directly. That was true, in the sense that all the gates in our circuit are still acting linearly through the amplitudes. So what’s happening? Well, you may remember in the last essay I asked you to guess what would happen if you applied the Hadamard gate twice in a row to the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state. Intuitively, the Hadamard gate mixes the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ and ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ states, so you might guess the end result would be to thoroughly mix them up. Only that’s not what happens. Instead, here’s the two steps: ∣ 0 ⟩ → ∣ 0 ⟩ + ∣ 1 ⟩ 2 → ∣ 0 ⟩ + ∣ 1 ⟩ 2 + ∣ 0 ⟩ − ∣ 1 ⟩ 2 2 . |0\\rangle \\rightarrow \\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} \\rightarrow \\frac{\\frac{|0\\rangle+|1\\rangle}{\\sqrt 2} + \\frac{|0\\rangle-|1\\rangle}{\\sqrt 2}}{\\sqrt 2}. ∣ 0 ⟩ → 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ → 2 ​ 2 ​ ∣ 0 ⟩ + ∣ 1 ⟩ ​ + 2 ​ ∣ 0 ⟩ − ∣ 1 ⟩ ​ ​ . If you look closely at the final expression on the right-hand side you see that the opposite signs on the two ∣ 1 ⟩ |1\\rangle ∣ 1 ⟩ states are canceling each other out, a phenomenon called destructive interference. Meanwhile, the two ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ states add up, a phenomenon called constructive interference. The net result is to concentrate all the amplitude in the ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ state, and so the outcome is just ∣ 0 ⟩ |0\\rangle ∣ 0 ⟩ . A similar (though more complicated) type of interference and sign cancellation is going on during the Grover iteration. Suppose we start out the iteration as follows: We then reflect about the ∣ s ⟩ |s\\rangle ∣ s ⟩ state, which inverts all the non- s s s amplitudes: The next step is to reflect about the ∣ E ⟩ |E\\rangle ∣ E ⟩ state. The effect of this step is to achieve a cancellation similar (but more complicated) than was going on in the second stage of the double-Hadamard. In particular, this reflection reduces the ∣ s ⟩ |s\\rangle ∣ s ⟩ amplitude a little, redistributing it over all the other computational basis states. At the same time, it takes the superposition over all the other states and reduces it slightly, redistributing it to the ∣ s ⟩ |s\\rangle ∣ s ⟩ state. The net effect is to grow the ∣ s ⟩ |s\\rangle ∣ s ⟩ amplitude and shrink the others, albeit “upside down”: The − 1 -1 − 1 global phase factor simply inverts everything, and the total effect is to grow the ∣ s ⟩ |s\\rangle ∣ s ⟩ amplitude and shrink the others. In net, we’ve used the ∣ s ⟩ |s\\rangle ∣ s ⟩ amplitude to cancel out some of the other amplitudes (destructive interference), and the other amplitudes to reinforce some of the ∣ s ⟩ |s\\rangle ∣ s ⟩ amplitude (constructive interference). This explanation is, alas, somewhat vague. I wish I could write it in a clearer way, but I can’t because I don’t really understand it in a clearer way. There’s more I could say – other perspectives, other calculations we could do. Going through all that would help, but only a little. At the core is still a clever way of using the ∣ s ⟩ |s\\rangle ∣ s ⟩ - amplitude to cancel out non- ∣ s ⟩ |s\\rangle ∣ s ⟩ amplitudes, and to use non- ∣ s ⟩ |s\\rangle ∣ s ⟩ amplitudes to reinforce the ∣ s ⟩ |s\\rangle ∣ s ⟩ - amplitude. Quantum parallelism: One thing the quantum search algorithm has in common with many other quantum algorithms is the use of large superposition states. For instance, the equal superposition state ∣ E ⟩ = ∑ x ∣ x ⟩ / N |E\\rangle = \\sum_x |x\\rangle/\\sqrt{N} ∣ E ⟩ = ∑ x ​ ∣ x ⟩ / N ​ shows up in many quantum algorithms. It’s a pretty common pattern in those algorithms to then modify that state so each term ∣ x ⟩ |x\\rangle ∣ x ⟩ picks up some information relevant to the solution of the overall problem, and then to trying to arrange cancellation of terms. This pattern is often known as quantum parallelism. Upon first acquaintance, this seems much like a conventional classical computer running a randomized (i.e., Monte Carlo) algorithm. In particular, it’s a bit like trying a random solution, and then computing some information relevant to the overall problem you’re trying to solve. But what is very different is that in a classical computer there’s no way of getting cancellation between different possible solutions. The ability to get that kind of interference is crucial to quantum computing. Why we use clean computation: Earlier, I promised you an explanation of why we used clean computation. In fact, for the interference to work, it’s essential that no other qubits are changed by the computation. Suppose we had a sum involving working qubits in lots of different states, something like (omitting factors) ∑ x ∣ x ⟩ ∣ w ( x ) ⟩ \\sum_x |x\\rangle|w(x)\\rangle ∑ x ​ ∣ x ⟩ ∣ w ( x ) ⟩ , i.e., a non-clean computation. We couldn’t get any sort of cancellation (or reinforcement) between terms with different values of w ( x ) w(x) w ( x ) . Those other qubits would be storing information which prevented cancellation of terms. That’s why clean computation is helpful. This helps explain why clean computation is useful, but may leave you wondering how you could ever have invented clean computation in the first place? In fact, historically the uncomputation trick for clean computation was discovered for reasons having nothing to do with quantum algorithms or with interference. It was discovered by people who were trying to figure out how to make conventional classical computers more energy efficient. Those people had come up with an argument (which I won’t describe here) that working bits actually contributed to energy inefficiency. And so they discovered uncomputation as a way of minimizing that energy cost. It was only later that it was realized that clean computation was extremely useful in quantum computing, especially for getting interference effects. This is a pattern which often occurs in creative work far beyond physics: ideas which arise in one context are often later reused for completely different reasons in another context. I believe that if that prior work on energy-efficient computing hadn’t been done, it might have taken quite a bit more effort to come up with the quantum search algorithm. Concluding thoughts: You’ve now worked through the details of a powerful, widely-usable quantum algorithm. Other algorithms are known, some of which – notably, the quantum algorithm for factoring integers – offer even larger speedups over known classical algorithms. Still, if and when quantum computers are eventually built the quantum search algorithm is likely to be an extremely useful application. What’s more, many of the ideas used in the search algorithm are used in other quantum algorithms. Thanks for reading this far. In a few days, you’ll receive a notification containing a link to your first review session. In that review session you’ll be retested on the material you’ve learned, helping you further commit it to memory. It should only take a few minutes. In subsequent days you’ll receive more notifications linking you to re-review, gradually working toward genuine long-term memory of all the core material in the essay. Thanks for reading this far. If you’d like to remember the core ideas of this essay durably, please set up an account below. We’ll track your review schedule and send you occasional reminders containing links that will take you to the review experience. Please sign in so we can save your progress and let you know the best times to review. Thank you! Your progress will be saved as you read. Acknowledgments Michael Nielsen is supported by Y Combinator Research. Citing this work In academic work, please cite this as: Andy Matuschak and Michael A. Nielsen, “How does the quantum search algorithm work?”, https://quantum.country/search, San Francisco (2019). Authors are listed in alphabetical order. License This work is licensed under a Creative Commons Attribution-NonCommercial 3.0 Unported License. This means you’re free to copy, share, and build on this essay, but not to sell it. If you’re interested in commercial use, please contact us. Last updated April 16, 2019 See our User Agreement and Privacy Policy."
  },
  {
    "categories": [
      "Computer Science"
    ],
    "authors": [
      "Marc ten Bosch"
    ],
    "title": "Let's remove Quaternions from every 3D Engine",
    "link": "https://marctenbosch.com/quaternions/",
    "description": "An Interactive Introduction to Rotors from Geometric Algebra",
    "content": "Let's remove Quaternions from every 3D Engine (An Interactive Introduction to Rotors from Geometric Algebra) - Marc ten Bosch\n\nLet's remove Quaternions from every 3D Engine\n\n(An Interactive Introduction to Rotors from Geometric Algebra)\nMarc ten Bosch\n\nThe clearest explanation of 3D geometric algebra within 15 minutes that I've seen so far —BrokenSymmetry\n\nI am sold. While I can understand quaternions to an extent, this way of thinking is a much more intuitive and elegant approach. —Jack Rasksilver\n\nThis sets a high standard for educational material, and is a shining example of how we can improve education with today's technologies. —Sebastien Pierre\n\nWhen I was in college, I asked one of my math professors why the cross product of two vectors results in a perpendicular vector whose magnitude is equal to the area of the parallelogram formed by the two vectors. Like..what? Why? And what about 2D?\n\nThey blew me off, and that was a big part of why I stopped taking math in college. [...]\n\nAnyway, I had pretty much given up on ever truly understanding the whole jumble of seemingly unrelated types that are cross products. But then I saw this: And...wow. Just 15 minutes and a lot more than just cross products suddenly make a lot more sense. —Mason Remaley\n\nTo represent 3D rotations graphics programmers use Quaternions. However, Quaternions are taught at face value. We just accept their odd multiplication tables and other arcane definitions and use them as black boxes that rotate vectors in the ways we want. Why does $\\mathbf{i}^2=\\mathbf{j}^2=\\mathbf{k}^2=-1$ and $\\mathbf{i} \\mathbf{j} = \\mathbf{k}$? Why do we take a vector and upgrade it to an \"imaginary\" vector in order to transform it, like $\\mathbf{q} (x\\mathbf{i} + y\\mathbf{j} + z \\mathbf{k}) \\mathbf{q}^{*}$? Who cares as long as it rotates vectors the right way, right?\n\nPersonally, I have always found it important to actually understand the things I am using. I remember learning about Cross Products and Quaternions and being confused about why they worked this way, but nobody talked about it. Later on I learned about Geometric Algebra and suddenly I could see that the questions I had were legitimate, and everything became so much clearer.\n\nIn Geometric Algebra there is a way to represent rotations called a Rotor that generalizes Quaternions (in 3D) and Complex Numbers (in 2D) and even works in any number of dimensions.\n\n3D Rotors are in a sense the true form of quaternions, or in other words Quaternions are an obfuscated version of Rotors. They are equivalent in that: they have the same number of components, their API is the same, they are as efficient, they are good for interpolation and avoiding gimbal lock, etc... in fact, they are isomorphic, so it is possible to do some math to turn a rotor into a quaternion, but doing so makes them less general and less intuitive (and loses extra capabilites).\n\nBut instead of defining Quaternions out of nowhere and trying to explain how they work retroactively, it is possible to explain Rotors almost entirely from scratch. This obviously takes more time, but I find it is very much worth it because it makes them much easier to understand!\n\nFor example, Quaternions are introduced as this mysterious four-dimensional object, but why introduce a fourth dimension of space to visualize a 3D concept? By contrast 3D Rotors do not require the use of a fourth dimension of space in order to be visualized.\n\nTrying to visualize quaternions as operating in 4D just to explain 3D rotations is a bit like trying to understand planetary motion from an earth-centric perspective i.e. overly complex because you are looking at it from the wrong viewpoint.\n\nIt would be great if we could start phasing out the use and teaching of Quaternions and replace them with Rotors. The change is simple and the code remains almost the same, but the understanding grows a lot.\n\nAs a side note, Geometric Algebra contains more than just Rotors, and is a very useful tool to have in one's toolbox. This article also serves as an introduction to it.\n\n(In the following article, every diagram is interactive. The video follows the article, and you can press the buttons to play the relevant section of video. \nConversely, you can press the button to go to the section of the article that corresponds to what the video is playing at this moment. You can maximize your window to have more space for the video, or you can press the button to set it to a fixed size.)\n\nPlanes of Rotations\n\nRotations happen in 2D planes\n\nIn 3D, we usually think of rotations as happening around an axis, like a wheel turning around its axle, but instead of thinking about the axle a more correct way is to think about the plane that the wheel lies on, perpendicular to the axle.\n\nThis old lady is spinning wheel in the $\\mathbf{xz}$ plane, perpendicular to the $\\mathbf{y}$ axis.\n\nThis is because if we split a vector into two pieces, one lying inside the plane ($\\mathbf{v}_\\parallel$) and one lying outside the plane ($\\mathbf{v}_\\perp$), the rotation rotates the inside part while keeping the outside part the same.\n\nRotation in the $yx$ plane [ Drag anywhere to move the camera ]\n\nIn 2D there is only one single plane to rotate in (there is no outside part). Therefore considering rotations to happen around a third axis (perpendicular to the 2D plane) is technically incorrect, since we shouldn't need to introduce another dimension to perform rotations.\n\nIf you told a 2D \"flatlander\" (who lives inside a 2D plane and has only ever experienced 2D) about a perpendicular rotation axis they would look at you and ask \"which direction does the axis point along? I can't picture it!\"\n\n[Aside] And in higher dimensions (4D and more) it is impossible to define a single normal vector to a 2D plane (for example in 4D there are two normal directions to a 2D plane, in 5D there are three normals directions, and in $n$D there are $n-2$)\n\nExplicit Sense of Rotation\n\nIn addition, when thinking about rotation around an axis, the sense of the rotation is undefined, and so needs to be defined by convention (via the so-called \"right hand rule\").\n\nHowever, if we think about rotations as happening inside planes, the sense is clear: rotation in the $\\mathbf{xy}$ plane means a rotation that takes the (unit) vector $\\mathbf{x}$ to the (unit) vector $\\mathbf{y}$, inside the plane they form together. Rotation in the $\\mathbf{yx}$ plane is the opposite rotation: it takes the vector $\\mathbf{y}$ to the vector $\\mathbf{x}$.\n\n[Aside] When I first learned about the three 3D rotation matrices along orthogonal planes, I remember thinking, why the hell does the $\\mathbf{R_y}$ matrix have the opposite sign? It's because of the \"right hand rule\", where we must define rotation around the $\\mathbf{y}$ axis to be from $\\mathbf{z}$ to $\\mathbf{x}$ instead of $\\mathbf{x}$ to $\\mathbf{z}$ to maintain a consistent right-handed sense of rotation. That convention becomes unnecessary when we talk directly about the plane itself.\n\n$R_X(\\theta) = \\begin{bmatrix}1 & 0 & 0\\\\0 & cos(\\theta) & -sin(\\theta)\\\\0 & sin(\\theta) & cos(\\theta)\\end{bmatrix} \\: \\: \\:\n R_Y(\\theta) = \\begin{bmatrix}cos(\\theta) & 0 & \\bbox[5px,border-bottom:2px solid red]{\\ \\ }sin(\\theta)\\\\0 & 1 & 0\\\\ \\bbox[5px,border-bottom:2px solid red]{-}sin(\\theta) & 0 & cos(\\theta)\\end{bmatrix} \\: \\: \\:\n R_Z(\\theta) = \\begin{bmatrix}cos(\\theta) & -sin(\\theta) & 0\\\\sin(\\theta) & cos(\\theta) & 0\\\\0 & 0 & 1\\end{bmatrix}$\n\nBivectors\n\nThe Outer Product\n\nTo compute the axis of rotation to rotate one vector $\\mathbf{a}$ to another vector $\\mathbf{b}$, we take the cross product of the two vectors to get a vector that is perpendicular to both. But why \"leave\" the plane, since a rotation is fundamentally a 2D thing?\n\nInstead we take what is called the outer product (also called exterior, or wedge product) of the two vectors, building a new element called a bivector (or 2-vector) $\\mathbf{B}$ that represents the plane the two vectors form together. If the cross product creates the normal vector to a plane, the outer product creates the plane itself. Taking the normal to the plane is extraneous.\n\n$$\\mathbf{B} = \\mathbf{a} \\wedge \\mathbf{b}$$\n\n$\\mathbf{B}$ can be represented as the parallelogram built from the vectors $\\mathbf{a}$ and $\\mathbf{b}$, in the plane they form together.\n\nThe idea of a bivector might seem a bit strange at first, but they are pretty much as fundamental as vectors, as we will see. If a vector is like a line, then a bivector is like a plane... The properties of the outer product are suited to capture the properties of planes.\n\nBasis for Bivectors\n\nBivectors have components, just like vectors. But they are defined in terms of basis planes instead of basis lines like vectors.\n\nThe three orthogonal basis planes are $\\mathbf{x} \\wedge \\mathbf{y}$, $\\mathbf{x} \\wedge \\mathbf{z}$, and $\\mathbf{y} \\wedge \\mathbf{z}$, as seen on the diagram to the right.\n\nBut first let's look at the simpler 2D case...\n\n2D Bivectors\n\nIn 2D there is only one plane, the $\\mathbf{xy}$ plane. So a 2D bivector only has one component. For a bivector built from vectors $\\mathbf{a}$ and $\\mathbf{b}$, this number $B_{xy}$ is equal to the (signed) area of the parallelogram the two vectors form together.\n\n$$\\mathbf{B}=\\mathbf{a} \\wedge \\mathbf{b} = B_{xy} (\\mathbf{x} \\wedge \\mathbf{y})$$\n\nYou can play with a 2D bivector in the following interactive diagram, by adjusting the (unit) vectors it is made from:\n\nYou can see that by changing the angle between the vectors the area of the parallelogram changes (according to the sine of the angle).\n\nIf the vectors are the same, or if they are parallel, they don't form a proper plane and the result is zero. This simple property defines what a bivector is:\n\n$$\\mathbf{a} \\wedge \\mathbf{a} = 0$$\n\nBy looking at the sum of two vectors, we can see that this property implies the following:\n\n$$\\begin{eqnarray}(\\mathbf{a}+\\mathbf{b}) \\wedge (\\mathbf{a}+\\mathbf{b}) &=& 0 \\\\\n\\mathbf{a} \\wedge \\mathbf{a} + \\mathbf{b} \\wedge \\mathbf{a} + \\mathbf{a} \\wedge \\mathbf{b} + \\mathbf{b} \\wedge \\mathbf{b} &=& 0 \\\\\n\\mathbf{b} \\wedge \\mathbf{a} + \\mathbf{a} \\wedge \\mathbf{b} &=& 0\n\\end{eqnarray}\n$$\n\nTherefore:\n\n$$\\mathbf{a} \\wedge \\mathbf{b} = -\\mathbf{b} \\wedge \\mathbf{a}$$\n\nJust like the sense of a rotation matters, the order of the arguments to the outer product matters. Swapping the arguments changes the sign of the result (this is called \"anti-symmetric\").\n\nIn the diagram, the sign is represented using the color, which changes from blue to green. The sign changes whenever the rotation from $\\mathbf{a}$ to $\\mathbf{b}$ goes from being clockwise to being anticlockwise (i.e. if it matches the ($\\mathbf{x}$ to $\\mathbf{y}$) direction or the ($\\mathbf{y}$ to $\\mathbf{x}$) direction).\n\nYou can see how the properties of the outer product are suited to capture the properties of planes and rotations.\n\n2D Bivectors from non-unit vectors\n\nThe vectors obviously don't have to be unit lengths, and in this diagram the restriction is removed:\n\nThe signed area of the parallelogram is proportional to the lengths of both vectors: $B_{xy} = sin(\\alpha)\\|a\\|\\|b\\|$ where $\\alpha$ is the angle between $\\mathbf{a}$ and $\\mathbf{b}$. So for example doubling the length of one vector doubles the area.\n\nWe can get the actual value by plugging in the vectors in component form:\n\n$$\\begin{eqnarray}\\mathbf{a} \\wedge \\mathbf{b} &=& (a_x \\mathbf{x} + a_y \\mathbf{y}) \\wedge (b_x \\mathbf{x} + b_y \\mathbf{y}) \\\\\n &=& a_x b_x (\\mathbf{x} \\wedge \\mathbf{x}) + a_x b_y (\\mathbf{x} \\wedge \\mathbf{y}) + a_y b_x (\\mathbf{y} \\wedge \\mathbf{x}) + a_y b_y (\\mathbf{y} \\wedge \\mathbf{y}) \\\\\n &=& a_x b_y (\\mathbf{x} \\wedge \\mathbf{y}) + a_y b_x (\\mathbf{y} \\wedge \\mathbf{x}) \\\\\n &=& a_x b_y (\\mathbf{x} \\wedge \\mathbf{y}) - a_y b_x (\\mathbf{x} \\wedge \\mathbf{y}) \\\\\n &=& (a_x b_y - a_y b_x) (\\mathbf{x} \\wedge \\mathbf{y})\n\\end{eqnarray}$$\n\n$$B_{xy} = a_x b_y - b_x a_y$$\n\n3D Bivectors\n\nJust like the coordinates of a vector $\\mathbf{v}$ can be thought of as the projections of the vector onto the three orthogonal basis axes ($\\mathbf{x},\\mathbf{y},\\mathbf{z}$), the coordinates of a bivector $\\mathbf{B}$ can be thought of as the projections of the small plane onto the three orthogonal basis planes.\n\nThe projections of the vector are the lengths of that vector along each basis vector, while the projections of the bivector are the areas of the plane on each basis plane.\n\nFor a vector:\n\n$$\\mathbf{v} = \\bbox[5px,border-bottom:2px solid red]{v_x} \\mathbf{x} + \\bbox[5px,border-bottom:2px solid green]{v_y} \\mathbf{y} + \\bbox[5px,border-bottom:2px solid blue]{v_z} \\mathbf{z}$$\n\nFor a bivector:\n\n$$\\mathbf{B} = \\bbox[5px,border-bottom:2px solid coral]{B_{xy}} (\\mathbf{x} \\wedge \\mathbf{y}) + \\bbox[5px,border-bottom:2px solid gold]{B_{xz}} (\\mathbf{x} \\wedge \\mathbf{z}) + \\bbox[5px,border-bottom:2px solid DarkViolet]{B_{yz}} (\\mathbf{y} \\wedge \\mathbf{z})$$\n\nWhere $B_{xy}, B_{xz}, B_{yz}$ are just numbers like $v_x, v_y, v_z$ (they are underlined to match the diagram colors).\n\nThe components of a 3D bivector are just the three 2D projections of the bivector onto the 2D basis planes.\n\nUsing the same method as before we find that the actual values of the components look a lot like the XY component from the 2D case, but applied to all three planes:\n\n$$B_{xy} = a_x b_y - b_x a_y$$\n$$B_{xz} = a_x b_z - b_x a_z$$\n$$B_{yz} = a_y b_z - b_y a_z$$\n\nYou can play with a 3D bivector in the following interactive diagram:\n\nDoes the exterior product remind you of anything? In 3D, the definition of the outer product is very similar to that of the cross product. In fact, in 3D a vector that comes from a cross product (such as a normal vector) will have three components which are equal to the components of the bivector (the numbers are the same, but the basis is different).\n\n$$\\begin{eqnarray}\\mathbf{a} \\wedge \\mathbf{b} &=& & (a_x b_y - b_x a_y)(\\mathbf{x} \\wedge \\mathbf{y}) \\\\\n & & + & (a_x b_z - b_x a_z)(\\mathbf{x} \\wedge \\mathbf{z}) \\\\\n & & + & (a_y b_z - b_y a_z)(\\mathbf{y} \\wedge \\mathbf{z}) \\\\\n \\\\\n \\mathbf{a} \\times \\mathbf{b} &=& & (a_x b_y - b_x a_y) \\  \\mathbf{z} \\\\\n & & - & (a_x b_z - b_x a_z) \\  \\mathbf{y} \\\\\n & & + & (a_y b_z - b_y a_z) \\  \\mathbf{x}\\end{eqnarray}$$\n\n[Aside] I chose a lexicographic order for the basis because it is easy to remember, but choosing $\\mathbf{z} \\wedge \\mathbf{x}$ instead of $\\mathbf{x} \\wedge \\mathbf{z}$ would make the signs the same. It would also makes the bivector basis directions consistent. This is the right hand rule, except the understandable non-arbitrary version :)\n\nThe bivector definition makes sense geometrically, instead of appearing out of thin air. I remember thinking when I was learning the cross product, why the hell does it return a vector that has length equal to the area of the parallelogram formed by the two vectors? That feels so arbitrary. And why would you be allowed to turn the area of the parallelogram into the length of the vector?\n\nSemantics of Vectors and Bivectors\n\nIn 3D, a bivector has three coordinates, one per plane: ($\\mathbf{xy}$, $\\mathbf{xz}$, and $\\mathbf{yz}$). Vectors also have three coordinates, one per axis ($\\mathbf{x}$, $\\mathbf{y}$ and $\\mathbf{z}$). Each plane is perpendicular to one axis. This is a coincidence that only happens in three dimensions (*) and it is why historically we have been confusing bivectors with vectors.\n\n[Aside] (*) In 2D there is only one basis bivector ($\\mathbf{xy}$), in 3D there are 3 basis bivectors ($\\mathbf{xy},\\mathbf{xz},\\mathbf{yz}$), in 4D there are 6 basis bivectors ($\\mathbf{xy},\\mathbf{xz},\\mathbf{xw},\\mathbf{yz},\\mathbf{yw},\\mathbf{zw}$), etc...\n\nIn programming terms, they both have the same memory layout, but different operations. Using a 3D vector instead of a 3D bivector is like \"type-casting\" the bivector.\n\nHere's an example: you might have seen how normal vectors transform differently than regular vectors, using the \"inverse transpose\" of the matrix $(\\mathbf{M}^{T})^{-1}$ instead of the matrix itself. That's because they are not really vectors, but actually bivectors, which we have \"type-cast\" to vectors. In physics, there's a hack called an \"axial vector,\" which has been introduced to differentiate vectors that come from cross products from regular vectors. Bivector is the actual \"type\" of the object and it should be thought of and manipulated as such.\n\n[Aside] Trivectors\n\nWe can keep taking the outer product to build not just oriented 2D areas, but oriented 3D volumes as well. A trivector $T$ can be built by taking the outer product twice:\n\n$$\\mathbf{T} = \\mathbf{a} \\wedge \\mathbf{b} \\wedge \\mathbf{c}$$\n\nIn 3D it stops there. Just like in 2D there is only one plane which fills all of 2D space, in 3D there is only one volume which fills all of 3D space.\n\n[But in nD we can keep building more outer products of vectors, until we reach the n-th dimension. For example in 4D, we have four basis tri-vectors ($\\mathbf{xyz},\\mathbf{xyw},\\mathbf{xwz},\\mathbf{yzw}$) and one basis 4-vector $\\mathbf{xyzw}$]\n\nIn 3D a trivector has only one basis component ($T_{xyz}$), equal to the volume of the parallelepiped generated by the three vectors. The triple outer product is a better version of the scalar triple product ($(\\mathbf{a} \\times \\mathbf{b}) \\cdot \\mathbf{c}$) because it only involves one kind of operation, returns the correct type (volume instead of scalar) and works in any number of dimensions.\n\n$$\\mathbf{T} =  T_{xyz} \\mathbf{x} \\wedge \\mathbf{y} \\wedge \\mathbf{z}$$\n\nThe Geometric Product\n\nMultiplying Vectors together\n\nThe geometric product $\\mathbf{a b}$ (denoted without a symbol) is another operation one can do on vectors. The geometric product is defined so that vectors have inverses (i.e $\\mathbf{a} \\mathbf{a}^{-1}= 1$ where 1 is just the number $1$!) and have nice properties like associativity ($\\mathbf{a} (\\mathbf{b} \\mathbf{c}) = (\\mathbf{a} \\mathbf{b}) \\mathbf{c}$). The goal is to be able to multiply vectors together so that —just like for matrices—  multiplication corresponds to geometric operations.\n\n[Aside] Having inverses is useful so that whatever object $\\mathbf{a} \\mathbf{a}^{-1}$ is, it won't have an effect on vectors, i.e. it will just act like multiplying by the number $1$\n\nTo define the product, first note that it is possible to split a product (or any function that takes two arguments) into the sum of a part that does not change if we swap the arguments and one that does change, in the following way:\n\n$$\\begin{eqnarray}\\mathbf{a} \\mathbf{b} &=& \\frac{1}{2} (\\mathbf{a} \\mathbf{b} + \\mathbf{a} \\mathbf{b} + \\mathbf{b} \\mathbf{a} - \\mathbf{b} \\mathbf{a}) \\\\\n &=& \\frac{1}{2} (\\mathbf{a} \\mathbf{b} + \\mathbf{b} \\mathbf{a}) + \\frac{1}{2} (\\mathbf{a} \\mathbf{b} - \\mathbf{b} \\mathbf{a})\\end{eqnarray}$$\n\nThe first term does not depend on the order of the arguments $\\mathbf{a}$ and $\\mathbf{b}$ anymore (it is called the \"symmetric\" part), while the second term changes sign when the arguments are swapped (it is called the \"antisymmetric\" part).\n\nThe dot product of two vectors (also called inner product) is symmetric and is a measure of distance ($\\mathbf{a} \\cdot \\mathbf{a} = \\|\\mathbf{a}\\|^2 $), so it sounds useful geometrically to set it equal to the symmetric part:\n\n$$\\frac{1}{2} (\\mathbf{a} \\mathbf{b} + \\mathbf{b} \\mathbf{a}) = \\mathbf{a} \\cdot \\mathbf{b}$$\n\nSimilarity, the outer product of two vectors is antisymmetric, so it sounds useful geometrically to set it equal to the antisymmetric part:\n\n$$\\frac{1}{2} (\\mathbf{a} \\mathbf{b} - \\mathbf{b} \\mathbf{a}) = \\mathbf{a} \\wedge \\mathbf{b}$$\n\nIn addition, the dot product contains the cosine of the angle between the two vectors ($\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\|\\|\\mathbf{b}\\|cos(\\alpha)$), while the outer product contains the sine of the angle. Together they fully describe the angle between the vectors, as well the plane they form.\n\n[Aside] The completeness of the description is what makes the product invertible, since we can get from one vector to the other using the information contained in their product. If I give you $\\mathbf{a}$ and $\\mathbf{a} \\mathbf{b}$ you can get $\\mathbf{b}$. This could not be done with just the cosine, or just the sine/plane.\n\nSo the geometric product is:\n\n$$\\mathbf{a} \\mathbf{b} = \\mathbf{a} \\cdot \\mathbf{b} + \\mathbf{a} \\wedge \\mathbf{b}$$\n\nIt is strange because multiplying two vectors together gives the sum of two different things: a scalar and a bivector. However this is similar to how a complex number is the sum of a scalar and an \"imaginary\" number, so you might be used to it already. Here the bivector part corresponds to the \"imaginary\" part of the complex number. Except it is not \"imaginary,\" it's just a bivector, which we have a concrete picture of!\n\nBasically, by multiplying two vectors together we compute useful properties about them (the \"length of their projections onto each other\" / \"cosine of the angle\" ($\\mathbf{a} \\cdot \\mathbf{b}$), and the \"plane they form together\" / \"sine of the angle\" ($\\mathbf{a} \\wedge \\mathbf{b}$)), which we keep bundled together via the \"plus\" sign. The geometric product also gives these \"property bundles\" operations that can be applied to them, and these operations have geometric interpretations (for example: rotating and reflecting vectors), as we shall see now.\n\n[Aside] It is possible to express the geometric product in terms of the sine and cosine: $\\mathbf{a} \\mathbf{b} = \\|\\mathbf{a}\\|\\|\\mathbf{b}\\| ( cos(\\alpha) + sin(\\alpha) \\mathbf{B} )$ where $\\mathbf{B}$ is a bivector in the plane of both vectors, made from two unit perpendicular vectors.\n\nMultiplication Table\n\nThe multiplication table helps make this product more concrete: let's see what happens if we take products of the basis vectors ($\\mathbf{x}$,$\\mathbf{y}$,$\\mathbf{z}$).\n\nFor any basis vector, such as the $\\mathbf{x}$ axis, the result is $1$:\n\n$$\\mathbf{x} \\mathbf{x} = \\mathbf{x} \\cdot \\mathbf{x} + \\mathbf{x} \\wedge \\mathbf{x} = 1$$\n\nFor any pair of basis vectors, such as the $\\mathbf{x}$ and $\\mathbf{y}$ axes, the result is just the bivector they form together:\n\n$$\\mathbf{x} \\mathbf{y} = \\mathbf{x} \\cdot \\mathbf{y} + \\mathbf{x} \\wedge \\mathbf{y} = \\mathbf{x} \\wedge \\mathbf{y}$$\n\n(so we can call $\\mathbf{x} \\wedge \\mathbf{y}$ simply $\\mathbf{x} \\mathbf{y}$ since they are the same thing! This is true for basis vectors, as well vectors which are perpendicular i.e. have their dot product equal to zero)\n\nThis gives the following table:\n\n$\\mathbf{a} \\mathbf{b}$\n$\\mathbf{b}$\n\n$\\mathbf{x}$\n$\\mathbf{y}$\n$\\mathbf{z}$\n\n$\\mathbf{a}$\n$\\mathbf{x}$\n$1$\n$\\mathbf{x} \\mathbf{y}$\n$\\mathbf{x} \\mathbf{z}$\n\n$\\mathbf{y}$\n$-\\mathbf{x} \\mathbf{y}$\n$1$\n$\\mathbf{y} \\mathbf{z}$\n\n$\\mathbf{z}$\n$-\\mathbf{x} \\mathbf{z}$\n$-\\mathbf{y} \\mathbf{z}$\n$1$\n\nIt is basically trivial, unlike the quaternion table for example.\n\n[Aside] For example, multiplying the two vectors $(5,3,0)$ and $(2,0,1)$:\n\n$$\\begin{eqnarray}( 5 \\mathbf{x} + 3 \\mathbf{y} ) ( 2 \\mathbf{x} + 1 \\mathbf{z} ) &=& 5 \\  2 \\  \\mathbf{x} \\mathbf{x} + 5 \\  1 \\  \\mathbf{x} \\mathbf{z} + 3 \\  2 \\  \\mathbf{y} \\mathbf{x} + 3 \\  1 \\  \\mathbf{y} \\mathbf{z}\\\\ &=& 10 + 5 \\  \\mathbf{x} \\mathbf{z} - 6 \\  \\mathbf{x} \\mathbf{y} + 3 \\  \\mathbf{y} \\mathbf{z}\n\\end{eqnarray}\n$$\n\nThe Reflection Formula (Traditional Version)\n\nReflection by a vector [you can move each vector]\n\nIf we have a unit vector $\\mathbf{a}$ and a vector $\\mathbf{v}$ we can reflect $\\mathbf{v}$ by the plane perpendicular to $\\mathbf{a}$.\n\nThis is done the usual way: we decompose $\\mathbf{v}$ into a part perpendicular to the plane $\\mathbf{v}_\\perp = (\\mathbf{v} \\cdot \\mathbf{a}) \\mathbf{a}$, and a part parallel to the plane $\\mathbf{v}_\\parallel = \\mathbf{v} - \\mathbf{v}_\\perp = \\mathbf{v} - (\\mathbf{v} \\cdot \\mathbf{a})\\mathbf{a}$.\n\nThen, to reflect the vector, flip the perpendicular part while keeping the parallel part unchanged:\n$$\\begin{eqnarray}R_{\\mathbf{a}}(\\mathbf{v}) &=& \\mathbf{v}_\\parallel - \\mathbf{v}_\\perp \\\\ \n &=& ( \\mathbf{v} - (\\mathbf{v} \\cdot \\mathbf{a})\\mathbf{a} ) - ((\\mathbf{v} \\cdot \\mathbf{a}) \\mathbf{a}) \\\\\n &=&\\mathbf{v} - 2 (\\mathbf{v} \\cdot \\mathbf{a}) \\mathbf{a}\\end{eqnarray}$$\n\nThe Reflection Formula (Geometric Product Version)\n\nAt this point we can replace the dot product $\\mathbf{v} \\cdot \\mathbf{a}$ by its geometric product version $\\frac{1}{2} (\\mathbf{v} \\mathbf{a} + \\mathbf{a} \\mathbf{v})$ to get the following:\n$$\\begin{eqnarray}R_{\\mathbf{a}}(\\mathbf{v}) &=& \\mathbf{v} - 2(\\frac{1}{2}( \\mathbf{v} \\mathbf{a} + \\mathbf{a} \\mathbf{v})) \\mathbf{a} \\\\ &= & \\mathbf{v} - \\mathbf{v} \\mathbf{a}^2 - \\mathbf{a} \\mathbf{v} \\mathbf{a} \\\\ &= & - \\mathbf{a} \\mathbf{v} \\mathbf{a}\\end{eqnarray}$$\n\n($\\mathbf{a}^2 = \\mathbf{a} \\cdot \\mathbf{a} = 1$ since $\\mathbf{a}$ is a unit vector)\n\nThis is saying the exact same thing but in a different notation. Using a simple product notation instead of a formula to encode a fundamental operation such as a reflection is going to prove very useful!\n\n[Aside] If you are wondering how taking the geometric product more than once works, just look at the basis vectors and use the associativity property. There are only four possible cases:\n\n$$\\begin{eqnarray}\\mathbf{x} (\\mathbf{x} \\mathbf{x}) &=&  \\mathbf{x} 1 = \\mathbf{x} \\\\\n\n\\mathbf{x} (\\mathbf{x} \\mathbf{y}) &=& (\\mathbf{x} \\mathbf{x}) \\mathbf{y} = \\mathbf{y} \\\\\n\\mathbf{x} (\\mathbf{x} \\mathbf{z}) &=& (\\mathbf{x} \\mathbf{x}) \\mathbf{z} = \\mathbf{z} \\\\\n\\mathbf{x} (\\mathbf{y} \\mathbf{z}) &=& \\mathbf{x} \\mathbf{y} \\mathbf{z}\\end{eqnarray}$$\n\nThe results are: vector, vector, vector, trivector. However the last case can only happen if the three vectors are independent, which is never the case for $-\\mathbf{ava}$\n\nYou can also swap a pair of elements. This flips the sign since pairs of elements are bivectors. You can do this until you get pairs that combine to $1$:\n$$\\mathbf{y} (\\mathbf{x} \\mathbf{y}) =\\mathbf{y} \\mathbf{x} \\mathbf{y} = - \\mathbf{x} \\mathbf{y} \\mathbf{y} = -\\mathbf{x} $$\n\nThis gives the following table for multiplying a vector and a bivector:\n\n$\\mathbf{a} \\mathbf{B}$\n$\\mathbf{B}$\n\n$\\mathbf{xy}$\n$\\mathbf{xz}$\n$\\mathbf{yz}$\n\n$\\mathbf{a}$\n$\\mathbf{x}$\n$\\mathbf{y}$\n$\\mathbf{z}$\n$\\mathbf{xyz}$\n\n$\\mathbf{y}$\n$-\\mathbf{x}$\n$-\\mathbf{xyz}$\n$\\mathbf{z}$\n\n$\\mathbf{z}$\n$\\mathbf{xyz}$\n$-\\mathbf{x}$\n$-\\mathbf{y}$\n\n[Aside] For those curious it is possible to look at what happens at each step of $- \\mathbf{a} \\mathbf{v} \\mathbf{a}$ in terms of the geometric product.\n\nThe first step is:\n\n$$\\mathbf{v} \\mathbf{a} = \\mathbf{v} \\cdot \\mathbf{a} + \\mathbf{v} \\wedge \\mathbf{a}$$\n\nIf, like before, we split $\\mathbf{v}$ into a part perpendicular to the plane ($\\mathbf{v}_\\perp$) and a part parallel to it ($\\mathbf{v}_\\parallel$), we get:\n\n$$\\begin{eqnarray}(\\mathbf{v}_\\perp + \\mathbf{v}_\\parallel) \\mathbf{a} &=& (\\mathbf{v}_\\perp + \\mathbf{v}_\\parallel) \\cdot \\mathbf{a} + (\\mathbf{v}_\\perp + \\mathbf{v}_\\parallel) \\wedge \\mathbf{a} \\\\\n &=& \\mathbf{v}_\\perp \\cdot \\mathbf{a} + \\mathbf{v}_\\parallel \\cdot \\mathbf{a} + \\mathbf{v}_\\perp \\wedge \\mathbf{a} + \\mathbf{v}_\\parallel \\wedge \\mathbf{a}\\end{eqnarray}$$\n\n$\\mathbf{v}_\\parallel \\cdot \\mathbf{a} = 0$ since these vectors are perpendicular, while $\\mathbf{v}_\\perp \\wedge \\mathbf{a} = 0$ since these vectors are parallel.\n\n$$\\mathbf{v} \\mathbf{a} = \\mathbf{v}_\\perp \\cdot \\mathbf{a} + \\mathbf{v}_\\parallel \\wedge \\mathbf{a}$$\n\nThe first term is just the length of the projection of $\\mathbf{v}$ onto $\\mathbf{a}$, i.e the first term is just length of $\\mathbf{v}_\\perp$.\n\nLet's call $\\hat{\\mathbf{v}_\\parallel}$ the normalized version of $\\mathbf{v}_\\parallel$, such that $\\mathbf{v}_\\parallel = \\hat{\\mathbf{v}_\\parallel}\\|\\mathbf{v}_\\parallel\\|$. Then the second term is just a bivector $\\mathbf{B} = \\hat{\\mathbf{v}_\\parallel} \\wedge \\mathbf{a}$ multiplied by the length of $\\mathbf{v}_\\parallel$.\n\nThis bivector $\\mathbf{B}$ is made out of two perpendicular unit vectors, so it is a very pure representation of the plane of vectors $\\mathbf{a}$ and $\\mathbf{v}$. It doesn't contain information about their relative angle or their lengths, just the orientation of the plane.\n\nSo both terms are just the decomposition of $\\mathbf{v}$ into two orthogonal projections ($\\mathbf{v}_\\parallel$ and $\\mathbf{v}_\\perp$), as well as the plane they form ($\\mathbf{B}$):\n\n$$\\| \\mathbf{v}_\\perp \\| + \\| \\mathbf{v}_\\parallel \\| \\mathbf{B}$$\n\nBefore moving on to the next step, we can replace the outer product by the geometric product because $\\mathbf{a}$ and $\\mathbf{v}_\\parallel$ are perpendicular and so their outer and geometric product are equivalent (since the dot product part of their geometric product is zero).\n\n$$\\mathbf{v}_\\perp \\cdot \\mathbf{a} + \\mathbf{v}_\\parallel \\wedge \\mathbf{a} = \\mathbf{v}_\\perp \\cdot \\mathbf{a} + \\mathbf{v}_\\parallel \\mathbf{a}$$\n\nThe second step is:\n\n$$\\mathbf{a} \\mathbf{v} \\mathbf{a} = \\mathbf{a} (\\mathbf{v}_\\perp \\cdot \\mathbf{a}) + \\mathbf{a} \\mathbf{v}_\\parallel \\mathbf{a}$$\n\nThe first term is just the component of $\\mathbf{v}$ along $\\mathbf{a}$, i.e. the component of $\\mathbf{v}$ perpendicular to the plane. Or in other words the first term is just $\\mathbf{v}_\\perp$.\n\n$$\\mathbf{a} \\mathbf{v} \\mathbf{a} = \\mathbf{v}_\\perp + \\mathbf{a} \\mathbf{v}_\\parallel \\mathbf{a}$$\n\nSince (again) $\\mathbf{a}$ and $\\mathbf{v}_\\parallel$ are perpendicular their geometric product is just their outer product, so we can swap them and negate the sign.\n\n$$\\begin{eqnarray}\\mathbf{a} \\mathbf{v} \\mathbf{a} &=& \\mathbf{v}_\\perp - \\mathbf{v}_\\parallel \\mathbf{a} \\mathbf{a} \\\\\n&=& \\mathbf{v}_\\perp - \\mathbf{v}_\\parallel\\end{eqnarray}$$\n\nFinally the last step flips the sign:\n\n$$-\\mathbf{a} \\mathbf{v} \\mathbf{a} = -\\mathbf{v}_\\perp + \\mathbf{v}_\\parallel$$\n\nSo we see that the component of $\\mathbf{v}$ perpendicular to the plane is flipped, while the parallel part stays the same!\n\n[Aside] The length of $\\mathbf{a}$ is not very important so the following ignores it, but if $\\mathbf{a}$ is not a unit vector, we must divide by its length and the formula becomes $- \\mathbf{a} \\mathbf{v} \\mathbf{a}^{-1}$, which looks more like the \"Sandwich Product\" you might be used to.\n\nTwo Reflections is a Rotation: 2D case\n\nIt turns out that if we apply two successive reflections to $\\mathbf{v}$ (using vector $\\mathbf{a}$ followed by vector $\\mathbf{b}$) we get a rotation by twice the angle between the vectors $\\mathbf{a}$ and $\\mathbf{b}$.\n\nYou can apply each successive Reflection Step in the diagram below :\n\nYou can also change the vectors $\\mathbf{a}$, $\\mathbf{b}$, and $\\mathbf{v}$, but the initial configuration of vectors in the diagram (click the \"Reset Vector Positions\" button) should make it especially clear why the rotation ends up being twice the angle. Another configuration that is not bad is to set $\\mathbf{a}$ and $\\mathbf{b}$ to the $\\mathbf{x}$ and $\\mathbf{y}$ axes.\n\nTwo Reflections is a Rotation: 3D case\n\nIn the 3D case the vector $\\mathbf{v}$ can be split into two different parts, one lying inside the plane defined by $\\mathbf{a}$ and $\\mathbf{b}$, and one lying outside (perpendicular to) the plane. As seen in the following diagram, when the vector gets reflected by each plane its outside part stays the same. So for the inside part, we are back to the 2D case, and it just gets rotated by twice the angle!\n\nRotors\n\nIn terms of the Geometric Product, the two reflections simply correspond to:\n$$R_{\\mathbf{b}}(R_{\\mathbf{a}}(\\mathbf{v})) = - \\mathbf{b} (-\\mathbf{a} \\mathbf{v} \\mathbf{a}) \\mathbf{b} = \\mathbf{b} \\mathbf{a} \\: \\mathbf{v} \\: \\mathbf{a} \\mathbf{b}$$\n\nWe call $\\mathbf{a} \\mathbf{b} = \\mathbf{a} \\cdot \\mathbf{b} + \\mathbf{a} \\wedge \\mathbf{b}$ a Rotor because by multiplying by $\\mathbf{a} \\mathbf{b}$ on both sides of a vector we perform a rotation ($\\mathbf{b} \\mathbf{a}$ the same as $\\mathbf{a} \\mathbf{b}$ except the bivector part is flipped).\n\nApplying a Rotor $\\mathbf{a} \\mathbf{b}$ to both sides of a vector rotates this vector in the plane of vectors $\\mathbf{a}$ and $\\mathbf{b}$ by twice the angle between $\\mathbf{a}$ and $\\mathbf{b}$.\n\n✨⭐💖 That's all there is to it! 💖⭐✨�\n\n3D Rotors vs Quaternions\n\nWe can notice that 3D Rotors look a lot like Quaternions:\n\n$$a + B_{xy} \\  \\mathbf{x} \\wedge \\mathbf{y} + B_{xz} \\  \\mathbf{x} \\wedge \\mathbf{z} + B_{yz} \\  \\mathbf{y} \\wedge \\mathbf{z}$$\n\n$$a + b \\ \\mathbf{i} + c \\ \\mathbf{j} + d \\ \\mathbf{k}$$\n\nIn fact the code/math is basically the same! The main difference is that $\\mathbf{i}$, $\\mathbf{j}$ and $\\mathbf{k}$ get replaced by $\\mathbf{y} \\wedge \\mathbf{z}$, $\\mathbf{x} \\wedge \\mathbf{z}$ and $\\mathbf{x} \\wedge \\mathbf{y}$, but they work mostly the same way. Here is the code comparison. I did not include everything, such as log/exp for interpolation, but they are easy to make.\n\nHowever, as we have seen, 3D Rotors are a 3D concept that does not require the use of \"4D double rotations\" or \"stereographic projection\" to visualize. Trying to visualize quaternions as operating in 4D just to explain 3D rotations is a bit like trying to understand planetary motion from an earth-centric perspective i.e. overly complex because you are looking at it from the wrong viewpoint.\n\nAs we have seen, representing rotations as operating inside planes instead of around vectors helps a lot. For example the basis bivectors square to $-1$, just like the basis quaternions ($\\mathbf{i}^2=\\mathbf{j}^2=\\mathbf{k}^2 = -1$) :\n\n$$(\\mathbf{x} \\mathbf{y})^2 = (\\mathbf{x} \\mathbf{y}) (\\mathbf{x} \\mathbf{y}) = - (\\mathbf{y}  \\mathbf{x})  (\\mathbf{x}  \\mathbf{y}) = -\\mathbf{y}  (\\mathbf{x}  \\mathbf{x})  \\mathbf{y} = - \\mathbf{y}  \\mathbf{y} = -1$$\n\nMultiplying two bivectors together gives a third bivector, but this is basically trivial, and we don't have to remember how $\\mathbf{i} \\mathbf{j} = \\mathbf{k}$:\n\n$$(\\mathbf{x} \\mathbf{y}) (\\mathbf{y} \\mathbf{z}) = \\mathbf{x} (\\mathbf{y} \\mathbf{y}) \\mathbf{z} = \\mathbf{x} \\mathbf{z}$$\n\n(Note that we have used $\\mathbf{x} \\wedge \\mathbf{y} = \\mathbf{x} \\mathbf{y}$)\n\nThese properties are a consequence of the geometric product instead of appearing out of thin air!\n\nFurther Reading\n\n(by the way, Geometric Algebra contains a lot more cool stuff than rotors!)\n\nLinear and Geometric Algebra by Macdonald [ Amazon Link ] Great because it is very clear and simple since it is meant to replace an undergraduate Linear Algebra Textbook.\n\nGeometric Algebra For Computer Science by Dorst et al. [ Amazon Link ]\nGreat because programming something often makes you understand it better. Note: in the book the authors give the impression that Geometric Algebra is slower than Quaternions (etc...). It actually should be almost the same exact code (i.e. don't write Geometric Algebra code by making a generic struct that can contain all possible types of k-vectors, just write one struct per k-vector type, as needed. This means writing one Bivector struct, and one Rotor struct which is a Scalar + Bivector to replace Quaternions).\n\n[Aside] History\n\nIn 1773, Lagrange introduced the component form of both the dot and cross products in order to study the tetrahedron in three dimensions.\n\nIn 1844 Grassmann published The Theory of Linear Extension, which introduced the inner (dot), outer (wedge) product and n-vectors, as well as a lot of what we call linear algebra and vector analysis today.\n\nAround the same time (1843), Hamilton introduced Quaternions, trying to generalize complex numbers from 2D to 3D. Hamilton wanted the imaginary part of quaternions to work like a point in space (a vector), even though it does not (it works like a bivector), and that created confusion to this day.\n\nIn 1856 Arthur Cayley introduced matrix multiplication and the inverse matrix (but methods for solving linear systems were first considered by Leibniz in 1693).\n\nThe dot and cross products were introduced as part of the operations on quaternions. Around 1881 Gibbs split them off from quaternions to build the vector analysis we still use today because he considered quaternion analysis too complicated and restrictive. Gibbs was aware of Grassmann's work and wanted to make it more useful for his own research.\n\nIn 1878 Clifford combined Grassmann’s n-vectors and quaternion algebra to make the geometric product. In 1886 Lipschitz generalized Clifford’s work to n-dimensions.\n\nThe 20th century and present has seen a revival of interest for and work in Geometric Algebra (notably by Hestenes), and work to present the subject in an easier to digest form.\n\nHere are some good resources:\n\nOn the Evolution of Geometric Algebra and Geometric Calculus by D. Hestenes [Has a good history diagram!] [ link ] A History of Vector Analysis by Michael J. Crowe, 2002  [ link ] Quaternions: A History Of Complex Noncommutative Rotation Groups In Theoretical Physics by Johannes C. Familton, 2015 [ link ] Hamilton, Rodrigues, and the Quaternion Scandal by Simon L. Altmann, 1989 [ link ] Wikipedia: Geometric Algebra History [ link ] Wikipedia: Quaternion History [ link ]\n\nCredit\n\nMade by Marc ten Bosch. If you enjoyed this video/article combo thing, you might consider donating on PayPal or Patreon."
  },
  {
    "categories": [
      "Computer Science",
      "Games"
    ],
    "authors": [
      ""
    ],
    "title": "Trigonometry for Games",
    "link": "https://demoman.net/?a=trig-for-games",
    "description": "Trigonometry is a notorious topic because it's often mandatory in public school...but the grand majority of public school students have no use for it. A game programmer has plenty of uses for trig, though, which makes it much more compelling and approachable - let's learn the basics!",
    "content": "Trigonometry for Games (Making a Homing Rocket)\n\nDemo-Man\n\n(interactive gamedev tutorials)\n\nLogin\n\nSign Up\n\nDonations & Bribes\n\nWebsite by 2DArray (who makes games with a friend)\n\nTrigonometry for Games\n(Making a homing rocket)\nTrigonometry is a notorious topic because it's often mandatory in public school...but the grand majority of public school students have no use for it. \nA game programmer has plenty of uses for trig, though, which makes it much more compelling and approachable - let's learn the basics!\n\n(Once you start the slideshow, its captions will go here)\n\nClick to start slideshow\n\nBack to index\n\nHelp"
  },
  {
    "categories": [
      "Computer Science"
    ],
    "authors": [
      "Shaun Lebron"
    ],
    "title": "Visualizing Projections",
    "link": "https://shaunlebron.github.io/visualizing-projections/",
    "description": "",
    "content": "Visualizing Projections\n\nVisualizing Projections\nby @shaunlebron\n\nEveryone knows that a photograph is a 2D image of a 3D world. Take this\nspinning cube for example:\n\nWhen the colors of light travel from the object to the camera, they pass\nthrough and mark the photo, creating the colored pixels of the image. You can\nimagine the same scenario from a top-down perspective. Incidentally, this is\nthe same as imagining a 1D image of a 2D object.\n\nOur objective is to illustrate projections using this simplified 2D\nmodel.\n\nThe Standard Projection\n\nThe figure below is an interactive version of the previous diagram.  The\ncolored circles represent the objects being photographed.  The flat line is the\nphotograph itself.\n\nTry it: Drag the circles.\n\nThis is how all 3D graphics today are rendered to your 2D screen.  It is the\nStandard Projection, formally known as the Rectilinear Projection.\n\nUnfortunately, stretching is a nasty side effect of the Standard Projection\nwhen used for wide-angle views, and the angle of view must always be\nless than 180°.  To overcome these limitations we can use another\nprojection.\n\nThe Panorama\n\nThe next figure is a how a panoramic camera takes a picture.  The\nfilm is no longer flat, but spherical or cylindrical.  After the picture is\ntaken, we unroll it onto a flat frame.  If we use a cylinder, the\nprojection is formally known as a cylindrical projection.  If we use a sphere,\nthe projection is the common fisheye lens (formally known as equidistant\nazimuthal projection).\n\nTry it: Drag the circles.\n\nWe are able to capture a full 360° range without distortion near the\nmargins.\n\nThe Hybrid\n\nThe stereographic projection is a combination of the Standard and Panoramic\nprojections. It combines the range of the Panoramic Projection with the\nperspective effects of the Standard Projection (increases the size of the\nobjects near the periphery).\n\nThe stereographic projection requires two cameras.  The centered camera\nfirst projects the image onto the cylinderical or spherical screen, exactly\nlike before.  But instead of unrolling it onto a flat frame, we use a\nsecond camera to project it onto a flat frame.  This is known as a\nPanini projection if we use a cylinder rather than a sphere.\n\nTry it: Drag the circles.\n\nOther Projection Methods\n\nThere are hundreds of other projection methods used in cartography and\npanoramic photography tools.  The aforementioned projections are perhaps the\nmost intuitive to visualize.\n\nMotivation\n\nThis was originally intended as a supplement to\nBlinky, a prototype for\ntrying different projections in a game, where we found the stereographic/Panini\nto be the best way to represent our natural wide-angle view.\n\nCreating this illustration actually lead to a 1D game concept,\nBL1ND.\n\nSource Code\n\nhttps://github.com/shaunlebron/visualizing-projections"
  },
  {
    "categories": [
      "Computer Science",
      "Neural Networks"
    ],
    "authors": [
      ""
    ],
    "title": "A Neural Network Playground",
    "link": "http://playground.tensorflow.org/",
    "description": "Tinker With a Neural Network Right Here in Your Browser. Don’t Worry, You Can’t Break It. We Promise.",
    "content": "A Neural Network Playground\n\nTinker With a Neural Network Right Here in Your Browser. Don’t Worry, You Can’t Break It. We Promise.\n\nreplay\n\nplay_arrow\npause\n\nskip_next\n\nEpoch\n\nLearning rate\n\n0.00001\n0.0001\n0.001\n0.003\n0.01\n0.03\n0.1\n0.3\n1\n3\n10\n\nActivation\n\nReLU\nTanh\nSigmoid\nLinear\n\nRegularization\n\nNone\nL1\nL2\n\nRegularization rate\n\n0\n0.001\n0.003\n0.01\n0.03\n0.1\n0.3\n1\n3\n10\n\nProblem type\n\nClassification\nRegression\n\nData\n\nWhich dataset do you want to use?\n\nRatio of training to test data: XX %\n\nNoise: XX\n\nBatch size: XX\n\nRegenerate\n\nFeatures\nWhich properties do you want to feed in?\n\nClick anywhere to edit.\nWeight/Bias is 0.2 .\n\nThis is the output from one neuron. Hover to see it larger.\n\nThe outputs are mixed with varying weights, shown by the thickness of the lines.\n\nadd\n\nremove\n\nOutput\n\nTest loss\n\nTraining loss\n\nColors shows data, neuron and weight values.\n\nShow test data\n\nDiscretize output\n\nkeyboard_arrow_down\n\nUm, What Is a Neural Network?\nIt’s a technique for building a computer program that learns from data. It is based very loosely on how we think the human brain works. First, a collection of software “neurons” are created and connected together, allowing them to send messages to each other. Next, the network is asked to solve a problem, which it attempts to do over and over, each time strengthening the connections that lead to success and diminishing those that lead to failure. For a more detailed introduction to neural networks, Michael Nielsen’s Neural Networks and Deep Learning is a good place to start. For a more technical overview, try Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\nThis Is Cool, Can I Repurpose It?\nPlease do! We’ve open sourced it on GitHub with the hope that it can make neural networks a little more accessible and easier to learn. You’re free to use it in any way that follows our Apache License. And if you have any suggestions for additions or changes, please let us know.\nWe’ve also provided some controls below to enable you tailor the playground to a specific topic or lesson. Just choose which features you’d like to be visible below then save this link, or refresh the page.\n\nWhat Do All the Colors Mean?\nOrange and blue are used throughout the visualization in slightly different ways, but in general orange shows negative values while blue shows positive values.\nThe data points (represented by small circles) are initially colored orange or blue, which correspond to positive one and negative one.\nIn the hidden layers, the lines are colored by the weights of the connections between neurons. Blue shows a positive weight, which means the network is using that output of the neuron as given. An orange line shows that the network is assiging a negative weight.\nIn the output layer, the dots are colored orange or blue depending on their original values. The background color shows what the network is predicting for a particular area. The intensity of the color shows how confident that prediction is.\n\nWhat Library Are You Using?\nWe wrote a tiny neural network library\nthat meets the demands of this educational visualization. For real-world applications, consider the\nTensorFlow library.\n\nCredits\n\nThis was created by Daniel Smilkov and Shan Carter.\n        This is a continuation of many people’s previous work — most notably Andrej Karpathy’s convnet.js demo\nand Chris Olah’s articles about neural networks.\n        Many thanks also to D. Sculley for help with the original idea and to Fernanda Viégas and Martin Wattenberg and the rest of the\nBig Picture and Google Brain teams for feedback and guidance.\n\nSource on GitHub"
  },
  {
    "categories": [
      "Computer Science",
      "Neural Networks"
    ],
    "authors": [
      "Stephanie Yee",
      "Tony Chu"
    ],
    "title": "A visual introduction to machine learning",
    "link": "http://www.r2d3.us/visual-intro-to-machine-learning-part-1/",
    "description": "In machine learning, computers apply statistical learning techniques to automatically identify patterns in data. These techniques can be used to make highly accurate predictions.",
    "content": "A visual introduction to machine learning\n\nR2D3\n\nA visual introduction to machine learning\n:\n\nEnglish\n\nPortuguês\n\nFrançais\n\n中文\n\nрусский\n\nEspañol\n\nTürk\n\nItaliano\n\nلعربية\n\nBahasa Indonesia\n\n简体\n\nελληνικά\n\nDeutsch\n\nIn machine learning, computers apply statistical learning techniques to automatically identify patterns in data. These techniques can  be used to make highly accurate predictions.\nKeep scrolling. Using a data set about homes, we will create a machine learning model to distinguish homes in New York from homes in San Francisco.\n\nScroll\n\nFirst, some intuition\nLet’s say you had to determine whether a home is in San Francisco or in New York. In machine learning  terms, categorizing data points is a classification task.\nSince San Francisco is relatively hilly, the elevation of a home may be a good way to distinguish the two cities.\nBased on the home-elevation data to the right, you could argue that a home above 73 meters should be classified as one in San Francisco.\n\nAdding nuance\nAdding another dimension allows for more nuance. For example, New York apartments can be extremely expensive per square foot.\nSo visualizing elevation and price per square foot in a scatterplot helps us distinguish lower-elevation homes.\nThe data suggests that, among homes at or below 73 meters, those that cost more than $19,116.7 per square meter are in New York City.\nDimensions in a data set are called features, predictors, or variables.\n\nDrawing boundaries\nYou can visualize your elevation (>73 m) and price per square foot (>$19,116.7) observations as the boundaries of regions in your scatterplot. Homes plotted in the green and blue regions would be in San Francisco and New York, respectively.\nIdentifying boundaries in data using math is the essence of statistical learning.\nOf course, you’ll need additional information to distinguish homes with lower elevations and lower per-square-foot prices.\n\nThe dataset we are using to create the model has 7 different dimensions. Creating a model is also known as training a model.\nOn the right, we are visualizing the variables in a scatterplot matrix to show the relationships between each pair of dimensions.\nThere are clearly patterns in the data, but the boundaries for delineating them are not obvious.\n\nAnd now, machine learning\nFinding patterns in data is where machine learning comes in. Machine learning methods use statistical learning to identify boundaries.\nOne example of a machine learning method is a decision tree. Decision trees look at one variable at a time and are a reasonably accessible (though rudimentary) machine learning method.\n\nFinding better boundaries\nLet's revisit the 73-m elevation boundary proposed previously to see how we can improve upon our intuition.\nClearly, this requires a different perspective.\n\nBy transforming our visualization into a histogram, we can better see how frequently homes appear at each elevation.\nWhile the highest home in New York is 73m, the majority of them seem to have far lower elevations.\n\nYour first fork\nA decision tree uses if-then statements to define patterns in data.\nFor example, if a home's elevation is above some number, then the home is probably in San Francisco.\n\nIn machine learning, these statements are called forks, and they split the data into two branches based on some value.\nThat value between the branches is called a split point. Homes to the left of that point get categorized in one way, while those to the right are categorized in another. A split point is the decision tree's version of a boundary.\n\nTradeoffs\nPicking a split point has tradeoffs. Our initial split (~73 m) incorrectly classifies some San Francisco homes as New York ones.\nLook at that large slice of green in the left pie chart, those are all the San Francisco homes that are misclassified. These are called false negatives.\n\nHowever, a split point meant to capture every San Francisco home will include many New York homes as well. These are called false positives.\n\nThe best split\nAt the best split, the results of each branch should be as homogeneous (or pure) as possible. There are several mathematical methods you can choose between to calculate the best split.\n\nAs we see here, even the best split on a single feature does not fully separate the San Francisco homes from the New York ones.\n\nRecursion\nTo add another split point, the algorithm repeats the process above on the subsets of data. This repetition is called recursion, and it is a concept that appears frequently in training models.\n\nThe histograms to the left show the distribution of each subset, repeated for each variable.\n\nThe best split will vary based which branch of the tree you are looking at.\nFor lower elevation homes, price per square foot is, at $1061 per sqft, is the best variable for the next if-then statement. For higher elevation homes, it is price, at $514,500.\n\nGrowing a tree\nAdditional forks will add new information that can increase a tree's prediction accuracy.\n\nSplitting one layer deeper, the tree's accuracy improves to 84%.\n\nAdding several more layers, we get to 96%.\n\nYou could even continue to add branches until the tree's predictions are 100% accurate, so that at the end of every branch, the homes are purely in San Francisco or purely in New York.\n\nThese ultimate branches of the tree are called leaf nodes. Our decision tree models will classify the homes in each leaf node according to which class of homes is in the majority.\n\nMaking predictions\nThe newly-trained decision tree model determines whether a home is in San Francisco or New York by running each data point through the branches.\n\nHere you can see the data that was used to train the tree flow through the tree.\n\nThis data is called training data because it was used to train the model.\n\nBecause we grew the tree until it was 100% accurate, this tree maps each training data point perfectly to which city it is in.\n\nReality check\nOf course, what matters more is how the tree performs on previously-unseen data.\n\nTo test the tree's performance on new data, we need to apply it to data points that it has never seen before. This previously unused data is called test data.\n\nIdeally, the tree should perform similarly on both known and unknown data.\n\nSo this one is less than ideal.\n\nThese errors are due to overfitting. Our model has learned to treat every detail in the training data as important, even details that turned out to be irrelevant.\n\nOverfitting is part of a fundamental concept in machine learning explained in our next post.\n\nRecap\n\nMachine learning identifies patterns using statistical learning and computers by unearthing boundaries in data sets. You can use it to make predictions.\nOne method for making predictions is called a decision trees, which uses a series of if-then statements to identify boundaries and define patterns in the data.\nOverfitting happens when some boundaries are based on on distinctions that don't make a difference. You can see if a model overfits by having test data flow through the model.\n\nComing up next\nNext, we explore overfitting, and how it relates to a fundamental trade-off in machine learning. Check out Part II: Model Tuning and the Bias-Variance Tradeoff.\nQuestions? Thoughts? We would love to hear from you. Tweet us at @r2d3us or email us at team@r2d3.us.\n\nFollow us on Twitter...\nA visual introduction to machine learning Posted by @r2d3us on Twitter\n\n...or Facebook...\nA visual introduction to machine learning Posted by R2D3 on Facebook\n\n...or keep in touch with email\n\nPosts from R2D3.us\n\nKeep in touch!\n\nFootnotes\n\nMachine learning concepts have arisen across disciplines (computer science, statistics, engineering, psychology, etc), thus the different nomenclature.\nTo learn more about calculating the optimal split, search for 'gini index' or 'cross entropy'\nOne reason computers are so good at applying statistical learning techniques is that they're able to do repetitive tasks, very quickly and without getting bored.\nThe algorithm described here is greedy, because it takes a top-down approach to splitting the data. In other words, it is looking for the variable that makes each subset the most homogeneous at that moment.\nHover over the dots to see the path it took in the tree.\nSpoiler alert: It's the bias/variance tradeoff!\n\nWe are so sorry, but we designed this site for desktop\n    rather than mobile viewing. Please come back on\n    a desktop (or a screen at least 1024 by 768 pixels in size)! …or go ahead anyway.\n\nR2D3\n\nR2D3 is an experiment in expressing statistical thinking with interactive design. Find us at @r2d3us.\nQuestions? Check out the FAQs.\n\nStephanie interprets R2\nStephanie is currently at Stitch Fix ( & hiring  !!! ). In the past, she's been at Cardiogram, Sift Science, Google, Bain & Company, and Vector Capital. She's got a MS in Statistics from Stanford.\n\nFind Stephanie: LinkedIn Twitter Email\n\nTony visualizes with D3\nTony is a product design manager on Facebook ’s AI Platform and Tools team. Prior to Facebook, Tony led user experience and product design at Noodle Analytics, H2O and at Sift Science. He holds an MFA in Interaction Design at the School of Visual Arts in New York City, where he tried to change congress with a fancy infographic.\nFind Tony: Portfolio Twitter Blog LinkedIn Email"
  },
  {
    "categories": [
      "Computer Science"
    ],
    "authors": [
      "Omar Shehata"
    ],
    "title": "A Geometric Intuition for Linear Discriminant Analysis",
    "link": "https://omarshehata.github.io/lda-explorable/",
    "description": "Linear Discriminant Analysis, or LDA, is a useful technique in machine learning for classification and dimensionality reduction. It's often used as a preprocessing step since a lot of algorithms perform better on a smaller number of dimensions.",
    "content": "A Geometric Intuition for LDA\n\nA Geometric Intuition for Linear Discriminant Analysis\n\nOmar Shehata — St. Olaf College — 2018\n\nLinear Discriminant Analysis, or LDA, is a useful technique in machine learning for classification and dimensionality reduction. It's often used as a preprocessing step since a lot of algorithms perform better on a smaller number of dimensions.\nThe idea that you can take a rich 10 dimensional space and reduce it to 3 dimensions, while keeping most of the information intact, has always seemed a bit like magic to me. I like to think of LDA as a mathematical tool that allows you to peer into spaces that cannot directly be seen. It's like piercing the veil of the unknowable.\n\nIn this article, I'd like to explore the fascinating geometric side of this popular statistical technique. I found it much easier to understand once I got to see it this way, and I hope it will be useful for you as well.\n\nFor geometry is the gate of science, and the gate is so low and small that one can only enter it as a child.\n\n— William K. Clifford\n\nThe Premise\nYou're given high dimensional data and you're trying to reduce it. This need not be anything esoteric. It could be data about houses on the market with price, age, size, distance to public transport and number of rooms. That's already 5 dimensions — too many to visualize all at once.\nLike any good mathematician, you can start with a simple approach and explore its consequences. What happens if you just drop one of the dimensions?\nLet's see what this looks like. Here is some made up data about houses that's only in 2 dimensions. Blue dots could be houses that were sold and red ones are still looking for a buyer.\n\nThe graph on the left is the original data, and the line on the right is what it would look like after we drop it 1 dimension by just ignoring the Y axis.\nWe've successfully reduced our data, but remember that our goal is to reduce it without losing information. In this case, you can easily tell that there is a pattern to the data when looking at the 2D view. This pattern is completely lost in the 1D view.\nAnother way to think about this is to consider prediction. Given a new unclassified dot in the 2D view, you can easily guess whether it should be blue or red based on whether it's closer to the upper right or bottom left cluster. This would be much harder if you were given just the 1D view. In other words, our prediction algorithms would perform worse on the reduced data.\nWe've sacrificed accuracy for simplicity, but maybe there's a better way to reduce it. Perhaps we could drop the X axis instead?\nYou can try that out by clicking and dragging in the 2D graph below to rotate the projection line.\n\nYou can click here to see that dropping the X axis isn't much better, but these aren't the only two choices. Geometrically, we have an infinite number of lines we can project on!\nOut of all these possible lines, the line y = 0 provides the best possible separation. This best line is what LDA allows us to find.\nThe reduced data is only 1 dimensional, but it captures the important insight of the higher dimensional view. Think again about prediction. If we're looking at the 1D view, all we have to do to predict whether a new point should be blue or red is just see whether it's on the right or left side.\nThis means that not only would our prediction algorithms perform just as good on the reduced data, it would be computationally faster (because there's less data to process)! Strangely enough, by removing information, we've made it easier to gain insight.\nHigher Dimensions\nTo build on this intuition, let's go one dimension up and look at an example that we can still visualize in its unreduced form.\nHere we've got 3D data, this time with 3 classes, and we want to reduce it to 2D. The diagram below shows one such projection.\nIf you're using a keyboard, you can rotate the projection plane with W/S, A/D and Q/E.\n\nAgain, there's an obvious pattern to the data in its original form that's lost in the reduced view. Finding the best projection plane by hand that retains this pattern is a harder problem now because there's a much greater number of possible projection planes to explore in 3D compared to projection lines in 2D.\nFor this dataset, this best plane is x + y + z = 0. You can see how this gives us a clean separation in 2D compared to an arbitrary plane like this. Again, this best projection is what LDA allows us to find.\nNow here's the real test. What if we had 4D data that we couldn't even visualize? Below is what it looks projected down to 2D, (you can think of it as first projecting into 3D and then to 2D).\n\nα XY =\nα XZ =\nα YZ =\nα XW =\n\nα YW =\nα ZW =\n\nYou can rotate the projection plane in 4D space with the key pairs W/S, A/D, Q/E, J/L and I/K or if you're on a mobile device by clicking on the numbers and typing in an angle.\nAt first glance, this looks like an entangled mess. But we know that we might be able to find a projection that reveals the higher dimensional structure to us. I say might because we don't actually know what the original data looks like. We can keep rotating all the angles, and we might find something that looks like it has a good separation, but how do we know when we've found the best one?\nSo far I've been performing LDA behind the scenes to produce the answers. We will now peek behind the curtain and devise the algorithm that will allow us to make such bold claims as this is the best possible separation for this data.\n\nDevising an Algorithm\nThis is definitely the most exciting part of mathematics in my opinion. It's the creative process of starting with an open-ended question like “How do we look at high dimensional data?” and ending up with a tool that allows us to do just that for any dataset. Rather than spoil the punchline, I'll try to guide you a little further.\nThe first step is to recognize this as an optimization problem. It's hard to think in the abstract, but we can rely on the intuition we've built up so far. Think back to the simple 2D case. We had a closed set of solutions (we knew the angle of the projection line was between 0 and 360 degrees), and we were trying to pick the best one.\nBut what exactly does “best” mean? Intuitively it means the best separation between classes in the reduced data. So far we've been judging this visually. To solve this as an optimization problem, we need to come up with a metric. This metric should be a number for any given projection that's high when it has good separation, and low otherwise. If we have that, it becomes a standard optimization problem of finding the maximum that we can easily solve with existing techniques.\nSo what sort of formula can we come up with that would give us a high value for the projection below:\n\nCompared to a low value for this projection:\n\nThe simplest example of such a metric could be to simply compute the difference between the mean of each class in the reduced data. This works fairly well as a measure of how good the separation is in the above cases. But consider the case below, which I've constructed such that the mean of each class is the same as it is in the above dataset.\n\nSince the means of the classes in these two datasets are equal, our metric gives them the same number, which implies that this last separation is as good as the best separation above, which is clearly not true. A badly designed metric like this would lead our optimization algorithm astray.\nCan you think of a better way to numerically differentiate a good separation from a bad one? Pretend like it's the 1930's and no known solution for this exists. Give it some thought. What would you come up with?\n\nIf you came up with a solution that took into account both the means of the classes as well as the variance (spread), then give yourself a pat on the back. This is exactly what Ronald Fisher came up with 1936.\nThe formula he came up with is known as Fisher's Linear Discriminant and is defined as:\n$$\\frac{(\\mu_1 - \\mu_2)^2}{S_1 + S_2}$$\nWhere $\\mu_i$ is the mean of class $i$. $S_i$ is the “scatter” of class $i$ and is defined as $S_i = \\sum_{x\\in Class_i} (x - \\mu_i)^2$. It's a measure of how spread out the data is (by summing up how far each point is from the mean).\nWe want to maximize Fisher's Linear Discriminant, which means we want to favor projections that have greater distance between the means (a big numerator) and aren't very spread out (a small denominator).\nTo see how it works, as always, let's look at a concrete example. Use your mouse or finger in the diagram below to see the computed score for any projection.\n\n$$\\frac{(\\mu_1 - \\mu_2)^2}{S_1 + S_2}$$\n\n$$= \\frac{(0.00 - 0.00)^2}{00 + 00}$$\n\n$$= 0$$\n\nCan you see that projections that create better separation get higher scores? More importantly, can you see why?\nOne thing I discovered by playing around with this diagram is that there's only one way for a projection to have a score of 0, do you see what it is?\nNotice how nothing in this formula is specific to the 2D case. It's always the difference of the means divided by the sum of the scatter, whether we're working with 2, 4 or 10 dimensions.\nThis particular metric is what Fisher chose, but it's certainly not the only possible one. A different metric could optimize for different results (perhaps you care a little more about minimizing scatter so you multiply the denominator by a large number to give it more weight).\nI think by understanding that the design of the formulas we use is often a choice with consequences as opposed to just a given absolute, we unlock a power and joy in mathematics.\nAnd that's how LDA works!\n\nInitially created for the Explorable Explanations jam. The source code for this page and all interactive diagrams is available on GitHub and is public domain. The teacher's guide comes with some tips for using it in the classroom, like how to drag and drop your own data into the article.\nSpecial thanks to Professor Matt Richey for his inspiring lectures on LDA in the Algorithms for Decision Making class.\nResources & References\n\nThe associated Jupyter notebook shows you how to perform LDA yourself in Python using the data from this article.\nFor an example with real world data, Julia Silge writes about applying this to Stack Overflow data using PCA (very similar to LDA).\nFor the mathematical derivation, extending it to more than 2 classes and actually solving the optimization problem, I found these slides and this article very helpful while writing this article.\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "categories": [
      "Computer Science"
    ],
    "authors": [
      "Patricio Gonzalez Vivo",
      "Jen Lowe"
    ],
    "title": "The Book of Shaders",
    "link": "https://thebookofshaders.com/",
    "description": "",
    "content": "The Book of Shaders\n\nTiếng Việt - 日本語 - 中文版 - 한국어 - Español - Portugues - Français - Italiano - Deutsch - Русский - English\n\nThe Book of Shaders\nby Patricio Gonzalez Vivo and Jen Lowe\nThis is a gentle step-by-step guide through the abstract and complex universe of Fragment Shaders.\n\nContents\n\nAbout this book\n\nGetting started\n\nWhat is a shader?\n“Hello world!”\nUniforms\nRunning your shader\n\nAlgorithmic drawing\n\nShaping functions\nColors\nShapes\nMatrices\nPatterns\n\nGenerative designs\n\nRandom\nNoise\nCellular noise\nFractional brownian motion\nFractals\n\nImage processing\n\nTextures\nImage operations\nKernel convolutions\nFilters\nOthers effects\n\nSimulation\n\nPingpong\nConway\nRipples\nWater color\nReaction diffusion\n\n3D graphics\n\nLights\nNormal-maps\nBump-maps\nRay marching\nEnvironmental-maps (spherical and cube)\nReflect and refract\n\nAppendix: Other ways to use this book\n\nHow can I navigate this book offline?\nHow to run the examples on a Raspberry Pi?\nHow to print this book?\nHow can I collaborate?\nAn introduction for those coming from JS by Nicolas Barradeau\n\nExamples Gallery\n\nGlossary\n\nAbout the Authors\nPatricio Gonzalez Vivo (1982, Buenos Aires, Argentina) is a New York based artist and developer. He explores interstitial spaces between organic and synthetic, analog and digital, individual and collective. In his work he uses code as an expressive language with the intention of developing a better together.\nPatricio studied and practiced psychotherapy and expressive art therapy. He holds an MFA in Design & Technology from Parsons The New School, where he now teaches. Currently he works as a Graphic Engineer at Mapzen making openSource mapping tools.\nWebSite - Twitter - GitHub - Vimeo - Flickr\nJen Lowe is an independent data scientist and data communicator at Datatelling where she brings together people + numbers + words. She teaches in SVA's Design for Social Innovation program, cofounded the School for Poetic Computation, taught Math for Artists at NYU ITP, researched at the Spatial Information Design Lab at Columbia University, and contributed ideas at the White House Office of Science and Technology Policy. She's spoken at SXSW and Eyeo. Her work has been covered by The New York Times and Fast Company. Her research, writing, and speaking explore the promises and implications of data and technology in society. She has a B.S. in Applied Math and a Master's in Information Science. Often oppositional, she's always on the side of love.\nWebSite - Twitter - GitHub\nAcknowledgements\nThanks Scott Murray for the inspiration and advice.\nThanks Kenichi Yoneda (Kynd), Nicolas Barradeau, Karim Naaji for contributing with support, good ideas and code.\nThanks Kenichi Yoneda (Kynd) and Sawako for the Japanese translation (日本語訳)\nThanks Tong Li and Yi Zhang for the Chinese translation (中文版)\nThanks Jae Hyun Yoo for the Korean translation (한국어)\nThanks Nahuel Coppero (Necsoft) for the Spanish translation (español)\nThanks Raphaela Protásio and Lucas Mendonça for the Portuguese translation (portugues)\nThanks Nicolas Barradeau and Karim Naaji for the French translation (français)\nThanks Andrea Rovescalli for the Italian translation (italiano)\nThanks Michael Tischer for the German translation (deutsch)\nThanks Sergey Karchevsky for the Russian translation (russian)\nThanks Andy Stanton for fixing and improving the pdf/epub export pipeline\nThanks to everyone who has believed in this project and contributed with fixes or donations.\nGet new chapters\nSign up for the news letter or follow it on Twitter\nEnter your email address\n\nCopyright 2015 Patricio Gonzalez Vivo"
  }
]